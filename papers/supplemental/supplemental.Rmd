---
title: "\\LARGE Parents fine-tune their speech to children's vocabulary knowledge (SOM-R)"
author: "\\large \\emph{XXXXX, XXXXX, and XXXXX}"
header-includes:
  - \usepackage[section]{placeins}
  - \usepackage{float}
  - \floatplacement{figure}{h!} # make every figure with caption = t
  - \raggedbottom
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
documentclass: article
bibliography: animalgame-si.bib
fontsize: 11pt
geometry: margin=1in
csl: apa6.csl
---

```{r load-libraries, message=FALSE, warning=FALSE, include = F}
library(wordbankr)
library(readxl)
library(janitor)
library(here)
library(knitr)
library(papaja)
library(kableExtra)
library(tidyverse)
library(tidyboot)
library(feather)
library(lme4)
library(lmerTest)
library(broom)
library(broom.mixed)
opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, cache = TRUE, 
               tidy = FALSE, echo = FALSE)

theme_set(theme_classic(base_size = 12))

options(digits=2)
```

\renewcommand\thesection{S\arabic{section}}
\renewcommand{\thetable}{S\arabic{table}}  
\renewcommand{\thefigure}{S\arabic{figure}}
\section{Estimating ages of acquisition for animal words}

In designing the stimuli for our experiment, our goal was to use a set of target animals that varied in their average age of acquisition (AoA). To do this, we used two sources of information: (1) Concurrent parent-report estimates of children's vocabularies [Wordbank; @frank2017], and (2) Retrospective self-report estimates from a large group of adults on Amazon Mechanical Turk [@kuperman2012].

```{r load-wordbank-data}
administrations <- get_administration_data(language = "English (American)",
                                           original_ids = TRUE)

first_longitudinals <- administrations %>%
      filter(longitudinal) %>%
      arrange(original_id, age) %>%
      group_by(original_id) %>%
      slice(1)

cross_sectional <- administrations %>%
  mutate(cross_sectional = !longitudinal |
           (longitudinal & (data_id %in% first_longitudinals$data_id))) %>%
  filter(cross_sectional)
```

Wordbank is a large and growing repository of administrations of the MacArthur-Bates Communicative Development Inventory [CDI; @fenson2007]--a checklist of words and other items administered to parents in order to estimate their child's vocabulary. Because Wordbank contains a mixture of cross-sectional and longitudinal data, and we wanted to ensure independence of data across measurements, we used only the first administration for each American English-learning child in the database, yielding `r cross_sectional %>% distinct(original_id) %>% nrow()` children. For each animal word, we fit a separate  robust general linear model, estimating the proportion of children whose parents reported their producing the word from eight to 30 months (including data from both the Words and Gestures and Words and Sentences forms). Each word's normative age of acquisition was defined to be the first month of age at which 50% or more children were estimated to know the animal.

```{r wordbank-animals}
ws_animals <- get_item_data(language = "English (American)", form = "WS") %>%
  filter(category == "animals")

wg_animals <- get_item_data(language = "English (American)", form = "WG") %>%
  filter(category == "animals")

ws_animal_data <- get_instrument_data(language = "English (American)", 
                                      form = "WS", 
                                      items = ws_animals$item_id, 
                                      administrations = 
                                        filter(cross_sectional, 
                                               form == "WS")) %>%
  left_join(ws_animals, by = c("num_item_id", "language", "form")) %>%
  select(data_id, age, value, form, definition)


wg_animal_data <- get_instrument_data(language = "English (American)", 
                                      form = "WG", 
                                      items = wg_animals$item_id, 
                                      administrations = 
                                        filter(cross_sectional, 
                                               form == "WG")) %>%
  left_join(wg_animals, by = c("num_item_id", "language", "form")) %>%
  select(data_id, age, value,  form, definition)

animal_data <- bind_rows(ws_animal_data, wg_animal_data) %>%
  group_by(definition) %>%
  nest() %>%
  ungroup() %>%
  mutate(num_item_id = paste0("item_", 1:n())) %>%
  unnest(cols = c(data))

wordbank_aoas <- fit_aoa(animal_data, method = "glmrob") %>%
  ungroup() %>%
  select(-num_item_id) %>%
  mutate(source = "wordbank") %>%
  rename(word = definition) %>%
  mutate(word = case_when(
    word == "chicken (animal)" ~ "chicken",
    word == "fish (animal)" ~ "fish",
    word == "bunny" ~ "rabbit",
    T ~ word))
```

```{r load-snodgrass}
snodgrass_animals <- read_csv(here("corpus_data/cycowicz_data.csv")) %>%
  clean_names() %>%
  pull(intentional_name)
```

```{r load-kuperman}
kuperman_aoas <- read_excel(
  here("corpus_data/AoA_ratings_Kuperman_et_al_BRM.xlsx")) %>%
  clean_names() %>%
  select(word, rating_mean) %>%
  filter(word %in% snodgrass_animals) %>%
  rename(aoa = rating_mean) %>%
  mutate(aoa = as.numeric(aoa) * 12) %>%
  mutate(source = "kuperman", aoa = as.numeric(aoa))
```

```{r descriptives}
joint_aoas <- bind_rows(kuperman_aoas, wordbank_aoas) %>%
  pivot_wider(names_from = source, values_from = aoa) %>%
  filter(!is.na(wordbank) | !is.na(kuperman))

correlation <- cor.test(joint_aoas$kuperman, joint_aoas$wordbank)
```

Because only a subset of the animals in the @rossion2004 image set are included on the MacArthur-Bates Child Development Inventory, and thus available in Wordbank, we also used adult self-report norms from @kuperman2012 to derive estimates for the remaining animals. Typically, adult self-report estimates of age of acquisition are highly correlated with parent-report estimates, and they were for the `r correlation$parameter + 1` animals in both data sources ($r =$ `r correlation$estimate`, $t =$ `r correlation$statistic`, $p$ `r correlation$p.value %>% printp()`). However, self-report estimates were made on a 1-7 Likert scale rather than on the scale of months. 

```{r estimate-aoas}
aoa_model <- joint_aoas %>%
  filter(!is.na(wordbank) & !is.na(kuperman)) %>%
  lm(wordbank ~ kuperman, data = .)

predicted_aoas <- joint_aoas %>%
  mutate(predicted = predict(aoa_model, 
                             newdata = select(joint_aoas,kuperman))) %>%
  mutate(aoa = if_else(is.na(wordbank), predicted, wordbank)) %>%
  filter(word %in% snodgrass_animals) %>%
  arrange(aoa)
```

```{r animalgame-data}
subj_vocab <- read_csv(here("data/subj_vocab.csv")) 

word_difficulty <- subj_vocab %>%
  distinct(word, avg_known) %>%
  left_join(predicted_aoas, by = c("word"))

animalgame_cor <- cor.test(word_difficulty$avg_known, word_difficulty$aoa)
```

```{r aoa-table, results="asis"}
predicted_aoas %>%
    rename(animal = word, Wordbank = wordbank, Kuperman = kuperman, 
           `model estimate` = predicted, AoA = aoa) %>%
  arrange(animal) %>%
  select(animal, Wordbank, Kuperman, `model estimate`, AoA) %>%
  apa_table(format.args = list(na_string = ""), font_size = "footnotesize",
            caption = "Estimated age of acqusition (AoA) for each animal in months.")
```

```{r include = F, eval = F}
write_csv(predicted_aoas, here("corpus_data/predicted_aoas.csv"))
```

In order to estimate the ages of acquisition for animals missing from Wordbank, the fit a a general linear model estimating Wordbank age of acquisition from @kuperman2012 age of acquisition for all animals in both sets (\texttt{Wordbank $\sim$ Kuperman + 1}). We then used this model to scale age of acquisitions for the `r joint_aoas %>% filter(!is.na(kuperman), is.na(wordbank)) %>% nrow()` animals in the @kuperman2012 set missing from Wordbank. Table \ref{tab:aoa-table} shows the final estimated ages of acquisition for each animal in the @rossion2004 set as estimated from Wordbank, @kuperman2012, and our regression model. For comparison, Figure \ref{fig:difficulty-fig} shows the proportion of parents of 2-2.5-year-olds in our study who reported that their child knew each of the tested animals. These proportions were highly correlated with the model-predicted ages of acquisition ($r =$ `r animalgame_cor$estimate`, $t =$ `r animalgame_cor$statistic`, $p$ `r animalgame_cor$p.value %>% printp()`).

```{r difficulty-fig, fig.height = 2.5, fig.width = 4, fig.cap = "\\label{fig:difficulty-fig}Proportion of parents who reported that their child knew the canonical word for each target animal. Error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap."}
mean_word_difficulty <- subj_vocab %>%
  group_by(word) %>%
  tidyboot_mean(understands) %>%
  arrange(desc(empirical_stat))

plotting_words <- mean_word_difficulty %>%
  mutate(word = factor(word, levels = unique(word)))

ggplot(plotting_words, aes(x = word, y = empirical_stat, ymin = ci_lower, 
                            ymax = ci_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), 
        text = element_text(size = 10)) +
  geom_pointrange(size = .3) +
  labs(y = "Parents report known", x = "") 
```

\section{Model Details}

```{r load-data}
demos <- read_csv(here("data/demographics.csv"))

test_data <- read_csv(here("data/test_data.csv"))

target_data <- read_csv(here("data/transcripts.csv")) %>%
  left_join(subj_vocab, by = c("subj", "trial_target" = "word")) %>%
  filter(phase == "pre", !is.na(understands)) %>%
  complete(nesting(subj, target, understands, appearance),
           fill = list(length = 0)) %>%
  mutate(understands = factor(understands, levels = c(T,F), 
                              labels = c("Knows", "Doesn't Know")))

possible_vocab <- subj_vocab %>%
  distinct(possible_vocab) %>%
  pull()
```

```{r word-difficulty}
group_difficulty <- subj_vocab %>%
  group_by(type, word) %>%
  summarise(understands = mean(understands)) %>%
  tidyboot_mean(understands)

mean_word_difficulty <- subj_vocab %>%
  distinct(word, type, avg_known, ci_upper, ci_lower) %>%
  arrange(avg_known)
```

For readability, the main text includes only the key effects for each statistical model rather than a full specification. We include those here. In all cases, we began with the maximal model justified by the design. If this model did not converge, we removed effects iteratively beginning with interactions. We always prioritized random slopes of theoretical importance (e.g. random slopes of word knowledge for each participant) over control variables. Each model included at least a random intercept for each subject and item. Models were estimated using version 1.1-23 of the `lme4` package [@bates2015].

\subsection{Target animal difficulty}

To validate that parents were more likely to say that their children knew early age of acquisition animals than late age of acquisition animals, we fit a mixed-effects model predicting parents' judgments from *a priori* early and late categories (see paper). The model specification and output are shown in Table \ref{tab:difficulty-lmer}.

```{r difficulty-lmer, results = "asis"}
difficulty_lmer <- subj_vocab %>%
  mutate(type = factor(type, levels = c("early", "late"))) %>%
  glmer(understands ~ type + (1 | subj) + (1|word),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(term = c("intercept", "type late"),
         p.value = printp(p.value)) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value)

apa_table(difficulty_lmer,
          caption = "Late age of acquisition (AoA) animals were less likely to be known. Model was specified as \\texttt{understands $\\sim$ type + (1 | subj) + (1 | animal)}",
          escape = FALSE, placement = "h")
```


\subsection{Selection accuracy}

To confirm that parent-child dyads communicated successfully in the reference game, we analyzed children's choices. We fit 2 models. First, we asked whether children selected the target animal on each trial above chance levels (33\%). To do this we fit a mixed-effects model in which the only fixed effect was an intercept, and we used an offset of $log(\frac{1}{3})$ so that an intercept different from zero would indicate above chance performance. The results of this model are presented in Table \ref{tab:overall-acc-model}. We then repeated the same analysis separately for animals that parents judged that their children knew, and animals that they judged that their children did not (Table \ref{tab:type-acc-model}).


```{r test-data}
test_prediction_data <- target_data %>%
  filter(phase == "pre", !is.na(understands), !is.na(target)) %>%
  group_by(understands, subj, trial, target, appearance) %>%
  summarise(length = sum(length)) %>%
  mutate(log_length = log(length)) %>%
  left_join(select(test_data, subj, trial_num, target, correct), 
            by = c("subj", "trial" = "trial_num", "target")) %>%
  left_join(select(subj_vocab, subj, vocab) %>% distinct(), by = c("subj"))
```

```{r overall-acc-model, results = "asis", fig.pos = "H"}
overall_acc_model <- glmer(correct ~ 1 + offset(base) + (1 | subj) + (1 | target),
      data = test_prediction_data %>% mutate(base = log(1/3)), 
      family = "binomial") %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-group, -effect) %>%
  mutate(p.value = printp(p.value)) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  mutate(term = "intercept")

apa_table(overall_acc_model,
          caption = "Overall accuracy on each trial. Model specified as \\texttt{correct $\\sim$ 1 + offset(log(1/3)) + (1 | subj) + (1 | animal)}",
          escape = FALSE, placement = "h")
```

```{r type-acc-model, results = "asis"}
type_acc_model <- test_prediction_data %>% 
  mutate(base = log(1/3)) %>%
  group_by(understands)  %>%
  nest() %>%
  mutate(model = map(data, ~glmer(correct ~ 1 + offset(base) + (1 | subj) +
                                    (1 | target),
                                  data =. , family = "binomial") %>% 
                       tidy())) %>%
  select(-data) %>%
  unnest(cols = c(model)) %>%
  ungroup() %>%
  filter(effect == "fixed") %>%
  select(-group, -effect, -understands) %>%
  mutate(p.value = printp(p.value)) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  mutate(term = c("known intercept", "unknown intercept"))

apa_table(type_acc_model,
          caption = "Accuracy for known and unknown animals. Models specified as \\texttt{correct $\\sim$ 1 + offset(log(1/3)) + (1 | subj) + (1 | animal)} separately for known and unknown animals",
          escape = FALSE, placement = "h")
```


After confirming that parents were communicating successfully overall, we asked what predicted children's success at picking the correct animal on each trial. We predicted success on each trial from the number of words in the child's vocabulary (as estimated by the pre-experiment survey), the (log) length of parents' referring expressions, whether parents believed their child knew the target animal, and the interaction between length and whether the animal was known. Children with larger vocabularies were more accurate, children were more accurate for known animals, and longer utterances lead to lower success for known animals and higher success for unknown animals (Table \ref{tab:test-lmer}).

```{r test-lmer, results = "asis", }
test_lmer <- test_prediction_data %>%
    mutate(base = log(1/3)) %>%
  glmer(correct ~  offset(base) + log_length * understands + scale(vocab) + understands +
        (1 | subj) + (1 | target), 
      family = "binomial",
      data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-group, -effect) %>%
  mutate(p.value = printp(p.value)) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value) %>%
  mutate(term = c("intercept", "(log) length", "unknown animal", "scaled(vocab size)", "(log) length $\\cdot$ unknown animal"))

apa_table(test_lmer,
           caption = "Predicting accuracy on each trial from referring expressions. Model specified as \\texttt{correct $\\sim$ log(length) $\\cdot$ unknown + scaled(vocab) + offset(log(1/3)) + (1 | subj) + (1 | animal)}",
          escape = FALSE, placement = "h")
```

\section{Tuning}

Our key analyses concerned the relationship between the length of parents' referring expressions and their children's lexical knowledge. If parents tune the information in their utterances to children's language knowledge, they should produce longer referring expressions for unknown animals. To test this, we fit a model predicting the (log) length of parents' referential expressions on each trial from the child's vocabulary, the proportion of parents who reported that their child knew the target animal, whether the parent reported that their individual child knew the target animal, whether this was the first or second appearance of the target, and the interaction of appearance and the child's target animal knowledge. We found that both the proportion of all children who knew the animal and the parent's belief about their individual child's knowledge affected the length of parents' referring expressions. However, the effect of the individual child's knowledge was reduced on the second appearance of each animal (Table \ref{tab:primary-lmer}). 

```{r trial-known-data}
trial_known_data <- target_data %>%
  mutate(phase = factor(phase, levels = c("pre", "post")),
         understands = factor(understands, labels = c("unknown animal", 
                                                      "known animal"))) %>%
  group_by(phase, avg_known, understands, subj, appearance, trial, trial_target) %>%
  summarise(length = sum(length)) %>%
  left_join(select(subj_vocab, subj, vocab), by = "subj") %>%
  mutate(log_length = log(length))
```

```{r primary-lmer, results = "asis"}
primary_lmer <- trial_known_data %>%
  lmer(log_length ~ appearance * understands + avg_known + scale(vocab) +
         (understands | subj) + 
         (appearance | trial_target),
       data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = printp(p.value),
         term = c("intercept", "second appearance", "known animal", 
                  "prop. known", "scaled(vocab size)", "second $\\cdot$ known")) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value)

apa_table(primary_lmer,
           caption = "Predicting length of referring expressions. Model specified as \\texttt{log(length) $\\sim$ appearance $\\cdot$ known + prop. know + scaled(vocab) +  (known | subj) + (appearance | animal)}",
          escape = FALSE, placement = "h")
```

One possible explanation for the smaller effect of parents' a priori beliefs about the child's knowledge on the second appearance of each animal is that they gathered information from its first appearance. To test this hypothesis, we analyzed the second appearance of each animal, predicting (log) length of referring expressions from accuracy on the first appearance, the parents' belief about the child's knowledge of that animal, and the interaction. We found that parents who thought their child knew an animal, but discovered that they did not, used a longer referring expressions in its second appearance (Table \ref{tab:lag-lmer}).

```{r lag-data}
setup_lag_data <- test_prediction_data %>%
  arrange(subj, understands, target,appearance, correct) %>%
  group_by(subj, understands, target) %>%
  mutate(correct = factor(correct, levels = c(TRUE, FALSE), 
                          labels = c("correct", "incorrect"))) %>%
  mutate(lag_correct = lag(correct)) 
```


```{r lag-lmer, results = "asis"}
lag_lmer <- setup_lag_data %>%
  ungroup() %>%
  mutate(understands = fct_relevel(understands, 
                                    "Doesn't Know", "Knows")) %>%
  filter(!is.na(lag_correct)) %>%
  lmer(log_length ~ lag_correct * understands + 
         ( understands | subj) + (1 | target), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  mutate(p.value = printp(p.value),
         term = c("intercept", "first accuracy", "known animal", 
                  "first accuracy $\\cdot$ known animal")) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value)

apa_table(lag_lmer,
           caption = "Predicting length on an animal's second appearance. \\texttt{log(length) $\\sim$ first accuracy $\\cdot$ known + (known | subj) + (1 | animal)}",
          escape = FALSE, placement = "h")
```

\section{Content of referring expressions}

In our primary analyses, we focused on the length of parents' referring expressions as a theory-agnostic proxy for the amount of information in them. To assess how the content of these utterances changed in accord with parents' estimates of their children's animal knowledge, we manually coded utterances for the following features: (1) Use of the animal's canonical label (e.g., "leopard"), (2) Use of a descriptor (e.g., "spotted"), (3) Use of a comparison (e.g., "like a cat"), (4) Use of a subordinate category label (e.g., "Limelight Larry" for peacock), and (5) Use of a superordinate level category label (e.g., "bird" for peacock). Because the rates of usage of each of these kinds of reference varied widely, we fit a logistic mixed effects model separately for each reference kind, estimating whether it would be used on each trial from whether the parent thought their child knew the animal (Table \ref{tab:qual-lmer}). We also coded two other features: use of anaphora (e.g. "the spotted one") and use of animal sounds (e.g. "moo"). However, these were so rare that they could not be analyzed quantitatively.

```{r qual-coding}
tidy_qual_data <- target_data %>%
  select(-num_anaphoric, -num_animal_sounds) %>%
  group_by(understands, appearance, target, target, subj) %>%
  summarise_at(vars(length, num_descriptors:num_superordinate), sum) %>%
  pivot_longer(cols = num_descriptors:num_superordinate, 
               names_to = "qual_measure", 
               values_to = "used") %>%
  mutate(qual_measure = factor(qual_measure, 
                                levels = c("num_canonical", "num_comparison",
                                           "num_descriptors", "num_superordinate",
                                           "num_subordinate"))) %>%
  rowwise() %>%
  mutate(used = min(used, 1)) %>%
  ungroup()
```

```{r qual-lmer, results = "asis"}
qual_lmer <- tidy_qual_data %>%
  group_by(qual_measure) %>%
  nest() %>%
  mutate(model = map(data, 
                     ~glmer(used ~ understands + (1 | subj) + (1 | target), 
                    family = "binomial", data = . ) %>% tidy())) %>%
  select(-data) %>%
  unnest(cols = c(model)) %>%
  filter(effect == "fixed") %>%
  select(-effect, -group) %>%
  ungroup() %>%
  mutate(p.value = printp(p.value),
         qual_measure = case_when(qual_measure == 
                                    "num_descriptors" ~ "descriptor",
                                  qual_measure == 
                                    "num_canonical" ~ "canonical name",
                                  qual_measure == 
                                    "num_comparison" ~ "comparison",
                                  qual_measure == 
                                    "num_subordinate" ~ "subordinate",
                                  qual_measure == 
                                    "num_superordinate" ~ "suoperordinate"),
         term = case_when(term == "(Intercept)" ~ "intercept",
                          T ~ "unknown animal")) %>%
  rename(`$z$-value` = statistic, `$p$-value` = p.value, 
         measure = qual_measure)

apa_table(qual_lmer,
           caption = "Qualitative analysis of referential expressions. Models were specified as \\texttt{usage $\\sim$ unknown animal + (1 | subj) + (1 | animal)}",
          escape = FALSE, placement = "h")

```

\newpage

\section*{References}

\begingroup
\setlength{\parindent}{-0.5in}

\noindent

<div id = "refs"></div>
\endgroup