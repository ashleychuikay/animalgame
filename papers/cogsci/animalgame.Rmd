---
title: "Parents Calibrate Speech to Their Children's Vocabulary Knowledge"
bibliography: animalgame.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Ashley Leung, Alexandra Tunkel, and Daniel Yurovsky \\
         \texttt{\{ashleyleung, aetunkel, yurovsky\}@uchicago.edu} \\
        Department of Psychology \\ University of Chicago}

abstract: >
  Young children learn language at an incredible rate. While children come prepared with powerful statistical learning mechanisms, the statistics they encounter are also prepared for them: Children learn from caregivers motivated to communicate with them. Do caregivers modify their speech in order to support children's comprehension? We asked children and their parents to play a simple reference game in which the parent's goal was to guide their child to select a target animal from a set of three. We show that parents calibrate their referring expressions to their children's language knowledge, producing more informative references for animals that they thought their children did not know. Further, parents learn about their children's knowledge over the course of the game, and calibrate their referring expressions accordingly. These results underscore the importance of understanding the communicative context in which language learning happens. 

keywords:
  "parent-child interaction; language development; communication"
    
output: cogsci2016::cogsci_paper
final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)

options(digits=2)
```

```{r, libraries}
library(png)
library(grid)
library(xtable)
library(tidyverse)
library(knitr)
library(papaja)
library(ggthemes)
library(lme4)
library(lmerTest)
library(directlabels)
library(ggrepel)
library(feather)
library(here)
library(tidyboot)
library(broom)
library(broom.mixed)

theme_set(theme_classic(base_size = 10))
```

# Introduction

Children learn language at astonishing rates, acquiring thousands of words by the time they are toddlers. How do children learn so many words before they know how to dress themselves? One account for children's rapid language acquisition is statistical learning. Young children can attend to the distributional structure of language, learning to discriminate words and identify word order from speech streams [@saffran1996; @saffran2003]. Statistical learning can be a powerful tool for early language learning, and showcases the ability that children have to harvest information from their surroundings. However, the particular structure of children's language environments may also play a role in supporting language development.

The way we speak to children often differs from the way we speak to adults. Child-directed speech (CDS) exists across cultures, and is characterized by higher pitches and more exaggerated enunciations when compared to adult-directed speech (ADS) [@cooper1990; @grieser1988]. Not only do children prefer CDS over ADS, CDS is also a better predictor for language learning than overheard ADS [@shneidman2013]. CDS does not only differ from ADS in prosodic features- the structural qualities of CDS make speech segmentation and word learning easier [@thiessen2005; @yurovsky2012]. While children live in the same physical environments as adults, their *language environments* contain specific types of input that facilitate early language learning.

Children’s language environments are not only suited for their abilities; they also change across development. Parents play a role in changing their children's language environment, and there is evidence suggesting that these changes aid language development. Parents use simpler, more redundant language when talking to toddlers, and more complex syntactic structures when speaking with school-aged children [@snow1972]. Importantly, sensitive modification of parent response shapes language learning in children [@hoff-ginsberg1982; @tamis-lemonda2014]. 

Why do parents modify the way they speak according to their children? One possible explanation is that parents are actively teaching their children. Indeed, some have posited that CDS is an ostensive cue for social learning, and that infants are born prepared to attend to these cues [@csibra2009]. While it may be true that parents hope to impart knowledge to their children, we argue that effective communication is the proximal goal. The field of linguistics has long established that adults communicate in ways that are efficient. Grice's [-@grice1975] maxim of quantity states that speech should be as informative as necessary, and no more. Adults are able to adhere to these maxims, adapting speech according to conversational partners' knowledge as needed for successful communication [@clark1986]. We argue that the parent's goal to communicate with their child drives the change in language use. Specifically, parents adapt their speech according to their children's language abilities. 

Parents modify their language as a *means* to achieve successful communication. Research show that parents use simpler language and are more linguistically aligned with their younger children, and these patterns of speech change as their children develop [@snow1972; @yurovsky2016]. Parents are also sensitive to children’s vocabulary knowledge, and the way they refer to objects change markedly depending on whether they are novel, comprehended, or familiar to their children [@masur1997]. These changes in parent speech may indicate adaptations that are aimed at fulfilling the goal of effective communication, and that the language necessary to fulfill that goal changes as children develop.

Based on work by @masur1997, we developed a study to investigate how parents adapt their speech according to their children’s vocabulary knowledge. Masur’s study involved parents and children engaging in unstructured free play, and parents reported their children’s vocabulary knowledge after the session. Our study uses a structured interactive game that allows us to control for the amount and type of stimuli presented to the parent-child dyads, and parent-reported vocabulary measures are collected before the study. Our paradigm also introduces a communicative goal within a structured game, which also allows parent utterances to be more comparable across dyads.

We designed an interactive iPad game in which parents verbally guide their children to select animals on an iPad. Each animal in the game appeared as a target twice. We predicted that parents would modify their speech based on their beliefs about their children’s vocabulary knowledge. Specifically, we predicted: (1) Parents should use shorter referring expressions when describing animals that they believe their children know, and (2) Upon the second appearance of an animal, parents would adapt the length of their referring expression according to whether the child responded accurately on the first appearance of the animal.

# Method

```{r load_data}
target_data <- read_feather(here("data/target_data.feather")) %>%
    filter(phase != "during", person == "parent", !is.na(understands)) %>%
    complete(nesting(subj, target, understands,  correct,  appearance),
           fill = list(length = 0))

word_difficulty <- read_feather(here("data/word_difficulty.feather"))
demos <- read_feather(here("data/demos.feather"))
```

## Participants

Toddlers (aged 2.0 to 2.5 years) and their parents were recruited from a database of families in the local community or approached on the floor of a local science museum in order to achieve a planned sample of 40 parent-child dyads. A total of 46 parent-child pairs were recruited, but data from six pairs were dropped from analysis due to experimental error or failure to complete the study. The final sample consisted of `r nrow(demos)` children aged `r min(demos$age)` to `r max(demos$age)` years ($M =$ `r mean(demos$age)`), `r demos %>% summarise(female = sum(demos == "female")) %>% pull` of whom were girls. 

```{r word_difficulty}
group_difficulty <- word_difficulty %>%
  group_by(type, word) %>%
  summarise(understands = mean(understands)) %>%
  tidyboot_mean(understands)

difficulty_lmer <- word_difficulty %>%
  filter(type %in% c("early", "late")) %>%
  mutate(type = factor(type, levels = c("early", "late"))) %>%
  glmer(understands ~ type + (1|subj), family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed")
```


## Stimuli

Eighteen animal images were selected from the @rossion2004 image set, which is a colored version of the @snodgrass1980 object set. Animals were selected based on age of acquisition (AoA), using data from WordBank [@frank2017]. The AoA of the selected animals ranged from 12 to 31 months. Half of the animals had lower AoA (12-20 months), and the other half had higher AoA (25-31 months). Each trial featured three animals, all from either the low AoA or high AoA category. 

A modified version of the MacArthur-Bates Communicative Development Inventory [CDI; @fenson2007], a parent-reported measure of children’s vocabulary, was administered before the testing session via an online survey. The selected animal words were embedded among the 85 words in the survey. Two of the animal words--one in the early AOA and one in the late AOA category--were accidentally omitted, so trials for those words were not included in analysis.

## Design and Procedure

Each parent-child pair played an interactive game using two iPads. Children were given two warm-up trials to get used to the iPads. The practice and experimental trials began after the warm-up. On each trial, three images of animals were displayed side by side on the child’s screen, and a single word appeared on the parent’s screen (Figure \ref{fig:ipads}). Parents were instructed to communicate as they normally would with their child, and encourage them to choose the object corresponding to the word on their screen. The child was instructed to listen to their parent for cues. Once an animal was tapped, the trial ended, and a new trial began. There was a total of 36 experimental trials, such that each animal appeared as the target twice. Trials were randomized for each participant, with the constraint that the same animal could not be the target twice in a row. Practice trials followed the same format as experimental trials, with the exception that images of fruit and vegetables were shown. All sessions were videotaped for transcription and coding.

```{r ipads, fig.env = "figure", fig.pos = "tb", fig.align='center', fig.width=2, fig.height=2, out.width = "150px", set.cap.width=T, num.cols.cap=1, fig.cap = "Example iPad screens for the child (top) and parent (bottom) during the experiment."}
include_graphics("figs/ipads.pdf")
```

## Results

The data of interest in this study were parent utterances used during the interactive game and parents’ responses on the adapted CDI. Transcripts of the videos were analyzed for length of referring expressions. We measured the length of parents' referring utterances as a proxy for amount of information given in each utterance. Parent utterances irrelevant to the iPad game (e.g. asking the child to sit down) were not analyzed. Children’s utterances were coded when audible, but were not analyzed.

### Word difficulty. 

We first confirm that the animals predicted be later learned were less likely to be marked known by the parents of children in our studies. As predicted, animals in the early AoA category were judged to be understood by `r group_difficulty %>% filter(type == "early") %>% mutate(empirical_stat = empirical_stat * 100) %>% pull(empirical_stat) %>% round(0)`% of parents, and items in the late AoA category were judged understood by `r group_difficulty %>% filter(type == "late") %>% mutate(empirical_stat = empirical_stat * 100) %>% pull(empirical_stat) %>% round(0)`%.

```{r difficulty_fig, set.cap.width=T, num.cols.cap=1, fig.cap = "Proportion of parents who reported that their child understood the word for each of our target animals. Error bars indicate 95\\% confidence intervals computed by non-parametric bootstrap.", fig.height = 2.1}
mean_word_difficulty <- word_difficulty %>%
  group_by(word) %>%
  tidyboot_mean(understands) %>%
  arrange(desc(empirical_stat))

plotting_words <- mean_word_difficulty %>%
  mutate(word = factor(word, levels = unique(word)))

ggplot(plotting_words, aes(x = word, y = empirical_stat, ymin = ci_lower, 
                            ymax = ci_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), 
        text = element_text(size = 10)) +
  geom_pointrange(size = .3) +
  labs(y = "Parents report known", x = "") 
```

The difference between these groups was confirmed statistically with a logistic mixed effects regression with a fixed effect of AoA type and random effects of participants. The late AoA items were judged known by a significantly smaller proportion of parents ($\beta =$ `r difficulty_lmer %>% filter(term == "typelate") %>% pull(estimate)`, $t =$ `r difficulty_lmer %>% filter(term == "typelate") %>% pull(statistic)`, $p$ `r difficulty_lmer %>% filter(term == "typelate") %>% pull(p.value) %>% printp()`). Parents' judgments for each target word are shown in Figure \ref{fig:difficulty_fig}.

### Length of referring expressions.

```{r trial_known_data}
trial_known_data <- target_data %>%
  mutate(phase = factor(phase, levels = c("pre", "post"), labels = c("before selection",
                                                                     "after selection")),
         understands = factor(understands, labels = c("unknown animal", "known animal"))) %>%
  group_by(phase, difficulty, understands, subj, appearance, trial_target) %>%
  summarise(length = sum(length)) 

mean_known_data <- trial_known_data %>%
  group_by(phase, understands, subj, appearance, trial_target) %>%
  summarise(length = mean(length)) %>%
  summarise(length = mean(length)) %>%
  summarise(length = mean(length)) %>%
  tidyboot_mean(length)

known_lmer <- trial_known_data %>%
  ungroup() %>%
  mutate(length = log(length+1)) %>%
  lmer(length ~ phase * understands + (1|subj) + (1|trial_target), 
       data = .) %>%
  tidy() %>%
  filter(effect == "fixed")

known_difficulty_lmer <- trial_known_data %>%
  ungroup() %>% 
  filter(phase == "before selection") %>%
  mutate(length = log(length+1)) %>%
  mutate(difficulty = 1 - difficulty, 
         understands = factor(understands, levels = c("known animal", "unknown animal"))) %>%
  lmer(length ~ difficulty + understands + (1|subj) + (1|trial_target), data = .) %>%
  tidy() %>%
  filter(effect == "fixed")
```

If parents calibrate their referential expressions to their children's linguistic knowledge, they should provide more information to children for whom a simple bare noun (e.g. "leopard") would be insufficient to identify the target. Parents did this in a number of ways: With one or more adjectives (e.g., "the spotted, yellow leopard"), with similes (e.g., "the one that's like a cat"), and with allusions to familiar animal exemplars of the category. In all of these cases, parents would be required to produce more words. Thus, we analyzed the length of parents' referential expressions as a theory-agnostic proxy for informativeness.

We predicted that parents should produce more informative--and thus longer--referring expressions to refer to animals that they thought their children did not know. We divided every trial of the game into phases: The time before a child selected an animal, and the time following selection until the start of the next trial. Figure \ref{fig:known_plot} shows the number of words that parents produced to refer to animals that they believe their children know versus those they believe their children do not know--both before their children selected an animal and after. In line with our prediction, parents produced significantly longer referring expressions when talking about animals that they believe their children do not know. However, once the child had selected an animal, the expressions that followed did not differ between known and unknown animals. 

We confirmed this result statistically, predicting number of words from a mixed effects model with fixed effects of phase and animal knowledge and their interaction, and random effects of participant and item. In this and all future models, we analyzed the number of words on a log scale as that improved model fit, but results are qualitatively similar when raw number of words was the dependent variable. Phase and the interaction of phase and knowledge were significant: Parents produced fewer words after selection ($\beta =$ `r known_lmer %>% filter(term == "phaseafter selection") %>% pull(estimate)`, $t =$ `r known_lmer %>% filter(term == "phaseafter selection") %>% pull(statistic)`,
$p$ `r known_lmer %>% filter(term == "phaseafter selection") %>% pull(p.value) %>% printp()`), and when the animal was known, ($\beta =$ `r known_lmer %>% filter(term == "understandsknown animal") %>% pull(estimate)`, $t =$ `r known_lmer %>% filter(term == "understandsknown animal") %>% pull(statistic)`, $p =$ `r known_lmer %>% filter(term == "understandsknown animal") %>% pull(p.value) %>% printp()`), but the change was smaller for known animals ($\beta =$ `r known_lmer %>% filter(term == "phaseafter selection:understandsknown animal") %>% pull(estimate)`, $t =$ `r known_lmer %>% filter(term == "phaseafter selection:understandsknown animal") %>% pull(statistic)`, $p =$ `r known_lmer %>% filter(term == "phaseafter selection:understandsknown animal") %>% pull(p.value) %>% printp()`). In the remainder of our analyses, we focus on utterances in the pre-selection phase of each trial as the post selection phase did not vary across trial targets.

Although each parent only gave a single bit of information about each animal--whether they thought their child knew it or not--we pooled these judgments across parents to estimate a continuous measure of difficulty (Figure \ref{fig:difficulty_fig}). If parents' referring utterances reflect a sensitivity to this continuous difficulty, the length of their referring expressions should vary smoothly with the difficulty of words. Figure \ref{fig:continuous_difficulty_fig} shows this relationship, which was confirmed by a mixed effects model predicting length from fixed effects of difficulty and animal knowledge, and random effects of subject and trial target. Referring expressions were reliably longer for more difficult animals ($\beta =$ `r known_difficulty_lmer %>% filter(term == "difficulty") %>% pull(estimate)`, $t =$ `r known_difficulty_lmer %>% filter(term == "difficulty") %>% pull(statistic)`,
$p =$ `r known_difficulty_lmer %>% filter(term == "difficulty") %>% pull(p.value) %>% printp()`), over and above the increase for unknown animals ($\beta =$ `r known_difficulty_lmer %>% filter(term == "understandsunknown animal") %>% pull(estimate)`, $t =$ `r known_difficulty_lmer %>% filter(term == "understandsunknown animal") %>% pull(statistic)`,
$p =$ `r known_difficulty_lmer %>% filter(term == "understandsunknown animal") %>% pull(p.value) %>% printp()`) 

```{r known_plot, cache = T, fig.cap = "Length of parents' references before and after their child selected a target animal. Points indicate means, error bars indicate 95\\% confidence intervals computed by non-parametric bootstrapping.", fig.height = 2}
ggplot(mean_known_data,
       aes(x = phase, y = empirical_stat, 
           ymin = ci_lower, ymax = ci_upper, color = understands,
           label = understands)) + 
  geom_pointrange(position = position_dodge(.25)) +
  scale_color_ptol() +
  labs(x = "", y = "Parents' words produced") +
  geom_dl(method = list(dl.trans(x=x +2.2), "first.points", cex=.7)) +
  theme(legend.position = "none")
```

```{r continuous_plot_data}
continuous_plot_data <- target_data %>%
  filter(phase == "pre") %>%
  group_by(trial_target, subj, appearance) %>%
  summarise(length = sum(length)) %>%
  summarise(length = mean(length)) %>%
  tidyboot_mean(length) %>%
  rename(length = empirical_stat,
         length_upper = ci_upper,
         length_lower = ci_lower) %>%
  select(-mean, -n) %>%
  left_join(mean_word_difficulty, by = c("trial_target" = "word")) %>%
  rename(understands_lower = ci_lower, understands_upper = ci_upper, 
         understands = empirical_stat) %>%
  select(-mean) %>%
  mutate(plotting_target = if_else(trial_target %in% c("squirrel", "cat", "lobster"),
                                   trial_target, as.character(NA)))
```

```{r continuous_difficulty_fig, fig.env = "figure*", fig.width=5.5, fig.height=2.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Number of words in parents' referential expressions as a function of the proportion of children reported to know the word for target animal. Points show group averaged proportions, error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}
ggplot(continuous_plot_data, aes(x = understands, y = length, xmin = understands_lower,
                          xmax = understands_upper, ymin = length_lower,
                          ymax = length_upper,
                          label = plotting_target)) + 
  geom_smooth(method = "lm", se = F, color = "black") +
  geom_pointrange()+ 
  geom_errorbarh() + 
  geom_label_repel() +
  theme(legend.position = "none") +
  labs(x = "Proportion of children reported knowing target word", 
       y = "Parents' words produced") 
```

```{r lag_data}
lag_data <- target_data %>%
  filter(phase == "pre") %>%
  ungroup() %>%
  mutate(understands = factor(understands, levels = c(T, F), 
                              labels = c("parent believes known", "parent believes unknown"))) %>%
  mutate(correct = factor(correct, levels = c(TRUE, FALSE), 
                          labels = c("correct", "incorrect"))) %>%
  group_by(subj, phase, understands, target, appearance, correct) %>%
  summarise(length = sum(length)) %>%
  ungroup() %>%
  complete(nesting(subj, target, understands,  correct,  appearance),
           fill = list(length = 0)) %>%
  arrange(subj, understands, target,appearance, correct) %>%
  group_by(subj, understands, target) %>%
  mutate(lag_correct = lag(correct)) %>%
  filter(appearance == "second", !is.na(lag_correct)) %>%
  group_by(understands, lag_correct, subj, target) 

mean_lag_data <- lag_data %>%
  summarise(length = sum(length, na.rm = T)) %>%
  summarise(length = mean(length, na.rm = T)) %>%
  tidyboot_mean(length) 

lag_lmer <- lag_data %>%
  mutate(length = log(length+1)) %>%
  lmer(length ~ lag_correct * understands + (1|subj) + (1|target), 
                 data = .) %>%
  tidy() %>%
  filter(effect == "fixed")

```

```{r lag_plot,  fig.cap = "Length of parents' referring expressions on the second appearance of each animal. Points show group averaged proportions; error bars show 95\\% confidence intervals computed by non-parametric bootstrap.", fig.height = 2}
ggplot(mean_lag_data, aes(x = lag_correct, y = empirical_stat, 
           ymin = ci_lower, ymax = ci_upper, color = understands)) + 
  facet_wrap(~ understands) +
  geom_pointrange(position = position_dodge(.5), show.legend = F) +
  labs(x ="Child's selection on previous appearance", y = "Parents' words produced") +
  theme(strip.text.x = element_text(size = 7))+
  scale_color_ptol()
```

We then tested our second hypothesis: Parents should modify their productions over the course of the experiment as they obtain evidence about their children's knowledge. Because each animal was the target twice, parents could use their children's selection on the first appearance of the animal to inform their referential expressions on the second appearance. Figure \ref{fig:lag_plot} shows the length of parents' referring expressions as a function of their prior belief about their children's knowledge and their children's selection on the first appearance of the target animal. As predicted, parents who thought their children knew an animal, but who observed evidence that they didn't (i.e. their children selected the wrong animal), lengthened their referring expressions on its second appearance. Parents who thought their children did not know an animal before the start of the game did not shorten their referring expressions if their children were correct the first time. We cannot say definitively why their referring expressions do not change in length, but one likely explanation is that the references that lead to success the first time were heavily scaffolded and may not even have contained the animal's canonical label (e.g. "the one that looks like a cat" for leopard). We confirmed these results with a mixed effects model predicting length of expressions from parents' prior beliefs, their children's selection on the first trial, and their interaction. We found only the interaction to be significant: References were not reliably longer when parents thought their children did not know the animal ($\beta =$ `r lag_lmer %>% filter(term == "understandsparent believes unknown") %>% pull(estimate)`, $t =$ `r lag_lmer %>% filter(term == "understandsparent believes unknown") %>% pull(statistic)`, $p =$ `r lag_lmer %>% filter(term == "understandsparent believes unknown") %>% pull(p.value) %>% printp()`), nor when the children were incorrect on the previous trial  ($\beta =$ `r lag_lmer %>% filter(term == "lag_correctincorrect") %>% pull(estimate)`, $t =$ `r lag_lmer %>% filter(term == "lag_correctincorrect") %>% pull(statistic)`, $p =$ `r lag_lmer %>% filter(term == "lag_correctincorrect") %>% pull(p.value) %>% printp()`, but only when the parent thought their children did not know the animal and their children were incorrect on the previous trial ($\beta =$ `r lag_lmer %>% filter(term == "lag_correctincorrect:understandsparent believes unknown") %>% pull(estimate)`, $t =$ `r lag_lmer %>% filter(term == "lag_correctincorrect:understandsparent believes unknown") %>% pull(statistic)`, $p =$ `r lag_lmer %>% filter(term == "understandsparent believes unknown") %>% pull(p.value) %>% printp()`).

&nbsp;

### Children's selections.

```{r test_data}
test_data <- target_data %>%
  ungroup() %>%
  mutate(understands = factor(understands, levels = c(T,F), 
                              labels = c("Knows", "Doesn't Know"))) %>%
  filter(phase == "pre", !is.na(understands)) %>%
  group_by(understands, correct,  subj, trial, trial_target, appearance) %>%
  summarise(length = log(sum(length))+1) 


mean_test_data <- test_data %>%
  group_by(understands, subj, trial_target, appearance) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  tidyboot_mean(correct)

test_lmer <- test_data %>%
  mutate(length = log(length+1)) %>%
  glmer(correct ~ length * understands + appearance + trial + 
        (1|subj) + (1|trial_target), 
      family = "binomial", 
      control = glmerControl(optimizer = "bobyqa"),
      data = .) %>%
  tidy() %>%
  filter(effect == "fixed") 
```

```{r test_model, results="asis", tab.env = "table", cache = T}
model_table <- test_lmer %>%
  select(-effect, -group,-std.error) %>%
  mutate(term = c("intercept", "length (log)", "unknown", 
                  "second appearance", "trial number", "length * unknown"),
         p.value = printp(p.value)) %>%
  rename(`t-value` = statistic,
         `p-value` = p.value) %>%
  xtable(caption = "Coefficient estimates for a mixed-effects logistic regression predicting children's success in selecting the target animal. The model was specified as \\texttt{correct $\\sim$ log(length) * unknown + appearance + trial + (1|subj) + (1|animal)}.",
         label = "tab:test_model")


print(model_table, type = "latex", comment = F, table.placement = "tb",
      include.rownames = FALSE)
```


```{r selection_plot, cache = T, fig.cap = "Children's accuracy at selecting both known and unknown animals. Points indicate means, error bars indicate 95\\% confidence intervals computed by non-parametric bootstrapping.", fig.height = 2}
mean_test_data %>%
  mutate(understands = factor(understands, labels = c("Child knows",
                                                      "Child doesn't know"))) %>%
  ggplot(aes(x = understands, y = empirical_stat, 
           ymin = ci_lower, ymax = ci_upper, color = understands,
           label = understands)) + 
  geom_pointrange(position = position_dodge(.25)) +
  scale_color_ptol() +
  labs(x = "Parents' belief about animal", y = "Selection accuracy") +
  theme(legend.position = "none")
```

Overall, children performed significantly above chance for both low AoA and high AoA trials. In our previous analyses, we showed that parents calibrated the length of their referring expressions to their beliefs about their children's knowledge. They did this both in response to their prior beliefs (Figure \ref{fig:known_plot}), and their in-game observations of their children's knowledge (Figure \ref{fig:lag_plot}). In our final analyses, we asked whether this mattered for children's selections. Are children more likely to succeed in the task when parents provide well calibrated utterances? We asked this question by predicting children's selection trial by trial from a mixed effects logistic regression with fixed effects of parents' prior beliefs about children's knowledge of the target animal, whether the trial was the first or second appearance of the the target animal, the length of parents' referring expressions, and the interaction of parents' prior beliefs and the length of their expressions, as well as random effects of subject and trial target. Children were more likely to be correct when their parents produced longer references, but only for animals that their parents believed that they did not know. Thus, parents' informative references to unknown animals did appear to be supporting successful communication of the target animal. Table \ref{tab:test_model} shows coefficient estimates for all parameters.

# Discussion

Parents have a wealth of knowledge about their kids, including their linguistic development [@fenson2007]. Do they draw on this knowledge when they want to communicate? In a referential communication task, we showed that parents speak differently depending on their beliefs about their children's vocabulary knowledge. Specifically, they produce shorter, less informative expressions to refer to animals that they believe their children know relative to animals that they think their children do not know. Further, parents update their beliefs during the course of the task, producing more informative expressions on the second appearance of an animal they previously thought their children knew if they observed evidence to the contrary (i.e. when children selected the wrong animal). We further found that more informative referring expressions were associated with increased likelihood of successful communication: Children were more likely to correctly select animals whose names they did not know if their parents produced longer utterances to refer to them. We leveraged length as a proxy for informativeness in parents' expressions in the service of quantitative, theory-agnostic predictions. In ongoing work, we are analyzing *how* parents succeed on these trials, and investigating whether different strategies lead to different levels of success. 

In general, communicative success was high. Children selected the correct animal at above chance levels, even for targets whose names their parents thought they did not know. Because easy and hard animals appeared on separate trials, children's high accuracy in selecting unfamiliar animals is unlikely to be due to the use of strategies like mutual exclusivity [@markman1988]. Instead, parents must have produced sufficient information for their children to find the correct target. Taken together with our finding that parents used longer sentences for words they think their children do not know, our results suggest that parents modified their speech as a means to communicate. 

Our proposed explanation for these results is that they are produced by a pressure for effective communication: Parents need to produce sufficient information for their children to understand their intended meaning. That is, parents design their utterances for their children's benefit [speaker-design, @jaeger2013]. It could be instead that these utterances reflect pressure from speaking itself. For example, length of parents' utterances may reflect their difficulty in retrieving certain animal words [@macdonald2013]. We find this explanation unlikely given that parents were given the target words in written form on their iPad, essentially eliminating retrieval problems [@wingfield1968]. The fact that parents are using long and short referring expressions depending on their beliefs about children's vocabulary knowledge suggests that they are calibrating to their children.

It is important to note that our current results do not rule out the possibility that parents are engaging in pedagogy. Parents may be using longer referring expressions because they wish to teach their children certain words, and this could potentially explain why parents use longer references for words they believe their children do not know. To understand the motivations behind long and short utterances, we are currently analyzing the content of parents' speech. Preliminary qualitative analysis shows that parents use more adjectives on trials where they believe their children do not know the target word (e.g. "Pick the red lobster" instead of "Pick the lobster"). The use of adjectives on these trials may reflect an intention to teach children about a certain animal, but it could also indicate a pressure to communicate effectively. In the lobster example, the color "red" is likely a helpful cue for children, and parents may be using adjectives as a way to help children select the correct target quickly. While our current findings do not allow us to distinguish between the pedagogical and communicative hypotheses, we hope that further analysis of parents' speech will help us differentiate the two accounts.

Our work contributes to the current literature on parent-child interaction, and forms the basis for further experimental work examining the influences that parent speech has on children’s language development. In line with @masur1997, our findings provide evidence that parents calibrate speech sensitively to their children's vocabulary knowledge. These results are important in light of previous work suggesting that parent responsiveness and sensitivity shape the way young children learn language [@hoff-ginsberg1982; @tamis-lemonda2014]. Furthermore, we propose that parents are modifying their speech as a means to communicate, and that communicative intent shapes the language environments children experience. Further qualitative analysis of our dataset will shed light onto the characteristics of parent-child communication that are helpful for language acquisition.

Finally, this study highlights the importance of studying the parent-child pair as a unit, rather than viewing children as isolated learners: both parents and children contribute to the process of language development [@hoff-ginsberg1982; @brown1977]. Focusing on the interactive and communicative nature of language captures a more realistic picture of children's language environments: The input that children receive is not random – it is sensitive to their developmental level.

\vspace{1em} \fbox{\parbox[b][][c]{7.3cm}{\centering All code for these analyses are available at\ \url{https://github.com/ashleychuikay/animalgame}}}

# Acknowledgements

This research was funded by a James S. McDonnell Foundation Scholar Award to DY. 

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
