---
title             : "Parents Calibrate Speech to Their Children's Vocabulary Knowledge"
shorttitle        : "Parental Speech Calibration"

author: 
  - name          : "Ashley Leung"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "5848 S University Ave, Chicago IL, 60642"
    email         : "ashleyleung@uchicago.edu"
  - name          : "Alexandra Tunkel"
    affiliation   : "1"
  - name          : "Daniel Yurovsky"
    affiliation   : "1,2"

affiliation:
  - id            : "1"
    institution   : "The University of Chicago"
  - id            : "2"
    institution   : "Carnegie Mellon University"

abstract: |
   Young children learn language at an incredible rate. While children come prepared with powerful statistical learning mechanisms, the statistics they encounter are also prepared for them: Children learn from caregivers motivated to communicate with them. Do caregivers modify their speech in order to support children's comprehension? We asked children and their parents to play a simple reference game in which the parent's goal was to guide their child to select a target animal from a set of three. We show that parents calibrate their referring expressions to their children's language knowledge, producing more informative references for animals that they thought their children did not know. Further, parents learn about their children's knowledge over the course of the game, and calibrate their referring expressions accordingly. These results underscore the importance of understanding the communicative context in which language learning happens. 

authornote: |
  Parts of this work were presented at the Annual Conference of the Cognitive Science Society: Leung et al. (2019). All code for these analyses are available at \url{https://github.com/ashleychuikay/animalgame}

keywords          : "parent-child interaction; language development; communication"
wordcount         : "X"

bibliography      : ["animalgame.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r load_packages, include = FALSE}
library(png)
library(grid)
library(egg)
library(xtable)
library(knitr)
library(papaja)
library(ggthemes)
library(lme4)
library(lmerTest)
library(directlabels)
library(ggrepel)
library(feather)
library(here)
library(scales)
library(tidyboot)
library(broom)
library(broom.mixed)
library(kableExtra)
library(english)
library(ggthemes)
library(tidyverse)

theme_set(theme_few(base_size = 12) + theme(legend.position = "none"))

knitr::opts_chunk$set(fig.pos = 'tb', echo = FALSE, cache = TRUE, 
                      warning = FALSE, message = FALSE, 
                      sanitize = TRUE, fig.path='figs/', fig.width = 3,
                      fig.height = 3)
set.seed(42)
options(digits=3)
```

In just a few short years, children develop a striking mastery of their native language. Undoubtedly, a large share of the credit for this remarkable feat is due to the powerful learning mechanisms that children bring to bear on their input [@decasper1980; @saffran1996]. But, part of the credit may also may also be due to the structure of the language input itself. Indeed, individual differences in the quantity and quality of language children hear reliably related to individual differences in language learning [@hart1995; @huttenlocher2010; @rowe2012]. Further, ambient speech in the child's environment has little predictive power; the child-directed speech that occurs in children's interactions with their caregivers appears to be speech that matters [@weisleder2013; @romeo2018]. What makes child-directed speech so powerful?

Child-directed speech differs from adult-directed along a number of dimension, the majority of which are characterized by simplification [@nelson1989; @snow1977]. But, child-directed speech changes over development, with parents' producing longer and more complex utterances as their children grow older [@huttenlocher2010]. Thus, child-directed speech may support learning not because it is simpler, but instead because it changes as children change: Caregivers may tune their speech to just the right level of complexity for children's ongoing language development [@snow1972; @vygotsky1978]. One possibility is that this tuning might happen at a *coarse* level: Parents might calibrate the global complexity of their speech to their estimate of their child's global linguistic development. Alternatively, parents might *fine*-tune their speech, calibrating the way they talk about specific lexical items to what their children know about those same specific items. Fine-tuned speech would be a much more powerful vehicle for learning.

To date, almost all of the evidence of tuning has been found at a coarse level. For instance, the lengths of parents utterances, their articulation of parents' vowels, and the diversity of clauses in parents' speech change as children's speech changes [@moerk1976; @bernstein-ratner1984; @huttenlocher2010]. The only evidence for fine tuning comes from two corpus studies: One showing that parents are more likely to proivde their child with labels for novel as compared to familiar toys [@masur1997], and the second showing that the lengths of parents utterances in a high-density longitudinal recording dropped to their shortest just before the target child first produced those words.

In this paper, we present the first experimental evidence that parents fine-tune their speech for individual lexical items. Parents played a reference game with their children in which their goal was to get them to pick the correct target animal from a set of three. The length of parents' utterances reflected independent contributions from (1) the difficulty of the target animal word, (2) their global estimate of their child's vocabulary, and (3) their estimate of their child's knowledge of that particular animal. Further, parents sensitively adapted over the course of the reference game, providing more information on subsequent trials when they discovered that their child did not know an animal.

# Method

```{r load_data}
target_data <- read_feather(here("data/target_data.feather")) %>%
    filter(phase != "during", person == "parent", !is.na(understands)) %>%
    complete(nesting(subj, target, understands,  correct,  appearance),
           fill = list(length = 0))

subj_vocab <- read_feather(here("data/subj_vocab.feather"))

demos <- read_feather(here("data/demos.feather"))

tidy_test_data <- read_feather(here("data/qual_model.feather"))

qual_data <- read_feather(here("data/qual_data.feather"))
```

## Participants

```{r youngest_and_oldest}
youngest_child <- demos %>%
  filter(age_years == min(age_years))

oldest_child <- demos %>%
  filter(age_years == max(age_years))

mean_age <- demos %>%
  summarise(age_days = mean(age_years * 365)) %>%
  mutate(age_months = floor(age_days / 30.5),
         age_extra_days = floor(age_days - (age_months * 30.5)))

youngest_chid_months <- youngest_child %>% 
  pull(age_months)
youngest_child_days <- youngest_child %>% 
  pull(age_extra_days)


oldest_child_months <- oldest_child %>% 
  pull(age_months)
oldest_child_days <- oldest_child %>% 
  pull(age_extra_days)

mean_months <- mean_age %>% pull(age_months)
mean_days <- mean_age %>% pull(age_extra_days)

n_girls <- demos %>% 
  summarise(female = sum(demos == "female")) %>% 
  pull %>% 
  words

white_pct <- demos %>% 
  summarise(race = mean(race == "White")) %>% 
  pull(race) %>% 
  percent()

degree_pct <- demos %>% 
  summarise(mom_ed = mean(mom_ed %in% c("4-Year College", 
                                        "Graduate degree"))) %>% 
  pull(mom_ed) %>% 
  percent()
```

Toddlers (aged 2-2.5 years) and their parents were recruited from a database of families in the local community or approached on the floor of a local science museum in order to achieve a planned sample of 40 parent-child dyads. A total of 46 parent-child pairs were recruited, but data from six pairs were dropped from analysis due to experimental error or failure to complete the study. The final sample consisted of `r nrow(demos)` children aged `r youngest_chid_months` mo.; `r youngest_child_days` days to `r oldest_child_months` mo.; `r oldest_child_days` days ($M =$ `r mean_months` mo.; `r mean_days` days), `r n_girls` of whom were girls. 

In our recruitment, we made an effort to sample children from a variety of racial and socio-economic groups. Our final sample was broadly representative of the racial composition of the Chicago Area and the US more broadly (`r white_pct` White). However, our sample was significantly more educated than the broader community (`r degree_pct` of mothers had a College or Graduate Degree).

## Stimuli

```{r aoa_data}
animals <- read_csv(here("corpus_data/predicted_aoas.csv"))%>%
  select(word, aoa) %>%
  left_join(select(subj_vocab, word, type) %>% distinct(), by = "word") %>%
  mutate(type = case_when(
    word == "rooster" ~ "late",
    word == "pig" ~ "early",
    T ~ type)) %>%
  filter(!is.na(type))

min_aoa <- animals %>% 
  pull(aoa) %>% 
  min() %>% 
  round()

max_aoa <- animals %>% 
  pull(aoa) %>% 
  max() %>% 
  round()

early_min_aoa <- animals %>% 
  filter(type == "early") %>% 
  pull(aoa) %>% 
  min() %>% 
  round()

early_max_aoa <- animals %>% 
  filter(type == "early") %>% 
  pull(aoa) %>% 
  max() %>% 
  round()

late_min_aoa <- animals %>% 
  filter(type == "late") %>% 
  pull(aoa) %>% 
  min() %>% 
  round()

late_max_aoa <- animals %>% 
  filter(type == "late") %>% 
  pull(aoa) %>% 
  max() %>% 
  round()

total_vocab <- subj_vocab %>% 
  pull(possible_vocab) %>% 
  mean()
```

Eighteen animal images were selected from the @rossion2004 image set, a colorized version of the @snodgrass1980 object set. Animals were selected based on estimates of their age of acquisition (AoA) for American English learning. To obtain these estimates, we used two sources of information: Parent-report estimates of children's age of acqusition from Wordbank [@frank2017], and retrospective self-report estimates of age of acquisition from adults [@kuperman2012, see Supporting Information for details]. The age of aquisition of the selected animals ranged from `r min_aoa` to `r max_aoa` months. Half of the animals were chosen to have an Early age of acquisition (`r early_min_aoa`-`r early_max_aoa` months), and the other half were chosen to have a Late age of acquisition (`r late_min_aoa`-`r late_max_aoa` months). Each trial featured three animals, all from either the low AoA or high AoA category. 

A modified version of the MacArthur-Bates Communicative Development Inventory [CDI; @fenson2007], a parent-reported measure of children’s vocabulary, was administered before the testing session via an online survey. The selected animal words were embedded among the `r total_vocab` in the survey. Two of the animal words--one in the early AOA (pig) and one in the late AOA category (rooster)--were accidentally omitted, so trials for those words were not included in analysis as we could not obtain individual-level estimates of children's knowledge.

## Design and Procedure

Each parent-child pair played an interactive game using two iPads. Children were given two warm-up trials to get used to the iPads. The practice and experimental trials began after the warm-up. On each trial, three images of animals were displayed side by side on the child’s screen, and a single word appeared on the parent’s screen (Figure \ref{fig:ipads}). Parents were instructed to communicate as they normally would with their child, and encourage them to choose the object corresponding to the word on their screen. The child was instructed to listen to their parent for cues. Once an animal was tapped, the trial ended, and a new trial began. There was a total of 36 experimental trials, such that each animal appeared as the target twice. Trials were randomized for each participant, with the constraint that the same animal could not be the target twice in a row. Practice trials followed the same format as experimental trials, with the exception that images of fruit and vegetables were shown. All sessions were videotaped for transcription and coding.

## Data analysis

The data of interest in this study were parent utterances used during the interactive game and parents’ responses on the adapted CDI. Transcripts of the videos were analyzed for length of referring expressions. We measured the length of parents' referring utterances as a proxy for amount of information given in each utterance. Subsequently, utterances were manually coded for the following: use of canonical labels, basic category labels, subordinate category labels, descriptors, and comparison to other animals. Parent utterances irrelevant to the iPad game (e.g. asking the child to sit down) were not analyzed. Children’s utterances were coded when audible, but were not analyzed.

# Results

```{r word_difficulty}
group_difficulty <- subj_vocab %>%
  group_by(type, word) %>%
  summarise(understands = mean(understands)) %>%
  tidyboot_mean(understands)

mean_word_difficulty <- subj_vocab %>%
  distinct(word, type, avg_known, ci_upper, ci_lower) %>%
  arrange(avg_known)

difficulty_lmer <- subj_vocab %>%
  mutate(type = factor(type, levels = c("early", "late"))) %>%
  glmer(understands ~ type + (type|subj) + (1|word),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group)


difficulty_type_estimate <- difficulty_lmer %>% 
  filter(term == "typelate") %>% 
  pull(estimate)

difficulty_type_statistic <- difficulty_lmer %>%
  filter(term == "typelate") %>% 
  pull(statistic)

difficulty_type_p <- difficulty_lmer %>% 
  filter(term == "typelate") %>% 
  pull(p.value) %>% 
  printp()
```

## Word difficulty. 

```{r descriptives}
early_understood <- group_difficulty %>% 
  filter(type == "early") %>% 
  mutate(empirical_stat = empirical_stat * 100) %>% 
  pull(empirical_stat) %>% 
  round(0)

late_understood <- group_difficulty %>%
  filter(type == "late") %>% 
  mutate(empirical_stat = empirical_stat * 100) %>% 
  pull(empirical_stat) %>% 
  round(0)
```

We first confirm that the animals predicted be later learned were less likely to be marked known by the parents of children in our studies. As predicted, animals in the Early AoA category were judged to be understood by `r early_understood`% of parents, and items in the Late AoA category were judged understood by `r late_understood`%.

The difference between these groups was confirmed statistically with a logistic mixed effects regression (\texttt{correct $\sim$ type + (type | subject) + (1 | word)}). The Late AoA items were judged known by a significantly smaller proportion of parents ($\beta =$ `r difficulty_type_estimate`, $t =$ `r difficulty_type_statistic`, $p$ `r difficulty_type_p`). Parents' judgments for each target word are shown in Figure \ref{fig:difficulty-fig}A.

```{r continuous-plot-data}
continuous_plot_data <- target_data %>%
  filter(phase == "pre") %>%
  group_by(trial_target, subj, appearance) %>%
  summarise(length = sum(length)) %>%
  summarise(length = mean(length)) %>%
  tidyboot_mean(length) %>%
  rename(length = empirical_stat,
         length_upper = ci_upper,
         length_lower = ci_lower) %>%
  select(-mean, -n) %>%
  left_join(mean_word_difficulty, by = c("trial_target" = "word")) %>%
  rename(avg_known_lower = ci_lower, avg_known_upper = ci_upper) %>%
  mutate(plotting_target = if_else(trial_target %in% c("squirrel", "cat",
                                                       "swan"),
                                   trial_target, as.character(NA)))
```

```{r difficulty-fig, fig.width = 4.5, fig.height = 6, fig.cap = "(A) Proportion of parents who reported that their child understood the word for each of our target animals. Colors indicate apriori categorization of words into Early (blue) and Late (red) age of acquisition. (B) Number of words in parents' referential expressions as a function of the proportion of children reported to know the word for target animal. Points show group averaged proportions, error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}
plotting_words <- mean_word_difficulty %>%
  mutate(word = factor(word, levels = unique(word)))

p1 <- ggplot(plotting_words, aes(x = word, y = avg_known, ymin = ci_lower, 
                            ymax = ci_upper, color = type)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  geom_pointrange(size = .3) +
  labs(y = "Parents reporting child knows", x = "") +
  scale_color_ptol()

p2 <- ggplot(continuous_plot_data, aes(x = avg_known, y = length, 
                                 xmin = avg_known_lower,
                          xmax = avg_known_upper, ymin = length_lower,
                          ymax = length_upper,
                          label = plotting_target,
                          color = type)) + 
  geom_smooth(method = "lm", se = F, color = "black") +
  geom_pointrange()+ 
  geom_errorbarh() + 
  geom_label_repel() +
  theme(legend.position = "none") +
  labs(x = "Proportion of children reported knowing target word", 
       y = "Parents' words produced") +
  scale_color_ptol()

ggarrange(p1, p2, nrow = 2, heights = c(1.1, 1), labels = c("a", "b"))
```

## Testing the tuning hypothesis

```{r trial-known-data}
trial_known_data <- target_data %>%
  mutate(phase = factor(phase, levels = c("pre", "post"), 
                        labels = c("before selection", "after selection")),
         understands = factor(understands, labels = c("unknown animal", 
                                                      "known animal"))) %>%
  group_by(phase, avg_known, understands, subj, appearance, trial, trial_target) %>%
  summarise(length = sum(length)) %>%
  left_join(select(subj_vocab, subj, vocab), by = "subj")

mean_known_data <- trial_known_data %>%
  group_by(phase, understands, subj, appearance, trial_target) %>%
  summarise(length = mean(length)) %>%
  summarise(length = mean(length)) %>%
  summarise(length = mean(length)) %>%
  tidyboot_mean(length)
```

```{r primary-lmer}
primary_lmer <- trial_known_data %>%
  ungroup() %>%
  mutate(length = log(length + 1),
         understands = fct_relevel(understands, 
                                   "known animal", "unknown animal")) %>%
  lmer(length ~ appearance * phase  + vocab * phase + understands * phase +
         avg_known * phase +
         (understands | subj) + 
         (appearance | trial_target),
       data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group, -df, -std.error) %>%
  mutate(p.value = printp(p.value))

second_appearance_estimate <- primary_lmer %>% 
  filter(term == "appearancesecond") %>% 
  pull(estimate)

second_appearance_statistic <- primary_lmer %>% 
  filter(term == "appearancesecond") %>% 
  pull(statistic)

second_appearance_p <- primary_lmer %>% 
  filter(term == "appearancesecond") %>% 
  pull(p.value)

avg_known_estimate <- primary_lmer %>% 
  filter(term == "avg_known") %>% 
  pull(estimate)

avg_known_statistic <- primary_lmer %>% 
  filter(term == "avg_known") %>% 
  pull(statistic)

avg_known_p <- primary_lmer %>% 
  filter(term == "avg_known") %>% 
  pull(p.value)

vocab_estimate <- primary_lmer %>% 
  filter(term == "vocab") %>% 
  pull(estimate)

vocab_statistic <- primary_lmer %>% 
  filter(term == "vocab") %>% 
  pull(statistic)

vocab_p <- primary_lmer %>% 
  filter(term == "vocab") %>% 
  pull(p.value)

unknown_animal_estimate <- primary_lmer %>% 
  filter(term == "understandsunknown animal") %>% 
  pull(estimate)

unknown_animal_statistic <- primary_lmer %>% 
  filter(term == "understandsunknown animal") %>% 
  pull(statistic)

unknown_animal_p <- primary_lmer %>% 
  filter(term == "understandsunknown animal") %>% 
  pull(p.value)

after_selection_estimate <- primary_lmer %>% 
  filter(term == "phaseafter selection") %>% 
  pull(estimate)

after_selection_statistic <- primary_lmer %>% 
  filter(term == "phaseafter selection") %>% 
  pull(statistic)

after_selection_p <- primary_lmer %>% 
  filter(term == "phaseafter selection") %>% 
  pull(p.value)

after_selection_second_appearance_estimate <- primary_lmer %>% 
  filter(term == "appearancesecond:phaseafter selection") %>% 
  pull(estimate)

after_selection_second_appearance_statistic <- primary_lmer %>% 
  filter(term == "appearancesecond:phaseafter selection") %>% 
  pull(statistic)

after_selection_second_appearance_p <- primary_lmer %>% 
  filter(term == "appearancesecond:phaseafter selection") %>% 
  pull(p.value)

after_selection_avg_known_estimate <- primary_lmer %>% 
  filter(term == "phaseafter selection:avg_known") %>% 
  pull(estimate)

after_selection_avg_known_statistic <- primary_lmer %>% 
  filter(term == "phaseafter selection:avg_known") %>% 
  pull(statistic)

after_selection_avg_known_p <- primary_lmer %>% 
  filter(term == "phaseafter selection:avg_known") %>% 
  pull(p.value)

after_selection_vocab_estimate <- primary_lmer %>% 
  filter(term == "phaseafter selection:vocab") %>% 
  pull(estimate)

after_selection_vocab_statistic <- primary_lmer %>% 
  filter(term == "phaseafter selection:vocab") %>% 
  pull(statistic)

after_selection_vocab_p <- primary_lmer %>% 
  filter(term == "phaseafter selection:vocab") %>% 
  pull(p.value)

after_selection_unknown_estimate <- primary_lmer %>% 
  filter(term == "phaseafter selection:understandsunknown animal") %>% 
  pull(estimate)

after_selection_unknown_statistic <- primary_lmer %>% 
  filter(term == "phaseafter selection:understandsunknown animal") %>% 
  pull(statistic)

after_selection_unknown_p <- primary_lmer %>% 
  filter(term == "phaseafter selection:understandsunknown animal") %>% 
  pull(p.value)
```

If parents calibrate their referential expressions to their children's linguistic knowledge, they should provide more information to children for whom a simple bare noun (e.g. "leopard") would be insufficient to identify the target. Parents did this in a number of ways: With one or more adjectives (e.g., "the spotted, yellow leopard"), with similes (e.g., "the one that's like a cat"), and with allusions to familiar animal exemplars of the category. In all of these cases, parents would be required to produce more words (see below for further qualitative analyses). Thus, we analyzed the length of parents' referential expressions as a theory-agnostic proxy for informativeness.

If parents tune their referring expressions to children's knowlede, they should produce more informative--and thus longer--referring expressions when they think their children will need them. To test this hypothesis, we divided every trial of the game into two phases: The time before a child selected an animal, and the time following selection until the start of the next trial. We then fit a mixed effects model predicting the number of words parents produced (log), phase (before vs. after selection), target appearance (first vs. second), and three potential measures of tuning: (1) The total number of words the parent thought their child knew, (2) the proportion of all children whose parents reported they knew each target animal, and (3) whether each individual parent thought their child knew each individual word. We also estimated the interaction of each of these variables with phase. We began with a maximal random effect structure and removed random effects until the model converged, prioritizing variables of greatest theoretical for subjects and design-relevant variables for items. The final model included random intercepts and slopes of individual-child knowledge estimates for subjects and random intercepts and slopes of appearance for items. 

Before children had selected an animal, parents produced reliably fewer words on the second appearance of each animal ($\beta =$ `r second_appearance_estimate`, $t =$ `r second_appearance_statistic`, $p$ `r second_appearance_p`), reliably fewer words for animals that more children were reported to know ($\beta =$ `r avg_known_estimate`, $t =$ `r avg_known_statistic`, $p$ `r avg_known_p`), and reliably more words for animals that they believed their individual child did not know ($\beta =$ `r unknown_animal_estimate`, $t =$ `r unknown_animal_statistic`, $p =$ `r unknown_animal_p`). Children's total vocabularies did not reliably affect the number of words parents produced ($\beta =$ `r vocab_estimate`, $t =$ `r vocab_statistic`, $p =$ `r vocab_p`). After children had selected an animal, parents produced reliably fewer words ($\beta =$ `r after_selection_estimate`, $t =$ `r after_selection_statistic`, $p$ `r after_selection_p`), but this reduction was smaller on an animal's second appearance ($\beta =$ `r after_selection_second_appearance_estimate`, $t =$ `r after_selection_second_appearance_statistic`, $p$ `r after_selection_second_appearance_p`), smaller for animals known by more children ($\beta =$ `r after_selection_avg_known_estimate`, $t =$ `r after_selection_avg_known_statistic`, $p$ `r after_selection_avg_known_p`), and bigger for children who knew more words ($\beta =$ `r after_selection_vocab_estimate`, $t =$ `r after_selection_vocab_statistic`, $p$ `r after_selection_vocab_p`). The number of words produced after selection did not vary with parents beliefs about their child's knowledge of that individual animal ($\beta =$ `r after_selection_unknown_estimate`, $t =$ `r after_selection_unknown_statistic`, $p =$ `r after_selection_unknown_p`). Thus, when parents were trying to get their children to select the correct target animal, they provided more information for animals that were generally known by fewer children (coarse tuning; Figure \ref{fig:difficulty-fig}B), but over and above that provided more information for animals that they believed their individual child did not know (fine tuning; \ref{fig:known-fig}A)). In addition, parent produced fewer words after selection for children who knew more words, perhaps because they needed less support and reinforcement.



```{r lag-data}
lag_data <- target_data %>%
  ungroup() %>%
  mutate(understands = factor(understands, levels = c(F, T), 
                              labels = c("unknown animal", 
                                         "known animal"))) %>%
  mutate(correct = factor(correct, levels = c(TRUE, FALSE), 
                          labels = c("correct", "incorrect"))) %>%
  group_by(subj, phase, understands, target, appearance, correct) %>%
  summarise(length = sum(length)) %>%
  ungroup() %>%
  complete(nesting(subj, target, understands,  correct,  appearance),
           fill = list(length = 0)) %>%
  arrange(subj, understands, target,appearance, correct) %>%
  group_by(phase, subj, understands, target) %>%
  mutate(lag_correct = lag(correct)) %>%
  filter(appearance == "second", !is.na(lag_correct)) %>%
  group_by(phase, understands, lag_correct, subj, target) 

mean_lag_data <- lag_data %>%
  summarise(length = sum(length, na.rm = T)) %>%
  summarise(length = mean(length, na.rm = T)) %>%
  tidyboot_mean(length) 
```

```{r lag-lmer}
lag_lmer <- lag_data %>%
  mutate(length = log(length+1)) %>%
  lmer(length ~ lag_correct * understands * phase + 
         ( understands | subj) + (1 | target), 
                 data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group, -df, -std.error) %>%
  mutate(p.value = printp(p.value))


lag_incorrect_estimate <- lag_lmer %>% 
  filter(term == "lag_correctincorrect") %>% 
  pull(estimate)

lag_incorrect_statistic <- lag_lmer  %>% 
  filter(term == "lag_correctincorrect") %>% 
  pull(statistic)

lag_incorrect_p <- lag_lmer  %>% 
  filter(term == "lag_correctincorrect") %>% 
  pull(p.value)

lag_known_estimate <- lag_lmer %>% 
  filter(term == "understandsknown animal") %>% 
  pull(estimate)

lag_known_statistic <- lag_lmer  %>% 
  filter(term == "understandsknown animal") %>% 
  pull(statistic)

lag_known_p <- lag_lmer  %>% 
  filter(term == "understandsknown animal") %>% 
  pull(p.value)

lag_incorrect_known_estimate <- lag_lmer %>% 
  filter(term == "lag_correctincorrect:understandsknown animal") %>% 
  pull(estimate)

lag_incorrect_known_statistic <- lag_lmer  %>% 
  filter(term == "lag_correctincorrect:understandsknown animal") %>% 
  pull(statistic)

lag_incorrect_known_p <- lag_lmer  %>% 
  filter(term == "lag_correctincorrect:understandsknown animal") %>% 
  pull(p.value)

lag_phase_estimate <- lag_lmer %>% 
  filter(term == "phasepost") %>% 
  pull(estimate)

lag_phase_statistic <- lag_lmer  %>% 
  filter(term == "phasepost") %>% 
  pull(statistic)

lag_phase_p <- lag_lmer  %>% 
  filter(term == "phasepost") %>% 
  pull(p.value)

lag_known_phase_estimate <- lag_lmer %>% 
  filter(term == "understandsknown animal:phasepost") %>% 
  pull(estimate)

lag_known_phase_statistic <- lag_lmer  %>% 
  filter(term == "understandsknown animal:phasepost") %>% 
  pull(statistic)

lag_known_phase_p <- lag_lmer  %>% 
  filter(term == "understandsknown animal:phasepost") %>% 
  pull(p.value)

lag_incorrect_known_phase_estimate <- lag_lmer %>%
  filter(term == "lag_correctincorrect:understandsknown animal:phasepost") %>% 
  pull(estimate)

lag_incorrect_known_phase_statistic <- lag_lmer %>%
  filter(term == "lag_correctincorrect:understandsknown animal:phasepost") %>% 
  pull(statistic)

lag_incorrect_known_phase_p <- lag_lmer %>%
  filter(term == "lag_correctincorrect:understandsknown animal:phasepost") %>% 
  pull(p.value)
  

```

We found that parents referential expressions on the second appearance of each animal were affected by both measures of coarse tuning: The child's total vocabulary and the proportion of all children who knew that animal. They were not, however, affected by parents' beliefs about their child's knowledge of that animal. Why not? One possibility is that parents get information from the first appearance of each animal: They may have thought their child knew "leopard," but discovered from their incorrect choice that they did not. If so, they might produce a longer referring expression for the leopard the second time around. To test this hypothesis, we fit a mixed effects model predicted the length of parents' referring expressions on the second appearance of each animal from success on first appearance, phase, (before vs. after selection), whether parents thought their child knew the animal prior to the experiment, and all interactions. We followed the same approach with random effects, beginning with a maximal model and pruning effects until the model converged. The final model included random intercepts and slopes of prior belief by subject and random intercepts and slopes by phase for each animal. Before children had selected a target, parents produced shorter referring expressions when children were incorrect on the first appearance of each animal ($\beta =$ `r lag_incorrect_estimate`, $t =$ `r lag_incorrect_statistic`, $p =$ `r lag_incorrect_p`), and shorter referring expressions for animals that they believed their child knew ($\beta =$ `r lag_known_estimate`, $t =$ `r lag_known_statistic`, $p =$ `r lag_known_p`). However, they produced longer referring expressions following an incorrect response for animals they thought their chilren knew ($\beta =$ `r lag_incorrect_known_estimate`, $t =$ `r lag_incorrect_known_statistic`, $p$ `r lag_incorrect_known_p`). After children had selected a target, parents produced fewer words ($\beta =$ `r lag_phase_estimate`, $t =$ `r lag_phase_statistic`, $p$ `r lag_phase_p`), but this reduction was smaller for animals that their parents thought their children knew when they were correct on the first appearance ($\beta =$ `r lag_known_phase_estimate`, $t =$ `r lag_known_phase_statistic`, $p$ `r lag_known_phase_p`), and reliably longer for animals thought their children knew but were incorrect on the first apperance ($\beta =$ `r lag_incorrect_known_phase_estimate`, $t =$ `r lag_incorrect_known_phase_statistic`, $p$ `r lag_incorrect_known_phase_p`). Thus, when parents thought their children knew an animal, but they observed evidence that they did not, they provided more information in their referential expressions for children to make the correct selection the second time. In fact, parents referential expressions were indistinguishable in length for known and unknown animals when children had incorrectly selected on the first appearance (Figure \ref{fig:known-fig}B). 

Together, these two sets of analyses suggest that parents tune their referential expressions not just coarsely to their knowledge about how hard individual animal words are, or how much language their children generally but know, but also finely to their beliefs about their children's knowledge of individual lexical items. Further, when they discover that they have incorrect beliefs about their children's knowledge, they update these beliefs in real-time and leverage them on subsequent references to the same lexical item.

```{r known-fig, fig.width = 6, fig.height = 3, fig.env = "figure", fig.cap = "(A) Length of parents' references before and after their child selected a target animal. (B) Length of parents' referring expressions on the second appearance of each animal. Points show group averaged proportions; error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}
p3 <- ggplot(mean_known_data,
       aes(x = phase, y = empirical_stat, 
           ymin = ci_lower, ymax = ci_upper, color = understands,
           label = understands)) + 
  geom_pointrange(position = position_dodge(.25)) +
  scale_color_ptol() +
  labs(x = "", y = "Parents' words produced") +
  geom_dl(method = list(dl.trans(x=x +2.2), "first.points", cex=.7)) +
  theme(legend.position = "none")

p4 <- ggplot(mean_lag_data, aes(x = phase, y = empirical_stat, 
           ymin = ci_lower, ymax = ci_upper, color = understands, 
           label = understands)) + 
  facet_wrap(~ lag_correct) +
  geom_pointrange(position = position_dodge(.5), show.legend = F) +
  labs(x ="Child's selection on previous appearance", y = "Parents' words produced") +
  theme(strip.text.x = element_text(size = 7))+
  geom_dl(method = list(dl.trans(x=x +2.2), "first.points", cex=.7)) +
  scale_color_ptol()

ggarrange(p3, p4, nrow = 1, widths = c(1, 1.5), labels = c("a", "b"))
```

## Children's selections.

```{r test-data}
test_data <- target_data %>%
  ungroup() %>%
  mutate(understands = factor(understands, levels = c(T,F), 
                              labels = c("Knows", "Doesn't Know"))) %>%
  filter(phase == "pre", !is.na(understands)) %>%
  group_by(understands, correct,  subj, trial, trial_target, appearance) %>%
  summarise(length = log(sum(length))+1) 

mean_test_data <- test_data %>%
  group_by(understands, subj, trial_target, appearance) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  tidyboot_mean(correct)

test_lmer <- test_data %>%
  mutate(length = log(length+1)) %>%
  glmer(correct ~ length * understands + understands * appearance +
        (1 | subj) + (1 | trial_target), 
      family = "binomial", 
      data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group, -std.error) %>%
  mutate(p.value = printp(p.value))
```

```{r test-model, results="asis", tab.env = "table", eval = F}
model_table <- test_lmer %>%
  select(-effect, -group,-std.error) %>%
  mutate(term = c("intercept", "length (log)", "unknown", 
                  "second appearance", "trial number", "length * unknown"),
         p.value = printp(p.value)) %>%
  rename(`t-value` = statistic,
         `p-value` = p.value) %>%
  xtable(caption = "Coefficient estimates for a mixed-effects logistic regression predicting children's success in selecting the target animal. The model was specified as \\texttt{correct $\\sim$ log(length) * unknown + appearance + trial + (1|subj) + (1|animal)}.",
         label = "tab:test-model")

print(model_table, type = "latex", comment = F, table.placement = "tb",
      include.rownames = FALSE)
```

Overall, children performed significantly above chance for both low AoA and high AoA trials. In our previous analyses, we showed that parents calibrated the length of their referring expressions to their beliefs about their children's knowledge. They did this both in response to their prior beliefs (Figure \ref{fig:known-plot}), and their in-game observations of their children's knowledge (Figure \ref{fig:lag-plot}). In our final analyses, we asked whether this mattered for children's selections. Are children more likely to succeed in the task when parents provide well calibrated utterances? We asked this question by predicting children's selection trial by trial from a mixed effects logistic regression with fixed effects of parents' prior beliefs about children's knowledge of the target animal, whether the trial was the first or second appearance of the the target animal, the length of parents' referring expressions, and the interaction of parents' prior beliefs and the length of their expressions, as well as random effects of subject and trial target. Children were more likely to be correct when their parents produced longer references, but only for animals that their parents believed that they did not know. Thus, parents' informative references to unknown animals did appear to be supporting successful communication of the target animal. Table \ref{tab:test-model} shows coefficient estimates for all parameters.

```{r qual-coding}
#tidy_test_data <- test_data %>%
#  gather(measure, value, num_canonical, num_descriptors, num_comparison,
#         num_subordinate, num_superordinate)

models <- tidy_test_data %>%
  group_by(measure) %>%
  nest() %>%
  mutate(model = map(data, ~glmer(value ~ understands + appearance + length +
                      (1 |id), 
                      family = "binomial", data = .) %>% tidy())) %>%
  select(-data) %>%
  unnest(cols = c(model)) %>%
  filter(effect == "fixed") %>%
  select(-effect)
```


```{r qual-plot, fig.height = 4, fig.width = 8, fig.env = "figure", fig.cap = "Proportion of trials where parents used canonical labels, descriptors, comparisons, subordinate category labels, and basic category labels. Each panel shows proportions for known and unknown animals."}
qual_data %>%
  ungroup() %>%
  gather(measure, value, num_canonical, num_descriptors, num_comparison,
         num_subordinate, num_superordinate) %>%
  mutate(measure = case_when(
    measure == "num_canonical" ~ "Canonical label",
    measure == "num_descriptors" ~ "Descriptors",
    measure =="num_comparison" ~ "Comparison",
    measure =="num_subordinate" ~ "Subordinate",
    measure =="num_superordinate" ~ "Superordinate"),
    measure = factor(measure, levels = c("Comparison", "Descriptors",
                                         "Superordinate", "Subordinate", 
                                         "Canonical label")),
    understands = factor(understands, levels = c(T, F), 
                              labels = c("Known Animal", "Unknown Animal"))) %>%
  group_by(understands, measure) %>%
  tidyboot_mean(value, nboot = 10) %>%
  ggplot(aes(x = understands, y = empirical_stat, 
             ymin = ci_lower, ymax = ci_upper, color = understands)) + 
  labs(x = "Parents' belief about animal word", y = "Proportion of trials") +
  facet_wrap( ~ measure, scales = "free") + 
  geom_pointrange(position = position_dodge(.5)) + 
  theme(legend.position = "none") + 
  scale_color_ptol()
```

### Qualitative Analysis

We then analyzed the content of parents' utterances by calculating the proportion of trials where parents used the following: canonical labels (e.g. peacock), basic level category labels (e.g. bird), subordinate category labels (e.g. Limelight Larry), descriptors or adjectives, and comparison to other animals (e.g. "the one that looks like a cat"). We predicted that parents would use more descriptors, comparisons, as well as basic level category labels for unfamiliar words. On the other hand, we expected parents to use more canonical labels for familiar animals. We did not have a priori predictions about subordinate category label use, as the decision to include subordinate category labels in qualitative analysis arose upon noticing that parents used them to refer to animals during the game. An overview of results can be seen in Figure \ref{fig:qual-plot}.

We looked first at parents' use of canonical labels and subordinate category labels. Our analysis showed no difference in likelihood of using a canonical label in trials with familiar and unfamiliar animals. Instead, parents used canonical labels on most of the trials. However, we found that parents used more subordinate category labels (such as proper nouns) for words that they believe their children knew ($\beta =$ `r models %>% filter(measure == "num_subordinate") %>% filter(term == "understandsTRUE") %>% pull(estimate)`, $t =$ `r models %>% filter(measure == "num_subordinate") %>% filter(term == "understandsTRUE") %>% pull(statistic)`, $p =$ `r models %>% filter(measure == "num_subordinate") %>% filter(term == "understandsTRUE") %>% pull(p.value) %>% printp()`).

We then looked at how often parents used descriptors, comparisons, and basic level category labels. Analyses revealed patterns in line with our predictions. Parents used more descriptors ($\beta =$ `r models %>% filter(measure == "num_descriptors") %>% filter(term == "understandsTRUE") %>% pull(estimate)`, $t =$ `r models %>% filter(measure == "num_descriptors") %>% filter(term == "understandsTRUE") %>% pull(statistic)`, $p =$ `r models %>% filter(measure == "num_descriptors") %>% filter(term == "understandsTRUE") %>% pull(p.value) %>% printp()`), comparisons to other animals ($\beta =$ `r models %>% filter(measure == "num_comparison") %>% filter(term == "understandsTRUE") %>% pull(estimate)`, $t =$ `r models %>% filter(measure == "num_comparison") %>% filter(term == "understandsTRUE") %>% pull(statistic)`, $p =$ `r models %>% filter(measure == "num_comparison") %>% filter(term == "understandsTRUE") %>% pull(p.value) %>% printp()`), and basic level category labels ($\beta =$ `r models %>% filter(measure == "num_superordinate") %>% filter(term == "understandsTRUE") %>% pull(estimate)`, $t =$ `r models %>% filter(measure == "num_superordinate") %>% filter(term == "understandsTRUE") %>% pull(statistic)`, $p =$ `r models %>% filter(measure == "num_superordinate") %>% filter(term == "understandsTRUE") %>% pull(p.value) %>% printp()`) when they believed their children did not know the target word. 

# Discussion

Parents have a wealth of knowledge about their kids, including their linguistic development [@fenson2007]. Do they draw on this knowledge when they want to communicate? In a referential communication task, we showed that parents speak differently depending on their beliefs about their children's vocabulary knowledge. Specifically, they produce shorter, less informative expressions to refer to animals that they believe their children know relative to animals that they think their children do not know. Further, parents update their beliefs during the course of the task, producing more informative expressions on the second appearance of an animal they previously thought their children knew if they observed evidence to the contrary (i.e. when children selected the wrong animal). We further found that more informative referring expressions were associated with increased likelihood of successful communication: Children were more likely to correctly select animals whose names they did not know if their parents produced longer utterances to refer to them. We leveraged length as a proxy for informativeness in parents' expressions in the service of quantitative, theory-agnostic predictions. In ongoing work, we are analyzing *how* parents succeed on these trials, and investigating whether different strategies lead to different levels of success. 

In general, communicative success was high. Children selected the correct animal at above chance levels, even for targets whose names their parents thought they did not know. Because easy and hard animals appeared on separate trials, children's high accuracy in selecting unfamiliar animals is unlikely to be due to the use of strategies like mutual exclusivity [@markman1988]. Instead, parents must have produced sufficient information for their children to find the correct target. Taken together with our finding that parents used longer sentences for words they think their children do not know, our results suggest that parents modified their speech as a means to communicate. 

Our proposed explanation for these results is that they are produced by a pressure for effective communication: Parents need to produce sufficient information for their children to understand their intended meaning. That is, parents design their utterances for their children's benefit [speaker-design, @jaeger2013]. It could be instead that these utterances reflect pressure from speaking itself. For example, length of parents' utterances may reflect their difficulty in retrieving certain animal words [@macdonald2013]. We find this explanation unlikely given that parents were given the target words in written form on their iPad, essentially eliminating retrieval problems [@wingfield1968]. The fact that parents are using long and short referring expressions depending on their beliefs about children's vocabulary knowledge suggests that they are calibrating to their children.

Parents also modify the *content* of their speech. When talking about animals that they believe their children do not know, parents use more adjectives, comparison to other animals, and basic level category labels. These findings are in line with our predictions, and suggest that parents can use various strategies to ensure successful communication. By providing qualitatively different information, parents can guide their children to the correct animal, even if children do not know the canonical label for that animal. Contrary to our predictions, parents did not use more canonical labels for familiar animals. Parents used canonical labels on most of the trials, regardless of whether they believed their children knew the target word. This could be due to the fact that using the canonical label is not costly for the parents, even if the canonical label itself may be insufficient in guiding the child to select the correct animal. On the other hand, parents did use more subordinate category labels for familiar animals. In our sample, most of the subordinate category labels were proper nouns, such as character names from books or family pets. This shows that parents are not only sensitive to whether their children know a particular animal word, but also the particular animals or characters that their children associate an animal with. It is unlikely that a parent would say "Limelight Larry" instead of "peacock" when speaking to other adults, or even other children. Our findings therefore provide solid evidence that parents are sensitive to their children's knowledge, and can adapt their speech accordingly in order to achieve successful communication.

It is important to note that our current results do not completely rule out the possibility that parents are engaging in pedagogy. Parents may be introducing more information into their referring expressions because they wish to teach their children certain words, which is a potential explanation for why parents adapt the content of their speech when talking about animals their children do not know. The use of adjectives (e.g. "red lobster"), basic level category labels (e.g. "blue bird" for peacock), and comparison to other animals (e.g. "the donkey, it looks like a horse") could all reflect intentions to teach children about different animals. However, within the context of the game, these strategies also serve (at least in part) to facilitate successful communication. In the lobster example, the color "red" is likely a helpful cue for children, and parents may be using adjectives as a way to help children select the correct target quickly.

We would also like to acknowledge that our study used a WEIRD (@henrich2010) sample, and thus our results may not fully generalize to other populations. Language development is influenced by a variety of cultural, socio-economic, and environmental factors, and our findings do not account for many of these variables. However, we believe that our work still holds importance when thinking about langauge development in general. Our work focuses on the communicative aspect of language, and we believe that communication is necessary for users of any and all languages. Our study shows that the desire for effective communication can drive parents to modify their spoken language, and we believe this core finding would translate well to other populations, though the specific modifications may vary.

Our work contributes to the current literature on parent-child interaction, and forms the basis for further experimental work examining the influences that parent speech has on children’s language development. In line with @masur1997, our findings provide evidence that parents calibrate speech sensitively to their children's vocabulary knowledge. These results are important in light of previous work suggesting that parent responsiveness and sensitivity shape the way young children learn language [@hoff-ginsberg1982; @tamis-lemonda2014]. Furthermore, we propose that parents are modifying their speech as a means to communicate, and that communicative intent shapes the language environments children experience. 

Finally, this study highlights the importance of studying the parent-child pair as a unit, rather than viewing children as isolated learners: both parents and children contribute to the process of language development [@hoff-ginsberg1982; @brown1977]. Focusing on the interactive and communicative nature of language captures a more realistic picture of children's language environments: The input that children receive is not random – it is sensitive to their developmental level.

# Acknowledgements

This research was funded by a James S. McDonnell Foundation Scholar Award to DY.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
