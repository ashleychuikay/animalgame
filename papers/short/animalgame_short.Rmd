---
title             : "Parents Fine-tune Their Speech to Children's Vocabularies"
shorttitle        : "Parents Fine-tune speech"

author: 
  - name          : "Ashley Leung"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "5848 S University Ave, Chicago IL, 60637"
    email         : "ashleyleung@uchicago.edu"
  - name          : "Alexandra Tunkel"
    affiliation   : "1"
  - name          : "Daniel Yurovsky"
    affiliation   : "1,2"

affiliation:
  - id            : "1"
    institution   : "The University of Chicago"
  - id            : "2"
    institution   : "Carnegie Mellon University"

abstract: |
   Young children learn language at an incredible rate. While children come prepared with powerful statistical learning mechanisms, the statistics they encounter are also prepared for them: Children learn from caregivers motivated to communicate with them. How do caregivers adapt their speech in order to support children's comprehension? We asked children and their parents to play a simple reference game in which the parent's goal was to guide their child to select a target animal from a set of three. We show that parents fine-tune their referring expressions to their children's knowledge at the lexical level, producing more informative references for animals that they thought their children did not know. Further, parents learn about their children's knowledge over the course of the game, and tune their referring expressions accordingly. Child-directed speech may thus support children's learning not because it is uniformly simpler than adult-directed speech, but because it is tuned to individual children's language development.

authornote: |
  Parts of this work were presented at the Annual Conference of the Cognitive Science Society: Leung et al. (2019). All data and code for these analyses are available at https://github.com/ashleychuikay/animalgame.

keywords          : "parent-child interaction; language development; communication"
wordcount         : "854"
references         : "23"

bibliography      : ["animalgame.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r load_packages, include = FALSE}
library(png)
library(grid)
library(egg)
library(xtable)
library(knitr)
library(papaja)
library(ggthemes)
library(lme4)
library(lmerTest)
library(directlabels)
library(ggrepel)
library(feather)
library(here)
library(scales)
library(tidyboot)
library(broom)
library(broom.mixed)
library(kableExtra)
library(english)
library(ggthemes)
library(tidyverse)

theme_set(theme_few(base_size = 12) + theme(legend.position = "none"))

knitr::opts_chunk$set(fig.pos = 'tb', echo = FALSE, cache = TRUE, 
                      warning = FALSE, message = FALSE, 
                      sanitize = TRUE, fig.path='figs/', fig.width = 3,
                      fig.height = 3)
set.seed(42)
options(digits=3)
```

In just a few short years, children develop a striking mastery of their native language. Undoubtedly, a large share of the credit for this remarkable feat is due to the powerful learning mechanisms that children bring to bear on their input [@decasper1980; @saffran1996]. However, part of the credit may also be due to the structure of the language input itself. In line with this hypothesis, individual differences in the quantity and quality of language children hear are reliably related to individual differences in language learning [@hart1995; @huttenlocher2010; @rowe2012]. Critically, this relationship is driven by speech directed to children in interactions with their caregivers; differences in overheard speech do not predict differences in language learning even in communities where child-directed speech is relatively rare [@weisleder2013; @romeo2018; @schneidman2012]. What makes child-directed speech so powerful?

Child-directed speech differs from adult-directed speech along a number of dimensions, the majority of which are characterized by simplification [@nelson1989; @snow1977]. But, child-directed speech also changes over development, with parents producing longer and more complex utterances as children develop [@huttenlocher2010]. Child-directed speech may scaffold learning not because it is simpler, but because it is adaptive--Caregivers may tune their speech to the right level of complexity for children's ongoing language development [@snow1972; @vygotsky1978]. One possibility is that parents *coarse-tune* their speech, calibrating global complexity to children's global linguistic development. Alternatively, parents could *fine-tune* their speech, calibrating the complexity of language containing specific lexical items to children's knowledge of those same lexical items. Fine-tuned speech would be a particulary powerful vehicle for language learning, and thus an important part of the explanation for rapid children's rapid language learning [@bruner1983].

Parents have been shown to coarse-tune several aspects of child-directed speech: the lengths of utterances, the articulation of vowels, and the diversity of clauses in parents' speech change over children's language development. [@moerk1976; @bernstein-ratner1984; @huttenlocher2010]. However, the only evidence for fine-tuning comes from two observational studies, one showing that parents are more likely to provide their child with labels for novel as compared to familiar toys [@masur1997], and the second showing that the lengths of caregivers' utterances containing a particular word are shortest just before the target child first produces that word [@roy2009].

Here, we present the first experimental evidence for fine-tuning. We asked parents and their children to play a reference game in which the parent's goal was to guide their child to select a target animal from a set of three. Parents tuned the amount of information in their utterances not just to the average difficulty of each animal word, but to their prior estimates of their individual child's knowledge of that animal. Further, parents sensitively adapted over the course of the reference game, providing more information on subsequent trials when they discovered that their child did not know an animal. Together, these results show that parents leverage their considerable knowledge of their children's language development to fine-tune the information they provide.

# Method

```{r load_data}
target_data <- read_feather(here("data/target_data.feather")) %>%
    filter(phase != "during", person == "parent", !is.na(understands)) %>%
    complete(nesting(subj, target, understands,  correct,  appearance),
           fill = list(length = 0))

subj_vocab <- read_feather(here("data/subj_vocab.feather"))

demos <- read_feather(here("data/demos.feather"))

tidy_test_data <- read_feather(here("data/qual_model.feather"))

qual_data <- read_feather(here("data/qual_data.feather"))
```

## Participants

```{r youngest_and_oldest}
youngest_child <- demos %>%
  filter(age_years == min(age_years))

oldest_child <- demos %>%
  filter(age_years == max(age_years))

mean_age <- demos %>%
  summarise(age_days = mean(age_years * 365)) %>%
  mutate(age_months = floor(age_days / 30.5),
         age_extra_days = floor(age_days - (age_months * 30.5)))

youngest_chid_months <- youngest_child %>% 
  pull(age_months)
youngest_child_days <- youngest_child %>% 
  pull(age_extra_days)


oldest_child_months <- oldest_child %>% 
  pull(age_months)
oldest_child_days <- oldest_child %>% 
  pull(age_extra_days)

mean_months <- mean_age %>% pull(age_months)
mean_days <- mean_age %>% pull(age_extra_days)

n_girls <- demos %>% 
  summarise(female = sum(demos == "female")) %>% 
  pull %>% 
  words

white_pct <- demos %>% 
  summarise(race = mean(race == "White")) %>% 
  pull(race) %>% 
  percent()

degree_pct <- demos %>% 
  summarise(mom_ed = mean(mom_ed %in% c("4-Year College", 
                                        "Graduate degree"))) %>% 
  pull(mom_ed) %>% 
  percent()
```

Toddlers (aged 2-2.5 years) and their parents were recruited from a database of families in the local community or approached on the floor of a local science museum in order to achieve a planned sample of 40 parent-child dyads. A total of 48 parent-child pairs were recruited, but data from 7 pairs were dropped from analysis because of failure to complete the experiment as designed. The final sample consisted of `r nrow(demos)` children aged `r youngest_chid_months` mo.; `r youngest_child_days` days to `r oldest_child_months` mo.; `r oldest_child_days` days ($M =$ `r mean_months` mo.; `r mean_days` days), `r n_girls` of whom were girls. 

In our recruitment, we made an effort to sample children from a variety of racial and socio-economic groups. Our final sample was broadly representative of the racial composition of the Chicago Area and the US more broadly (`r white_pct` White). However, our sample was significantly more educated than the broader community (`r degree_pct` of mothers had a College or Graduate Degree).

## Stimuli

```{r aoa_data}
animals <- read_csv(here("corpus_data/predicted_aoas.csv"))%>%
  select(word, aoa) %>%
  left_join(select(subj_vocab, word, type) %>% distinct(), by = "word") %>%
  mutate(type = case_when(
    word == "rooster" ~ "late",
    word == "pig" ~ "early",
    T ~ type)) %>%
  filter(!is.na(type))

min_aoa <- animals %>% 
  pull(aoa) %>% 
  min() %>% 
  round()

max_aoa <- animals %>% 
  pull(aoa) %>% 
  max() %>% 
  round()

early_min_aoa <- animals %>% 
  filter(type == "early") %>% 
  pull(aoa) %>% 
  min() %>% 
  round()

early_max_aoa <- animals %>% 
  filter(type == "early") %>% 
  pull(aoa) %>% 
  max() %>% 
  round()

late_min_aoa <- animals %>% 
  filter(type == "late") %>% 
  pull(aoa) %>% 
  min() %>% 
  round()

late_max_aoa <- animals %>% 
  filter(type == "late") %>% 
  pull(aoa) %>% 
  max() %>% 
  round()

total_vocab <- subj_vocab %>% 
  pull(possible_vocab) %>% 
  mean()
```

Eighteen animal images were selected from the @rossion2004 image set, a colorized version of the @snodgrass1980 object set. Animals were selected based on estimates of their age of acquisition (AoA) for American English learners. To obtain these estimates, we used two sources of information: parent-report estimates of children's age of acqusition from Wordbank [@frank2017], and retrospective self-report estimates of AoA from adults [@kuperman2012, see Supporting Information for details]. The AoA of the selected animals ranged from `r min_aoa` to `r max_aoa` months. Half of the animals were chosen to have an Early age of acquisition (`r early_min_aoa`-`r early_max_aoa` months), and the other half were chosen to have a Late age of acquisition (`r late_min_aoa`-`r late_max_aoa` months). Each trial featured three animals, all from either the low AoA or high AoA category. This separation was designed to lower the likelihood that children could use knowledge of low AoA animals to infer the correct target on high AoA trials.

A modified version of the MacArthur-Bates Communicative Development Inventory [CDI; @fenson2007], a parent-reported measure of children's vocabulary, was administered before the testing session via an online survey. The selected animal words were embedded among the `r total_vocab` in the survey. Two of the animal words--one in the early AOA (pig) and one in the late AOA category (rooster)--were accidentally omitted, so trials for those words were not included in analysis as we could not obtain individual-level estimates of children's knowledge.

## Design and Procedure

Each parent-child pair played an interactive game using two iPads. Children began with two warm-up trials on which they tapped on circles that appeared on the iPads. Following these warm-up trials, children and their parents moved onto practice and then experimental trials. On each trial, three images of animals were displayed side by side on the child's screen, and a single word appeared on the parent's screen. Parents were instructed to communicate as they normally would with their child, and to encourage their child to choose the object corresponding to the word on their screen. The child was instructed to listen to their parent for cues. Once the child tapped an animal, the trial ended, and a new trial began. There were a total of 36 experimental trials, such that each animal appeared as the target twice. Trials were randomized for each participant, with the constraint that the same animal could not be the target twice in a row. Practice trials followed the same format as experimental trials, with the exception that images of fruit and vegetables were shown. All sessions were videotaped for transcription and coding.

## Data analysis

Ou primary quantity of interest was the amount of information that parents provided in each of their utterances. To approximate this, we measured the length of parents' referring expressions--the number of words they produced on each trial before their child selected an animal. Length is an imperfect proxy for information, but it is easy to quantify and theory-agnostic. 

Subsequently, utterances were manually coded for the following: use of canonical labels, basic category labels, subordinate category labels, descriptors, and comparison to other animals. Parent utterances irrelevant to the game (e.g. asking the child to sit down) were not analyzed. Children's utterances were coded when audible, but were not analyzed. 

Our second source of data was the vocabulary questionnaire that parents filled out prior to participation. Parents indicated whether their child produced each of the 85 words on the survey. In addition to analyzing parents' judgments for the animals in the task, we also computed the total number of words judged to be known for each child as a proxy for total vocabulary.

# Results

We begin by confirming that our a priori divisions of animals into low and high age of acquisition in the study design were reflected in parents' survey judgments, and that children were able to follow parents' references to select the correct target animal on each trial. After this, we present several tests of the fine-tuning hypothesis. 

```{r word_difficulty}
group_difficulty <- subj_vocab %>%
  group_by(type, word) %>%
  summarise(understands = mean(understands)) %>%
  tidyboot_mean(understands)

mean_word_difficulty <- subj_vocab %>%
  distinct(word, type, avg_known, ci_upper, ci_lower) %>%
  arrange(avg_known)

difficulty_lmer <- subj_vocab %>%
  mutate(type = factor(type, levels = c("early", "late"))) %>%
  glmer(understands ~ type + (type|subj) + (1|word),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group)


difficulty_type_estimate <- difficulty_lmer %>% 
  filter(term == "typelate") %>% 
  pull(estimate)

difficulty_type_statistic <- difficulty_lmer %>%
  filter(term == "typelate") %>% 
  pull(statistic)

difficulty_type_p <- difficulty_lmer %>% 
  filter(term == "typelate") %>% 
  pull(p.value) %>% 
  printp()
```

## Target animal difficulty

```{r descriptives}
early_understood <- group_difficulty %>% 
  filter(type == "early") %>% 
  mutate(empirical_stat = empirical_stat * 100) %>% 
  pull(empirical_stat) %>% 
  round(0)

late_understood <- group_difficulty %>%
  filter(type == "late") %>% 
  mutate(empirical_stat = empirical_stat * 100) %>% 
  pull(empirical_stat) %>% 
  round(0)
```

We first confirm that the animals predicted to be later learned were less likely to be marked known by the parents of children in our studies. As predicted, animals in the Early AoA category were judged to be understood by `r early_understood`% of parents, and items in the Late AoA category were judged understood by `r late_understood`%. We confirmed this difference statistically with a mixed effects logistic regression, predicting success on each trial from a fixed effect of type and a random intercept and slope of type by subject as well as a random intercept for each animal. The Late AoA items were judged known by a significantly smaller proportion of parents ($\beta =$ `r difficulty_type_estimate`, $t =$ `r difficulty_type_statistic`, $p$ `r difficulty_type_p`). Parents' judgments for each target word are shown in Figure \ref{fig:difficulty-fig}A.

## Selection accuracy

```{r test-data}
test_data <- target_data %>%
  ungroup() %>%
  mutate(understands = factor(understands, levels = c(T,F), 
                              labels = c("Knows", "Doesn't Know"))) %>%
  filter(phase == "pre", !is.na(understands)) %>%
  group_by(understands, correct,  subj, trial, trial_target, appearance) %>%
  summarise(length = log(sum(length))+1)  %>%
  left_join(select(subj_vocab, subj, vocab), by = "subj")

overall_correct <- test_data %>%
  group_by(subj, trial_target, appearance) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  mutate(correct = correct * 100) %>%
  pull()

type_correct <- test_data %>%
  group_by(understands, subj, trial_target, appearance) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  mutate(correct = correct * 100)

known_correct <- type_correct %>% 
  filter(understands == "Knows") %>%
  pull()

unknown_correct <- type_correct %>% 
  filter(understands == "Doesn't Know") %>%
  pull()


overall_acc_model <- glmer(correct ~ 1 + offset(base) + (1|subj) + (1|trial_target),
      data = test_data %>% mutate(base = log(1/3)), family = "binomial") %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group, -std.error) %>%
  mutate(p.value = printp(p.value))

overall_acc_estimate <- overall_acc_model %>%
  pull(estimate)

overall_acc_statistic <- overall_acc_model %>%
  pull(statistic)

overall_acc_p <- overall_acc_model %>%
  pull(p.value)

type_acc_model <- test_data %>% 
  mutate(base = log(1/3)) %>%
  group_by(understands)  %>%
  nest() %>%
  mutate(model = map(data, ~glmer(correct ~ 1 + offset(base) + (1|subj) +
                                    (1|trial_target),
                                  data =. , family = "binomial") %>% 
                       tidy())) %>%
  select(-data) %>%
  unnest() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group, -std.error) %>%
  mutate(p.value = printp(p.value))


known_acc_estimate <- type_acc_model %>%
  filter(understands == "Knows") %>%
  pull(estimate)

known_acc_statistic <- type_acc_model %>%
  filter(understands == "Knows") %>%
  pull(statistic)

known_acc_p <- type_acc_model %>%
  filter(understands == "Knows") %>%
  pull(p.value)

unknown_acc_estimate <- type_acc_model %>%
  filter(understands == "Doesn't Know") %>%
  pull(estimate)

unknown_acc_statistic <- type_acc_model %>%
  filter(understands == "Doesn't Know") %>%
  pull(statistic)

unknown_acc_p <- type_acc_model %>%
  filter(understands == "Doesn't Know") %>%
  pull(p.value)

test_lmer <- test_data %>%
  mutate(length = log(length+1)) %>%
  glmer(correct ~ length * understands + understands * appearance + 
          scale(vocab) * understands +
        (understands | subj) + (1 | trial_target), 
      family = "binomial", 
      data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group, -std.error) %>%
  mutate(p.value = printp(p.value))


test_length_estimate <- test_lmer %>%
  filter(term == "length") %>%
  pull(estimate)

test_length_statistic <- test_lmer %>%
  filter(term == "length") %>%
  pull(statistic)

test_length_p <- test_lmer %>%
  filter(term == "length") %>%
  pull(p.value)

test_unknown_estimate <- test_lmer %>%
  filter(term == "understandsDoesn't Know") %>%
  pull(estimate)

test_unknown_statistic <- test_lmer %>%
  filter(term == "understandsDoesn't Know") %>%
  pull(statistic)

test_unknown_p <- test_lmer %>%
  filter(term == "understandsDoesn't Know") %>%
  pull(p.value)

test_appearance_estimate <- test_lmer %>%
  filter(term == "appearancesecond") %>%
  pull(estimate)

test_appearance_statistic <- test_lmer %>%
  filter(term == "appearancesecond") %>%
  pull(statistic)

test_appearance_p <- test_lmer %>%
  filter(term == "appearancesecond") %>%
  pull(p.value)

test_vocab_estimate <- test_lmer %>%
  filter(term == "scale(vocab)") %>%
  pull(estimate)

test_vocab_statistic <- test_lmer %>%
  filter(term == "scale(vocab)") %>%
  pull(statistic)

test_vocab_p <- test_lmer %>%
  filter(term == "scale(vocab)") %>%
  pull(p.value)

test_length_unknown_estimate <- test_lmer %>%
  filter(term == "length:understandsDoesn't Know") %>%
  pull(estimate)

test_length_unknown_statistic <- test_lmer %>%
  filter(term == "length:understandsDoesn't Know") %>%
  pull(statistic)

test_length_unknown_p <- test_lmer %>%
  filter(term == "length:understandsDoesn't Know") %>%
  pull(p.value)

test_appearance_unknown_estimate <- test_lmer %>%
  filter(term == "understandsDoesn't Know:appearancesecond") %>%
  pull(estimate)

test_appearance_unknown_statistic <- test_lmer %>%
  filter(term == "understandsDoesn't Know:appearancesecond") %>%
  pull(statistic)

test_appearance_unknown_p <- test_lmer %>%
  filter(term == "understandsDoesn't Know:appearancesecond") %>%
  pull(p.value)

test_vocab_unknown_estimate <- test_lmer %>%
  filter(term == "understandsDoesn't Know:scale(vocab)" ) %>%
  pull(estimate)

test_vocab_unknown_statistic <- test_lmer %>%
  filter(term == "understandsDoesn't Know:scale(vocab)" ) %>%
  pull(statistic)

test_vocab_unknown_p <- test_lmer %>%
  filter(term == "understandsDoesn't Know:scale(vocab)") %>%
  pull(p.value)
```

On the whole, parents communicated effectively with their children, such that children selected the correct target on `r overall_correct`\% of trials. To determine whether this was reliably greater than we would expect by chance (33\%), we fit a mixed effects logistic regression predicting whether each selection was correct from a fixed intercept, random intercepts for subjects and animals, and an offset of log(1/3) so that the intercept estimated difference from chance. The intercept was significantly greater than 0 ($\beta =$  `r overall_acc_estimate`, $t =$  `r overall_acc_statistic`, $p$ `r overall_acc_p`), indiciating that children were selecting the correct animal at greater than chance levels. Children were above chance both for animals that parents thought they knew ($M =$ `r known_correct`, $\beta =$  `r known_acc_estimate`, $t =$  `r known_acc_statistic`, $p$ `r known_acc_p`), and for animals that parents thought their children did not know ($M =$ `r unknown_correct`, $\beta =$  `r unknown_acc_estimate`, $t =$  `r unknown_acc_statistic`, $p =$ `r unknown_acc_p`). Thus, parents successfully communicated the target referent to children, even when parents thought their children did not know the name for the animal at the start of the game. 

To determine whether variation in parents' utterances impacted children's selections, we fit a mixed effects logistic regression predicting whether children were successful on each trial from the (log) length of parents' referring expression, parents' estimate of whether their child knew the animal, the appearance number of the target animal (first vs. second), the child's (scaled) total vocabulary, and interactions between parents' estimates of whether their children knew the animal and length, apperance, and vocabulary. We began with a maximal random effect structure and removed interactions until the model converged, leading to random intercepts and slopes of whether the child knew the animal for each subject and random intercepts for animals. For known animals, this model showed children were less accurate for longer utterances ($\beta =$  `r test_length_estimate`, $t =$  `r test_length_statistic`, $p$ `r test_length_p`), animals that parents thought they did not know ($\beta =$  `r test_unknown_estimate`, $t =$  `r test_unknown_statistic`, $p$ `r test_unknown_p`), and on second appearances ($\beta =$  `r test_appearance_estimate`, $t =$  `r test_appearance_statistic`, $p$ `r test_appearance_p`). Children with bigger vocabularies were more accurate ($\beta =$  `r test_vocab_estimate`, $t =$  `r test_vocab_statistic`, $p$ `r test_vocab_p`). For unknown animals, children were more accurate when parents produced longer utterances ($\beta =$  `r test_length_unknown_estimate`, $t =$  `r test_length_unknown_statistic`, $p$ `r test_length_unknown_p`), on an animal's second appearance ($\beta =$  `r test_appearance_unknown_estimate`, $t =$  `r test_appearance_unknown_statistic`, $p$ `r test_appearance_unknown_p`), and the effect of having a larger vocabulary was reduced ($\beta =$  `r test_vocab_unknown_estimate`, $t =$  `r test_vocab_unknown_statistic`, $p =$ `r test_vocab_unknown_p`). Thus, longer utterances were associated with more successful referential communication for animals that children did not know, but were unhelpful for animals that they did know. We next ask whether parents tuned the lengths of their utterances appropriately, producing longer utterances for unknown animals.



```{r continuous-plot-data}
continuous_plot_data <- target_data %>%
  filter(phase == "pre") %>%
  group_by(trial_target, subj, appearance) %>%
  summarise(length = sum(length)) %>%
  summarise(length = mean(length)) %>%
  tidyboot_mean(length) %>%
  rename(length = empirical_stat,
         length_upper = ci_upper,
         length_lower = ci_lower) %>%
  select(-mean, -n) %>%
  left_join(mean_word_difficulty, by = c("trial_target" = "word")) %>%
  rename(avg_known_lower = ci_lower, avg_known_upper = ci_upper) %>%
  mutate(plotting_target = if_else(trial_target %in% c("squirrel", "cat",
                                                       "swan"),
                                   trial_target, as.character(NA)))
```

```{r difficulty-fig, fig.width = 4.5, fig.height = 6, fig.cap = "(A) Proportion of parents who reported that their child understood the word for each of our target animals. Colors indicate a priori categorization of words into Early (blue) and Late (red) age of acquisition. (B) Number of words in parents' referring expressions as a function of the proportion of children reported to know the word for target animal. Points show group averaged proportions, error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}
# plotting_words <- mean_word_difficulty %>%
#   mutate(word = factor(word, levels = unique(word)))
# 
# p1 <- ggplot(plotting_words, aes(x = word, y = avg_known, ymin = ci_lower, 
#                             ymax = ci_upper, color = type)) + 
#   theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
#   geom_pointrange(size = .3) +
#   labs(y = "Parents reporting child knows", x = "") +
#   scale_color_ptol()

ggplot(continuous_plot_data, aes(x = avg_known, y = length, 
                                 xmin = avg_known_lower,
                          xmax = avg_known_upper, ymin = length_lower,
                          ymax = length_upper,
                          label = plotting_target,
                          color = type)) + 
  geom_smooth(method = "lm", se = F, color = "black") +
  geom_pointrange()+ 
  geom_errorbarh() + 
  geom_label_repel() +
  theme(legend.position = "none") +
  labs(x = "Proportion of children reported knowing target word", 
       y = "Length of referring expressions") +
  scale_color_ptol()

#ggarrange(p1, p2, nrow = 2, heights = c(1.1, 1), labels = c("a", "b"))
```

## Tuning

```{r trial-known-data}
trial_known_data <- target_data %>%
  mutate(phase = factor(phase, levels = c("pre", "post")),
         understands = factor(understands, labels = c("unknown animal", 
                                                      "known animal"))) %>%
  group_by(phase, avg_known, understands, subj, appearance, trial, trial_target) %>%
  summarise(length = sum(length)) %>%
  left_join(select(subj_vocab, subj, vocab), by = "subj")

mean_known_data <- trial_known_data %>%
  group_by(phase, understands, subj, appearance, trial_target) %>%
  summarise(length = mean(length)) %>%
  summarise(length = mean(length)) %>%
  summarise(length = mean(length)) %>%
  tidyboot_mean(length)
```

```{r primary-lmer}
primary_lmer <- trial_known_data %>%
  ungroup() %>%
  mutate(length = log(length + 1),
         understands = fct_relevel(understands, 
                                   "known animal", "unknown animal")) %>%
  lmer(length ~ appearance * phase  + vocab * phase + understands * phase +
         avg_known * phase +
         (understands | subj) + 
         (appearance | trial_target),
       data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group, -df, -std.error) %>%
  mutate(p.value = printp(p.value))

second_appearance_estimate <- primary_lmer %>% 
  filter(term == "appearancesecond") %>% 
  pull(estimate)

second_appearance_statistic <- primary_lmer %>% 
  filter(term == "appearancesecond") %>% 
  pull(statistic)

second_appearance_p <- primary_lmer %>% 
  filter(term == "appearancesecond") %>% 
  pull(p.value)

avg_known_estimate <- primary_lmer %>% 
  filter(term == "avg_known") %>% 
  pull(estimate)

avg_known_statistic <- primary_lmer %>% 
  filter(term == "avg_known") %>% 
  pull(statistic)

avg_known_p <- primary_lmer %>% 
  filter(term == "avg_known") %>% 
  pull(p.value)

vocab_estimate <- primary_lmer %>% 
  filter(term == "vocab") %>% 
  pull(estimate)

vocab_statistic <- primary_lmer %>% 
  filter(term == "vocab") %>% 
  pull(statistic)

vocab_p <- primary_lmer %>% 
  filter(term == "vocab") %>% 
  pull(p.value)

unknown_animal_estimate <- primary_lmer %>% 
  filter(term == "understandsunknown animal") %>% 
  pull(estimate)

unknown_animal_statistic <- primary_lmer %>% 
  filter(term == "understandsunknown animal") %>% 
  pull(statistic)

unknown_animal_p <- primary_lmer %>% 
  filter(term == "understandsunknown animal") %>% 
  pull(p.value)

after_selection_estimate <- primary_lmer %>% 
  filter(term == "phasepost") %>% 
  pull(estimate)

after_selection_statistic <- primary_lmer %>% 
  filter(term == "phasepost") %>% 
  pull(statistic)

after_selection_p <- primary_lmer %>% 
  filter(term == "phasepost") %>% 
  pull(p.value)

after_selection_second_appearance_estimate <- primary_lmer %>% 
  filter(term == "appearancesecond:phasepost") %>% 
  pull(estimate)

after_selection_second_appearance_statistic <- primary_lmer %>% 
  filter(term == "appearancesecond:phasepost") %>% 
  pull(statistic)

after_selection_second_appearance_p <- primary_lmer %>% 
  filter(term == "appearancesecond:phasepost") %>% 
  pull(p.value)

after_selection_avg_known_estimate <- primary_lmer %>% 
  filter(term == "phasepost:avg_known") %>% 
  pull(estimate)

after_selection_avg_known_statistic <- primary_lmer %>% 
  filter(term == "phasepost:avg_known") %>% 
  pull(statistic)

after_selection_avg_known_p <- primary_lmer %>% 
  filter(term == "phasepost:avg_known") %>% 
  pull(p.value)

after_selection_vocab_estimate <- primary_lmer %>% 
  filter(term == "phasepost:vocab") %>% 
  pull(estimate)

after_selection_vocab_statistic <- primary_lmer %>% 
  filter(term == "phasepost:vocab") %>% 
  pull(statistic)

after_selection_vocab_p <- primary_lmer %>% 
  filter(term == "phasepost:vocab") %>% 
  pull(p.value)

after_selection_unknown_estimate <- primary_lmer %>% 
  filter(term == "phasepost:understandsunknown animal") %>% 
  pull(estimate)

after_selection_unknown_statistic <- primary_lmer %>% 
  filter(term == "phasepost:understandsunknown animal") %>% 
  pull(statistic)

after_selection_unknown_p <- primary_lmer %>% 
  filter(term == "phasepost:understandsunknown animal") %>% 
  pull(p.value)
```

If parents calibrate their referring expressions to their children's linguistic knowledge, they should provide more information to children for whom a simple bare noun (e.g. "leopard") would be insufficient to identify the target. Parents did this in a number of ways: with one or more adjectives (e.g., "the spotted, yellow leopard"), with similes (e.g., "the one that's like a cat"), and with allusions to familiar animal exemplars of the category (e.g. "pick Midnight"). In many of these cases, parents would be required to produce more words (see below for further qualitative analyses). Thus, we analyzed the length of parents' referring expressions as a theory-agnostic proxy for informativeness.

If parents tune their referring expressions to children's knowledge, they should produce more informative--and thus longer--referring expressions when they think their children will need them. To test this hypothesis, we divided every trial of the game into two phases: the time before a child selected an animal, and the time following selection until the start of the next trial. We then fit a mixed effects model predicting the number of words parents produced (log) from phase (before vs. after selection), target appearance (first vs. second), and three potential measures of tuning: (1) The total number of words the parent thought their child knew, (2) the proportion of all children whose parents reported they knew each target animal, and (3) whether each individual parent thought their child knew each individual word. We also estimated the interaction of each of these variables with phase. We began with a maximal random effect structure and removed random effects until the model converged, prioritizing variables of greatest theoretical relevance for subjects and design-relevant variables for items. The final model included random intercepts and slopes of word-knowledge for subjects, and random intercepts and slopes of appearance for items. 

Before children selected an animal, parents produced reliably fewer words on the second appearance of each animal ($\beta =$ `r second_appearance_estimate`, $t =$ `r second_appearance_statistic`, $p$ `r second_appearance_p`), reliably fewer words for animals that more children were reported to know ($\beta =$ `r avg_known_estimate`, $t =$ `r avg_known_statistic`, $p$ `r avg_known_p`), and reliably more words for animals that they believed their individual child did not know ($\beta =$ `r unknown_animal_estimate`, $t =$ `r unknown_animal_statistic`, $p =$ `r unknown_animal_p`). Children's total vocabularies did not reliably affect the number of words parents produced ($\beta =$ `r vocab_estimate`, $t =$ `r vocab_statistic`, $p =$ `r vocab_p`). After children selected an animal, parents produced reliably fewer words ($\beta =$ `r after_selection_estimate`, $t =$ `r after_selection_statistic`, $p$ `r after_selection_p`), but this reduction was smaller on an animal's second appearance ($\beta =$ `r after_selection_second_appearance_estimate`, $t =$ `r after_selection_second_appearance_statistic`, $p$ `r after_selection_second_appearance_p`), smaller for animals known by more children ($\beta =$ `r after_selection_avg_known_estimate`, $t =$ `r after_selection_avg_known_statistic`, $p$ `r after_selection_avg_known_p`), and bigger for children who knew more words ($\beta =$ `r after_selection_vocab_estimate`, $t =$ `r after_selection_vocab_statistic`, $p$ `r after_selection_vocab_p`). The number of words produced after selection did not vary with parents' beliefs about their child's knowledge of that individual animal ($\beta =$ `r after_selection_unknown_estimate`, $t =$ `r after_selection_unknown_statistic`, $p =$ `r after_selection_unknown_p`). Thus, when parents were trying to get their children to select the correct target animal, they provided more information for animals that were generally known by fewer children (coarse tuning; Figure \ref{fig:difficulty-fig}B), but over and above that provided more information for animals that they believed their individual child did not know (fine tuning; Figure \ref{fig:known-fig}). In addition, parents produced fewer words after selection for children who knew more words, perhaps because they needed less support and reinforcement.

```{r individual-plot-data}
individual_data <- target_data %>%
  filter(phase == "pre", appearance == "first") %>%
  group_by(understands, subj, trial_target) %>%
  summarise(length = sum(length)) %>%
  summarise(length = mean(length)) %>%
  mutate(type = "First appearance")

group_data <- individual_data %>%
  tidyboot_mean(length) %>%
  rename(length = empirical_stat,
         length_upper = ci_upper,
         length_lower = ci_lower) %>%
  select(-mean, -n) %>%
  mutate(subj = "mean") %>%
  mutate(type = "First appearance")
```

```{r lag-data}
setup_lag_data <- target_data %>%
  filter(phase == "pre") %>%
  ungroup() %>%
  mutate(correct = factor(correct, levels = c(TRUE, FALSE), 
                          labels = c("correct", "incorrect"))) %>%
  group_by(subj, understands, target, appearance, correct) %>%
  summarise(length = sum(length)) %>%
  ungroup() %>%
  complete(nesting(subj, target, understands,  correct,  appearance),
           fill = list(length = 0)) %>%
  arrange(subj, understands, target,appearance, correct) %>%
  group_by(subj, understands, target) %>%
  mutate(lag_correct = lag(correct)) 


indiv_lag_data <- setup_lag_data %>%
    filter(appearance == "second", !is.na(lag_correct)) %>%
  group_by(understands, lag_correct, subj, target) %>%
  summarise(length = sum(length)) %>%
  summarise(length = mean(length)) %>%
  rename(type = lag_correct)

group_lag_data <- indiv_lag_data %>%
  tidyboot_mean(length) %>%
  rename(length = empirical_stat,
         length_upper = ci_upper,
         length_lower = ci_lower) %>%
  select(-mean, -n) %>%
  mutate(subj = "mean")
```


```{r lag-lmer}
lag_lmer <- lag_data %>%
  mutate(length = log(length+1)) %>%
  lmer(length ~ lag_correct * understands + 
         ( understands | subj) + (1 | target), 
                 data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  select(-effect, -group, -df, -std.error) %>%
  mutate(p.value = printp(p.value))


lag_incorrect_estimate <- lag_lmer %>% 
  filter(term == "lag_correctincorrect") %>% 
  pull(estimate)

lag_incorrect_statistic <- lag_lmer  %>% 
  filter(term == "lag_correctincorrect") %>% 
  pull(statistic)

lag_incorrect_p <- lag_lmer  %>% 
  filter(term == "lag_correctincorrect") %>% 
  pull(p.value)

lag_known_estimate <- lag_lmer %>% 
  filter(term == "understandsknown\nanimal") %>% 
  pull(estimate)

lag_known_statistic <- lag_lmer  %>% 
  filter(term == "understandsknown\nanimal") %>% 
  pull(statistic)

lag_known_p <- lag_lmer  %>% 
  filter(term == "understandsknown\nanimal") %>% 
  pull(p.value)

lag_incorrect_known_estimate <- lag_lmer %>% 
  filter(term == "lag_correctincorrect:understandsknown\nanimal") %>% 
  pull(estimate)

lag_incorrect_known_statistic <- lag_lmer  %>% 
  filter(term == "lag_correctincorrect:understandsknown\nanimal") %>% 
  pull(statistic)

lag_incorrect_known_p <- lag_lmer  %>% 
  filter(term == "lag_correctincorrect:understandsknown\nanimal") %>% 
  pull(p.value)
```

We found that parents' referring expressions on the second appearance of each animal were affected by both measures of coarse tuning: The child's total vocabulary and the proportion of all children who knew that animal. They were not, however, affected by parents' beliefs about their child's knowledge of that animal. Why not? One possibility is that parents get information from the first appearance of each animal: they may have thought their child knew "leopard," but discovered from their incorrect choice that they did not. If so, they might produce a longer referring expression for the leopard the second time around. To test this hypothesis, we fit a mixed effects model predicting the length of parents' referring expressions on the second appearance of each animal from success on first appearance, whether parents thought their child knew the animal prior to the experiment, and their interaction. We followed the same approach with random effects, beginning with a maximal model and pruning effects until the model converged. The final model included random intercepts and slopes of prior belief by subject and a random intercept for each animal. Parents produced marginally shorter referring expressions when children were incorrect on the first appearance of each animal ($\beta =$ `r lag_incorrect_estimate`, $t =$ `r lag_incorrect_statistic`, $p =$ `r lag_incorrect_p`), and shorter referring expressions for animals that they believed their child knew ($\beta =$ `r lag_known_estimate`, $t =$ `r lag_known_statistic`, $p =$ `r lag_known_p`). However, they produced longer referring expressions following an incorrect response for animals they thought their chilren knew ($\beta =$ `r lag_incorrect_known_estimate`, $t =$ `r lag_incorrect_known_statistic`, $p$ `r lag_incorrect_known_p`). Thus, when parents thought their children knew an animal, but then observed evidence to the contrary, they provided more information in their referring expressions for children to make the correct selection the second time. In fact, parents' referring expressions were indistinguishable in length for known and unknown animals when children had incorrectly selected on the first appearance (Figure \ref{fig:known-fig}). 

Together, these two sets of analyses suggest that parents tune their referring expressions not just coarsely to their knowledge about how hard individual animal words are, or how much language their children generally know, but also finely to their beliefs about their children's knowledge of individual lexical items. Further, when they discover that they have incorrect beliefs about their children's knowledge, they update these beliefs in real-time and leverage them on subsequent references to the same lexical item.

```{r known-fig, fig.width = 6, fig.height = 2.5, fig.env = "figure", fig.cap = "Length of parents' referring expressions on the first appearance each animal, and on the second appearance following correct vs. incorrect selections on the first appearance. When parents thought their child knew an animal, but they were incorrect on its first appearance, they provided more information on the second appearance. Points show group averaged proportions; error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}
individual_plot_data <- bind_rows(individual_data, indiv_lag_data) %>%
  mutate(type = factor(type, 
                       levels = c("First appearance", "correct", "incorrect"),
                       labels = c("First appearance", "Following correct", 
                                  "Following incorrect")))

group_plot_data <- bind_rows(group_data, group_lag_data) %>%
  mutate(type = factor(type, 
                       levels = c("First appearance", "correct", "incorrect"),
                       labels = c("First appearance", "Following correct", 
                                  "Following incorrect")))

ggplot(individual_plot_data, aes(x = understands, y = length, group = subj)) + 
  geom_point(alpha = .5, position=position_dodge(0.06), color = "light gray") +
  geom_line(alpha = .5, color = "light gray", position=position_dodge(0.06)) +
  scale_x_discrete(labels= c("Unknown Animal", "Known Animal"), expand = c(.1, .1)) + 
  labs(x ="", y = "Length of referring expressions") +
  facet_wrap(~type) +
  geom_pointrange(aes(ymin = length_lower, ymax = length_upper), 
                  data = group_plot_data) + 
  geom_line(data = group_plot_data) +
  scale_y_continuous(limits = c(2, 15))
```


## Content of referring expressions

```{r qual-coding}
tidy_qual_data <- qual_data %>%
  select(-num_anaphoric) %>%
  pivot_longer(cols = num_canonical:num_superordinate, names_to = "qual_measure", 
               values_to = "used") %>%
  mutate(understands = factor(understands, levels = c(F, T), 
                              labels = c("unknown animal", 
                                         "known animal")),
          qual_measure = factor(qual_measure, 
                                levels = c("num_canonical", "num_comparison",
                                           "num_descriptors", "num_superordinate",
                                           "num_subordinate")))

qual_means <- tidy_qual_data %>%
  group_by(qual_measure, id, trial_target) %>%
  summarise(used = mean(used)) %>%
  summarise(used = mean(used)) %>%
  summarise(used = mean(used))


qual_knowledge_means <- tidy_qual_data %>%
  group_by(qual_measure, understands, id, trial_target) %>%
  summarise(used = mean(used)) %>%
  summarise(used = mean(used)) %>%
  summarise(used = mean(used))

canonical_used <- qual_means %>%
  filter(qual_measure == "num_canonical") %>%
  mutate(used = used * 100) %>%
  pull(used)

subordinate_used <- qual_means %>%
  filter(qual_measure == "num_subordinate") %>%
  mutate(used = used * 100) %>%
  pull(used)

qual_lmer <- tidy_qual_data %>%
  group_by(qual_measure) %>%
  nest() %>%
  mutate(model = map(data, 
                     ~glmer(used ~ understands + (1|id) + (1|trial_target), 
                    family = "binomial", data = . ) %>% tidy())) %>%
  select(-data) %>%
  unnest(cols = c(model)) %>%
  filter(effect == "fixed") %>%
  select(-effect, -group, -std.error) %>%
  mutate(p.value = printp(p.value)) %>%
  filter(term != "(Intercept)")

canonical_known <- qual_knowledge_means %>%
  filter(qual_measure == "num_canonical", understands == "known animal") %>%
  mutate(used = used * 100) %>%
  pull(used)

canonical_unknown <- qual_knowledge_means %>%
  filter(qual_measure == "num_canonical", understands == "unknown animal") %>%
  mutate(used = used * 100) %>%
  pull(used)

comparison_known <- qual_knowledge_means %>%
  filter(qual_measure == "num_comparison", understands == "known animal") %>%
  mutate(used = used * 100) %>%
  pull(used)

comparison_unknown <- qual_knowledge_means %>%
  filter(qual_measure == "num_comparison", understands == "unknown animal") %>%
  mutate(used = used * 100) %>%
  pull(used)

descriptors_known <- qual_knowledge_means %>%
  filter(qual_measure == "num_descriptors", understands == "known animal") %>%
  mutate(used = used * 100) %>%
  pull(used)

descriptors_unknown <- qual_knowledge_means %>%
  filter(qual_measure == "num_descriptors", understands == "unknown animal") %>%
  mutate(used = used * 100) %>%
  pull(used)

subordinate_known <- qual_knowledge_means %>%
  filter(qual_measure == "num_subordinate", understands == "known animal") %>%
  mutate(used = used * 100) %>%
  pull(used)

subordinate_unknown <- qual_knowledge_means %>%
  filter(qual_measure == "num_subordinate", understands == "unknown animal") %>%
  mutate(used = used * 100) %>%
  pull(used)

superordinate_known <- qual_knowledge_means %>%
  filter(qual_measure == "num_superordinate", understands == "known animal") %>%
  mutate(used = used * 100) %>%
  pull(used)

superordinate_unknown <- qual_knowledge_means %>%
  filter(qual_measure == "num_superordinate", understands == "unknown animal") %>%
  mutate(used = used * 100) %>%
  pull(used)

canonical_estimate <- qual_lmer %>% 
  filter(qual_measure == "num_canonical") %>% 
  pull(estimate)

canonical_statistic <- qual_lmer  %>% 
  filter(qual_measure == "num_canonical") %>% 
  pull(statistic)

canonical_p <- qual_lmer  %>% 
  filter(qual_measure == "num_canonical") %>% 
  pull(p.value)

comparison_estimate <- qual_lmer %>% 
  filter(qual_measure == "num_comparison") %>% 
  pull(estimate)

comparison_statistic <- qual_lmer  %>% 
  filter(qual_measure == "num_comparison") %>% 
  pull(statistic)

comparison_p <- qual_lmer  %>% 
  filter(qual_measure == "num_comparison") %>% 
  pull(p.value)

descriptors_estimate <- qual_lmer %>% 
  filter(qual_measure == "num_descriptors") %>% 
  pull(estimate)

descriptors_statistic <- qual_lmer  %>% 
  filter(qual_measure == "num_descriptors") %>% 
  pull(statistic)

descriptors_p <- qual_lmer  %>% 
  filter(qual_measure == "num_descriptors") %>% 
  pull(p.value)

superordinate_estimate <- qual_lmer %>% 
  filter(qual_measure == "num_superordinate") %>% 
  pull(estimate)

superordinate_statistic <- qual_lmer  %>% 
  filter(qual_measure == "num_superordinate") %>% 
  pull(statistic)

superordinate_p<- qual_lmer  %>% 
  filter(qual_measure == "num_superordinate") %>% 
  pull(p.value)

subordinate_estimate <- qual_lmer %>% 
  filter(qual_measure == "num_subordinate") %>% 
  pull(estimate)

subordinate_statistic <- qual_lmer  %>% 
  filter(qual_measure == "num_subordinate") %>% 
  pull(statistic)

subordinate_p <- qual_lmer  %>% 
  filter(qual_measure == "num_superordinate") %>% 
  pull(p.value)
```


Parents produced reliably longer referring expressions when trying to communicate about animals that they thought their children did not know. In the analyses presented so far, we used length as a theory-agnostic, quantitiative measure of information. *How* did parents successfully refer to animals that their children did not know? As a post-hoc descriptive analysis, we coded five qualitative features of referring expressions: (1) Use of the animal's canonical label (e.g. "leopard"), (2) Use of a descriptor (e.g. "spotted"), (3) Use of a comparison (e.g. "like a cat"), (4) Use of a subordinate category label (e.g. "Limelight Larry" for peacock), and (5) Use of a basic level category label (e.g. "bird" for peacock). Because the rates of usage of each of these kinds of reference varied widely (e.g. canonical labels were used on `r canonical_used`\% of trials, but subordinates were used on `r subordinate_used`\% of trials), we fit a logistic mixed effects model separately for each reference kind estimating whether it would be used on each trial from whether the parent thought their child knew the animal and random intercepts for subjects and animals. Canonical labels were used on almost all trials, and did not differ in frequency between unknown ($M =$ `r canonical_unknown`\%) and known ($M =$ `r canonical_known`\%) animals ($\beta =$ `r canonical_estimate`, $t =$ `r canonical_statistic`, $p =$ `r canonical_p`). Comparisons were used reliably more for unknown ($M =$ `r comparison_unknown`\%) than for known ($M =$ `r comparison_known`\%) animals ($\beta =$ `r comparison_estimate`, $t =$ `r comparison_statistic`, $p =$ `r comparison_p`), as were descriptors (known $M =$ `r descriptors_known`\%, unknown $M =$ `r descriptors_unknown`\%, $\beta =$ `r descriptors_estimate`, $t =$ `r descriptors_statistic`, $p$ `r descriptors_p`). Basic category labels were used marginally more for unknown ($M =$ `r superordinate_unknown`\%) than known ($M =$ `r superordinate_known`\%) animals ($\beta =$ `r superordinate_estimate`, $t =$ `r superordinate_statistic`, $p =$ `r superordinate_p`), and subordinates were used marignally less for unknown ($M =$ `r subordinate_unknown`\%) than for known ($M =$ `r subordinate_known`\%) animals ($\beta =$ `r subordinate_estimate`, $t =$ `r subordinate_statistic`, $p =$ `r subordinate_p`). Thus, parents used a variety of strategies refer to animals that children did not understand, but the use of descriptors was the most prominent. These descriptors are particularly apt to facilitate children's learning, connecting parents' fine-tuning for reference with their children's language acquisition.

```{r qual-plot, fig.height = 4, fig.width = 8, fig.env = "figure", eval = F, fig.cap = "Proportion of trials where parents used canonical labels, descriptors, comparisons, subordinate category labels, and basic category labels. Each panel shows proportions for known and unknown animals."}
qual_data %>%
  ungroup() %>%
  gather(measure, value, num_canonical, num_descriptors, num_comparison,
         num_subordinate, num_superordinate) %>%
  mutate(measure = case_when(
    measure == "num_canonical" ~ "Canonical label",
    measure == "num_descriptors" ~ "Descriptors",
    measure =="num_comparison" ~ "Comparison",
    measure =="num_subordinate" ~ "Subordinate",
    measure =="num_superordinate" ~ "Superordinate"),
    measure = factor(measure, levels = c("Comparison", "Descriptors",
                                         "Superordinate", "Subordinate", 
                                         "Canonical label")),
    understands = factor(understands, levels = c(T, F), 
                              labels = c("Known Animal", "Unknown Animal"))) %>%
  group_by(understands, measure) %>%
  tidyboot_mean(value, nboot = 10) %>%
  ggplot(aes(x = understands, y = empirical_stat, 
             ymin = ci_lower, ymax = ci_upper, color = understands)) + 
  labs(x = "Parents' belief about animal word", y = "Proportion of trials") +
  facet_wrap( ~ measure, scales = "free") + 
  geom_pointrange(position = position_dodge(.5)) + 
  theme(legend.position = "none") + 
  scale_color_ptol()
```

# Discussion


Parents have a wealth of knowledge about their children's linguistic development [@fenson2007]. Do they draw on this knowledge when they communicate? In a referential communication task, we showed that parents' references are tuned to their children in three ways: (1) they produce longer, more informative referring expressions for later-learned animals, (2) over and above this coarse tuning, the lengths of parents' utterances are calibrated to their individual children's knowledge of individual animals, and (3) when children do not know an animal that parents thought they did, parents' subsequent references reflect this updated belief. We further found that more informative referring expressions were associated with increased likelihood of successful communication: Children were more likely to correctly select animals whose names they did not know if their parents produced longer utterances to refer to them. Finally, we found that references to unknown animals are rich with descriptors and comparisons, helping children select the correct animal and potentially also serving as a source of learning input.

These data are consistent with a strong form of the linguistic tuning hypothesis, in which parents fine-tune the information in their speech to children's language knowledge at the individual-word level. Why should this happen? Although parents speech to children is unlikely to reflect a goal to teach, it is nonetheless goal-oriented: Parents want to communicate successfully [@bruner1983]. Fortunately, learning may piggyback on this optimization because of the inherent synergy between communication and learning--it is easier to learn from input that you understand [@yurovsky2018]. Our work thus highlights the importance of studying the parent-child dyad as a unit, rather than viewing children as isolated learners. Children bring powerful learning mechanisms to language acquisition, but these mechanisms are supported by an ecological niche designed for their success [@west1987].

# Acknowledgements

This research was funded by a James S. McDonnell Foundation Scholar Award to DY.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
