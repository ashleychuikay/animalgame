---
title             : "Parents fine-tune their speech to children's vocabulary knowledge"
shorttitle        : "Parents fine-tune speech"

author: 
  - name          : "XXXXX"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "XXXXX"
    email         : "XXXXX"
  - name          : "XXXXX"
    affiliation   : "1"
  - name          : "XXXXX"
    affiliation   : "1,2"

affiliation:
  - id            : "1"
    institution   : "XXXXX"
  - id            : "2"
    institution   : "XXXXX"

abstract: |
   Young children learn language at an incredible rate. While children come prepared with powerful statistical learning mechanisms, the statistics they encounter are also prepared for them: children learn from caregivers motivated to communicate with them. How precisely do parents tune their speech to their children's individual language knowledge? To answer this question, we asked parent-child pairs (n=41) to play a reference game in which the parent's goal was to guide their child to select a target animal from a set of three. Parents fine-tuned their referring expressions to their children's knowledge at the lexical level, producing more informative references for animals they thought their children did not know. Further, parents learned about their children's knowledge over the course of the game, and tuned their referring expressions accordingly. Child-directed speech may thus support children's learning not because it is uniformly simplified, but because it is tuned to individual children's language development.
   
   \begin{center} Statement of Relevance \end{center}
   
   The pace at which children learn language is one of the most impressive feats of early cognitive development. One possible explanation for this rapid pace is that the language caregivers produce is tuned to children's developing linguistic knowledge, maintaining just the right level of complexity to support rapid learning. We present the first experimental evidence of just how precise this tuning is, showing that parents tune not just to children's holistic language development, but to their knowledge of individual words. We developed a new method in which we experimentally controlled what parents talked about, but not how they could talk or what they could say, increasing the chance that these  results will generalize outside the lab. This work points to the importance of studying the parent-child dyad as a unit instead of focusing on children as isolated learners, both in the domain of language and in social learning more broadly.
   
authornote: |
  All data and code for these analyses are available at https://osf.io/3f8hy/?view_only=9a196db0444c4867bc899cc70a7a1e9c. Videos of experiment sessions are available on Databrary.

keywords          : "parent-child interaction; language development; communication"
wordcount         : "749"
references         : "44"

bibliography      : ["animalgame.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r load-packages, include = FALSE}
library(png)
library(grid)
library(egg)
library(xtable)
library(knitr)
library(papaja)
library(ggthemes)
library(lme4)
library(lmerTest)
library(directlabels)
library(ggrepel)
library(here)
library(scales)
library(tidyboot)
library(broom)
library(broom.mixed)
library(kableExtra)
library(english)
library(ggthemes)
library(effectsize)
library(glue)
library(tidyverse)

theme_set(theme_few(base_size = 10) + theme(legend.position = "none"))

knitr::opts_chunk$set(fig.pos = '!tb', echo = FALSE, cache = TRUE, 
                      warning = FALSE, message = FALSE, 
                      sanitize = TRUE, fig.path='figs/', fig.width = 3,
                      fig.height = 3, fig.pos = "t!")
set.seed(42)
options(digits=3, dplyr.summarise.inform = FALSE)
```

In just a few short years, children master their native language. Undoubtedly, a large share of the credit for this feat is due to powerful learning mechanisms that children bring to their input [@kuhl2004; @saffran1996; @smith2014]. However, a share of the credit may also be due to the structure of linguistic input itself: individual differences in both the quantity and quality of the language children hear are associated with individual differences in those children’s language learning [@hart1995; @huttenlocher2010; @rowe2012]. Further, associations between input and uptake are primarily driven by differences in speech directed to children. Differences in overheard speech do not predict differences in language learning, even in communities where child-directed speech is relatively rare [@romeo2018; @shneidman2012; @weisleder2013]. Why is child-directed speech so important to language learning?

The way we speak to children is markedly different from the way we speak to adults. For instance, child-directed speech tends to be slower, higher pitched, and exaggerated in enunciation relative to adult-directed speech [@cooper1990; @grieser1988]. Beyond acoustic and prosodic differences, child-directed speech is also marked by repetition, simpler syntactic structures, and higher proportions of questions [@fernald1984; @newport1977; @snow1972]. Children preferentially listen to child-directed speech over adult-directed speech [@cooper1990; @consortium2020], and their increased attention to child-directed speech may play a part in driving language acquisition [@soderstrom2007]. 

In addition to attentional effects, structural simplifications in child-directed speech have been tied to specific benefits in children’s language learning [e.g. @brent2001; @ma2011]. For instance, when parents refer to a particular object, they tend to place this object in the final position of their utterances, even in languages where this is not the canonical word order [@aslin1996]. This structural tendency has been tied directly to ease of word segmentation and subsequent word learning [@endress2009; @yurovsky2012]. 

Crucially, both acoustic and structural properties of child-directed speech change over development, with sentences getting longer, more complex, and less acoustically variable [e.g. @huttenlocher2010; @liu2009; @phillips1973]. The linguistic tuning hypothesis suggests that this changing nature of child-directed speech is what allows it to be such a powerful driver of language development [@snow1972]. If parents tune their speech to children’s developmental level, increasing the complexity of input at the same rate that children are developing their linguistic knowledge, input may always be at the optimal level of complexity to support language learning [@vygotsky1978]. 

How precisely do parents tune their speech? One possibility is that tuning is *coarse*: caregivers could tune the complexity of their speech generally, using a holistic sense of their children’s developing linguistic abilities. Consistent with a coarse-tuning hypothesis, parents tune their utterance lengths, articulation of vowels, and diversity of clauses to children's age [@bernstein-ratner1984; @huttenlocher2010; @moerk1976]. Over and above this coarse-tuning, parents might *fine-tune* their speech, taking into account not only children’s global linguistic development, but their specific knowledge of smaller units of language, such as lexical items. Fine-tuning would provide a particularly powerful and efficient vehicle for scaffolding language acquisition because of its specificity. If parents could fine-tune utterances containing specific words, phrases, or constructions, they could keep each aspect of language at a desirable difficulty to support learning, retention, and generalization [@bjork2015; @vlach2014].

To date, the only evidence for fine-tuning comes from two observational studies, one showing that parents are more likely to provide their child with labels for novel as compared to familiar toys [@masur1997], and the second showing that one child’s caregivers produce their shortest utterances containing a particular word just before the child first produces that word [@roy2009]. Here, we present the first *experimental* evidence for fine-tuning. 

Children and their parents played a reference game in which the parent’s goal was to guide their child to select a target animal from a set of three. Parents tuned the amount of information in their utterances not just to the average difficulty of each animal word, but to their prior estimates of their individual child’s knowledge of that animal. Further, parents sensitively adapted over the course of the game, providing more information on subsequent trials when they discovered that their child did not know an animal. Together, these results show that parents leverage their knowledge of their children’s language development to fine-tune the linguistic information they provide.


```{r make-text-vars}
make_text_vars <- function(df, term_name, term_filter = NULL) {
  if(!is.null(term_filter)) {
    filtered_df <- df %>%
      filter(term == term_filter) 
  } else{
    filtered_df <- df
  }
    
  walk(c("estimate", "statistic", "p.value", "d", "d_low", "d_high"), 
      ~assign(glue("{term_name}_{.x}"), 
              filtered_df %>% pull(!!.x), 
         envir = globalenv()))
}
```

# Method
```{r load-data}
demos <- read_csv(here("data/demographics.csv"))

subj_vocab <- read_csv(here("data/subj_vocab.csv"))

test_data <- read_csv(here("data/test_data.csv"))

target_data <- read_csv(here("data/transcripts.csv")) %>%
  left_join(subj_vocab, by = c("subj", "trial_target" = "word")) %>%
  filter(phase == "pre", !is.na(understands)) %>%
  complete(nesting(subj, target, understands, appearance),
           fill = list(length = 0)) %>%
  mutate(understands = factor(understands, levels = c(T,F), 
                              labels = c("Knows", "Doesn't Know")))

possible_vocab <- subj_vocab %>%
  distinct(possible_vocab) %>%
  pull()
```

## Participants

```{r youngest-and-oldest}
youngest_child <- demos %>%
  filter(age_years == min(age_years))

oldest_child <- demos %>%
  filter(age_years == max(age_years))

mean_age <- demos %>%
  summarise(age_days = mean(age_years * 365)) %>%
  mutate(age_months = floor(age_days / 30.5),
         age_extra_days = floor(age_days - (age_months * 30.5)))

youngest_chid_months <- youngest_child %>% 
  pull(age_months)
youngest_child_days <- youngest_child %>% 
  pull(age_extra_days)


oldest_child_months <- oldest_child %>% 
  pull(age_months)
oldest_child_days <- oldest_child %>% 
  pull(age_extra_days)

mean_months <- mean_age %>% pull(age_months)
mean_days <- mean_age %>% pull(age_extra_days)

n_girls <- demos %>% 
  summarise(female = sum(gender == "female")) %>% 
  pull

white_pct <- demos %>% 
  summarise(race = mean(race == "White")) %>% 
  pull(race) %>% 
  percent()

black_pct <- demos %>% 
  summarise(race = mean(race == "Black or African American")) %>% 
  pull(race) %>% 
  percent()

hispanic_pct <- demos %>% 
  summarise(hispanic = mean(hispanic == "yes", na.rm = TRUE)) %>% 
  pull(hispanic) %>% 
  percent()

degree_pct <- demos %>% 
  summarise(mom_ed = mean(mom_ed %in% c("4-Year College", 
                                        "Graduate degree"))) %>% 
  pull(mom_ed) %>% 
  percent()
```

Toddlers (aged 2-2.5 years) and their parents were recruited from a database of families in the local community or approached on the floor of a local science museum in order to achieve a planned sample of 40 parent-child dyads. Because our method was novel, we chose a sample size that would give us 95\% power to detect a medium-sized effect ($d$=.6) within-subjects and rounded up to the nearest multiple of 10. A total of 48 parent-child pairs were recruited, but data from 7 pairs were dropped from analysis because of failure to complete the experiment as designed. Of the 7 pairs that were dropped, 5 children fussed out, 1 had an older sibling interfering with the study, and 1 was a twin (only the twin who participated first was included). The final sample consisted of `r nrow(demos)` children aged `r youngest_chid_months` mo.; `r youngest_child_days` days to `r oldest_child_months` mo.; `r oldest_child_days` days ($M =$ `r mean_months` mo.; `r mean_days` days), `r n_girls` of whom were girls. 

In our recruitment, we made an effort to sample children from a variety of racial and socio-economic groups. Our final sample was roughly representative of the racial composition of the Chicago Area and the US more broadly (`r white_pct` White, `r black_pct` Black, `r hispanic_pct` Hispanic). However, our sample was significantly more educated than the broader community (`r degree_pct` of mothers had a College or Graduate Degree).

## Stimuli

```{r aoa-data}
animals <- read_csv(here("corpus_data/predicted_aoas.csv"))%>%
  select(word, aoa) %>%
  left_join(select(subj_vocab, word, type) %>% distinct(), by = "word") %>%
  mutate(type = case_when(
    word == "rooster" ~ "late",
    word == "pig" ~ "early",
    T ~ type)) %>%
  filter(!is.na(type))

min_aoa <- animals %>% 
  pull(aoa) %>% 
  min() %>% 
  round()

max_aoa <- animals %>% 
  pull(aoa) %>% 
  max() %>% 
  round()

early_min_aoa <- animals %>% 
  filter(type == "early") %>% 
  pull(aoa) %>% 
  min() %>% 
  round()

early_max_aoa <- animals %>% 
  filter(type == "early") %>% 
  pull(aoa) %>% 
  max() %>% 
  round()

late_min_aoa <- animals %>% 
  filter(type == "late") %>% 
  pull(aoa) %>% 
  min() %>% 
  round()

late_max_aoa <- animals %>% 
  filter(type == "late") %>% 
  pull(aoa) %>% 
  max() %>% 
  round()

total_vocab <- subj_vocab %>% 
  pull(possible_vocab) %>% 
  mean()
```

Eighteen animal images were selected from the @rossion2004 image set, a colorized version of the @snodgrass1980 object set. Animals were selected based on estimates of their age of acquisition (AoA) for American English learners. To obtain these estimates, we used two sources of information: parent-report estimates of children's age of acquisition from Wordbank [@frank2017], and retrospective self-report estimates of AoA from adults [@kuperman2012, see Supplemental Materials for details]. The AoA of the selected animals ranged from `r min_aoa` to `r max_aoa` months. Half of the animals were chosen to have an early AoA (`r early_min_aoa`-`r early_max_aoa` months), and the other half were chosen to have a late AoA (`r late_min_aoa`-`r late_max_aoa` months). Each trial featured three animals, all from either the early AoA or late AoA category. This separation was designed to lower the likelihood that children could use knowledge of early AoA animals to infer the correct target on late AoA trials.

A modified version of the MacArthur-Bates Communicative Development Inventory Short Form [CDI; @fenson2007], a parent-reported measure of children's vocabulary, was administered before the testing session via an online survey. The selected animal words were added to the standard words, producing an `r total_vocab` word survey. Two of the animal words--one in the early AoA (pig) and one in the late AoA category (rooster)--were accidentally omitted, so trials for those words were not included in analyses as we could not obtain individual-level estimates of children's knowledge.

## Design and Procedure

```{r ipads, fig.env = "figure", fig.pos = "tb", fig.align='center', fig.width=6, fig.height=4, set.cap.width=T, num.cols.cap=1, fig.cap = "A parent-child dyad playing the reference game. On each trial, the parent's goal was to use language to communicate to their child which animal to choose."}
include_graphics(here("papers/journal/figs/setup.png"))
```

Each parent-child pair played an interactive reference game using two iPads (Figure \ref{fig:ipads}). Children began with two warm-up trials in which they tapped on circles that appeared on the iPads. Following these warm-up trials, children and their parents moved on to practice and then experimental trials. On each trial, three images of animals were displayed side by side on the child's screen, and a single word appeared on the parent's screen. Parents were instructed to communicate as they normally would with their child, and to encourage their child to choose the object corresponding to the word on their screen. The child was instructed to listen to their parent for cues. Once the child tapped an animal, the trial ended, and a new trial began. There were a total of 36 experimental trials, such that each animal appeared as the target twice. Trials were randomized for each participant, with the constraint that the same animal could not be the target twice in a row. Practice trials followed the same format as experimental trials, with the exception that images of fruit and vegetables were shown. All sessions were videotaped for transcription and coding.

## Data analysis

Our primary quantity of interest was the amount of information that parents provided in each of their utterances. To approximate this, we measured the length of parents' referring expressions--the number of words they produced on each trial before their child selected an animal. Length is an imperfect proxy for information, but it is easy to quantify and theory-agnostic. Because utterance length is highly right-skewed (i.e. most utterances are short), we log-transformed length in all analyses. However, to facilitate interpretability, we show raw utterance length in our figures. Subsequently, utterances were manually coded for the following: (1) Use of an animal's canonical label (e.g., "leopard"), (2) Use of a descriptor (e.g., "spotted"), (3) Use of a comparison (e.g., "like a cat"), (4) Use of a superordinate level category label (e.g., "bird" for peacock), and (5) Use of a subordinate level category label (e.g., "Limelight Larry," a fictional character from a children's book, for peacock). Parent utterances irrelevant to the game (e.g. asking the child to sit down) were not analyzed. Children's utterances were coded when audible, but were not analyzed. 
Our second source of data was the vocabulary questionnaire that parents filled out prior to participation. Parents indicated whether their child produced each of the `r possible_vocab` words on the survey. In addition to analyzing parents' judgments for the animals in the task, we also computed the total number of words judged to be known for each child as a proxy for total vocabulary.

All of our analyses were done using mixed-effects models. In all cases we began with maximal random effects structures and pruned random effects until the models converged. We removed interaction terms before removing main effects, and opted to keep the most theory-relevant random effects when only a subset of main effects could be kept. For clarity, we present only the key findings and statistics here, but full model details can be found in the Supplemental Materials.

# Results

We begin by confirming that our a priori divisions of animals into early and late age of acquisition (AoA) in the study design were reflected in parents' survey judgments, and that children were able to follow parents' references to select the correct target animal on each trial. After this, we show that parents fine-tune their referring expressions, producing more information in their references to animals that they think their individual children do not know. Further, parents update their tuning over the course of the experiment, producing more information on subsequent references to animals they thought their children knew after observing evidence to the contrary (i.e. children made an incorrect selection). 

```{r word_difficulty}
group_difficulty <- subj_vocab %>%
  group_by(type, word) %>%
  summarise(understands = mean(understands)) %>%
  tidyboot_mean(understands)

mean_word_difficulty <- subj_vocab %>%
  distinct(word, type, avg_known, ci_upper, ci_lower) %>%
  arrange(avg_known)

difficulty_lmer <- subj_vocab %>%
  mutate(type = factor(type, levels = c("early", "late"))) %>%
  glmer(understands ~ type + (1|subj) + (1|word),
        family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value),
         d = odds_to_d(estimate, log = TRUE),
         d_high = odds_to_d(estimate + 1.96 * std.error, log = TRUE),
         d_low = odds_to_d(estimate - 1.96 * std.error, log = TRUE)) %>%
  select(-effect, -group, -std.error)

make_text_vars(difficulty_lmer, "difficulty_type", "typelate")
```

## Target animal difficulty

```{r descriptives}
early_understood <- group_difficulty %>% 
  filter(type == "early") %>% 
  mutate(empirical_stat = empirical_stat * 100) %>% 
  pull(empirical_stat) %>% 
  round(0)

late_understood <- group_difficulty %>%
  filter(type == "late") %>% 
  mutate(empirical_stat = empirical_stat * 100) %>% 
  pull(empirical_stat) %>% 
  round(0)
```

We first confirm that the early AoA animals were more likely to be marked "known" by the parents of children in our studies. As predicted, parents judged that their children knew `r early_understood`% of the animals in the early AoA category, and `r late_understood`% of the animals in the late AoA category, which were reliably different from each other ($\beta =$ `r difficulty_type_estimate`, $p$ `r difficulty_type_p.value`, $d$ = `r difficulty_type_d` [`r difficulty_type_d_low`,  `r difficulty_type_d_high`]). Parents' judgments for each target word are shown in the Supplemental Materials.

## Selection accuracy

```{r test-data}
test_prediction_data <- target_data %>%
  filter(phase == "pre", !is.na(understands), !is.na(target)) %>%
  group_by(understands, subj, trial, target, appearance) %>%
  summarise(length = sum(length)) %>%
  mutate(log_length = log(length)) %>%
  left_join(select(test_data, subj, trial_num, target, correct), 
            by = c("subj", "trial" = "trial_num", "target")) %>%
  left_join(select(subj_vocab, subj, vocab) %>% distinct(), by = c("subj"))

overall_correct <- test_prediction_data %>%
  group_by(subj, target, appearance) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  mutate(correct = correct * 100) %>%
  pull()

type_correct <- test_prediction_data %>%
  group_by(understands, subj, target, appearance) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  summarise(correct = mean(correct)) %>%
  mutate(correct = correct * 100)

known_correct <- type_correct %>% 
  filter(understands == "Knows") %>%
  pull()

unknown_correct <- type_correct %>% 
  filter(understands == "Doesn't Know") %>%
  pull()

overall_acc_model <- glmer(correct ~ 1 + offset(base) + (1 | subj) + (1 | target),
      data = test_prediction_data %>% mutate(base = log(1/3)), 
      family = "binomial") %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value),
         d = odds_to_d(estimate, log = TRUE),
         d_high = odds_to_d(estimate + 1.96 * std.error, log = TRUE),
         d_low = odds_to_d(estimate - 1.96 * std.error, log = TRUE)) %>%
  select(-effect, -group, -std.error)

make_text_vars(overall_acc_model, "overall_acc")

type_acc_model <- test_prediction_data %>% 
  mutate(base = log(1/3)) %>%
  group_by(understands)  %>%
  nest() %>%
  mutate(model = map(data, ~glmer(correct ~ 1 + offset(base) + (1 | subj) +
                                    (1 | target),
                                  data =. , family = "binomial") %>% 
                       tidy())) %>%
  select(-data) %>%
  unnest(cols = c(model)) %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value),
         d = odds_to_d(estimate, log = TRUE),
         d_high = odds_to_d(estimate + 1.96 * std.error, log = TRUE),
         d_low = odds_to_d(estimate - 1.96 * std.error, log = TRUE)) %>%
  select(-effect, -group, -std.error, -term) %>%
  rename(term = understands)

make_text_vars(type_acc_model, "known_acc", "Knows")
make_text_vars(type_acc_model, "unknown_acc", "Doesn't Know")

test_lmer <- test_prediction_data %>%
  glmer(correct ~ log_length * understands + scale(vocab) + understands +
        (1 | subj) + (1 | target), 
      family = "binomial",
      data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value),
         d = odds_to_d(estimate, log = TRUE),
         d_high = odds_to_d(estimate + 1.96 * std.error, log = TRUE),
         d_low = odds_to_d(estimate - 1.96 * std.error, log = TRUE)) %>%
  select(-effect, -group, -std.error)

walk2(c("test_length", "test_unknown", "test_vocab", "test_length_unknown"),
      c("log_length", "understandsDoesn't Know", "scale(vocab)", 
        "log_length:understandsDoesn't Know"), 
      ~ make_text_vars(test_lmer, .x, .y))
```

On the whole, parents communicated effectively with their children, such that children selected the correct target on `r overall_correct`\% of trials, reliably greater than would be expected by chance (33\%, $\beta =$  `r overall_acc_estimate`, $p$ `r overall_acc_p.value`, $d$ = `r overall_acc_d` [`r overall_acc_d_low`,  `r overall_acc_d_high`]). Children were above chance both for animals that parents thought they knew ($M =$ `r known_correct`\%, $\beta =$ `r known_acc_estimate`, $p$ `r known_acc_p.value`,  $d$ = `r known_acc_d` [`r known_acc_d_low`,  `r known_acc_d_high`]), and for animals that parents thought their children did not know ($M =$ `r unknown_correct`\%, $\beta =$  `r unknown_acc_estimate`, $p$ `r unknown_acc_p.value`, $d$ = `r unknown_acc_d` [`r unknown_acc_d_low`,  `r unknown_acc_d_high`]). Thus, parents successfully communicated the target referent to children, even when parents thought their children did not know the name for the animal at the start of the game. 

Was this accuracy driven by children's knowledge or parents' referring expressions? Because we did not measure children's knowledge of each animal directly, we use parents' estimates of children's knowledge as a proxy to answer this question. We fit a mixed-effects logistic regression predicting children's accuracy on each trial from children's total estimated vocabulary, parent-reported knowledge of the target animal, and the (log) length of parents' expressions. We found that children with bigger vocabularies were more accurate in general ($\beta =$  `r test_vocab_estimate`, $p =$ `r test_vocab_p.value`,  $d$ = `r test_vocab_d` [`r test_vocab_d_low`,  `r test_vocab_d_high`]), and that children were less accurate for animals whose names parents thought they did not know ($\beta =$  `r test_unknown_estimate`, $p$ `r test_unknown_p.value`, $d$ = `r test_unknown_d` [`r test_unknown_d_low`,  `r test_unknown_d_high`]). Longer referring expressions were associated with lower accuracy for animals that parents thought their children knew ($\beta =$ `r test_length_estimate`, $p =$ `r test_length_p.value`, $d$ = `r test_length_d` [`r test_length_d_low`,  `r test_length_d_high`]), but greater accuracy for animals that parents thought their children did not know ($\beta =$ `r test_length_unknown_estimate`, $p =$ `r test_length_unknown_p.value`, $d$ = `r test_length_unknown_d` [`r test_length_unknown_d_low`,  `r test_length_unknown_d_high`]).

Thus, longer referring expressions were associated with more successful communication for animals that parents thought their children did not know, but were unhelpful for animals that parents thought they did know. We next ask whether parents tuned the lengths of their utterances appropriately, producing longer expressions for animals they believe their children do not know.

```{r continuous-plot-data}
continuous_plot_data <- target_data %>%
  group_by(trial_target, subj, appearance) %>%
  summarise(length = sum(length)) %>%
  summarise(length = mean(length)) %>%
  tidyboot_mean(length) %>%
  rename(length = empirical_stat,
         length_upper = ci_upper,
         length_lower = ci_lower) %>%
  left_join(mean_word_difficulty, by = c("trial_target" = "word")) %>%
  rename(avg_known_lower = ci_lower, avg_known_upper = ci_upper) %>%
  mutate(plotting_target = if_else(trial_target %in% c("squirrel", "cat",
                                                       "swan"),
                                   trial_target, as.character(NA)))
```

```{r difficulty-fig, fig.width = 4.5, fig.height = 2.5, fig.env = "figure", fig.cap = "Parents produced longer referring expressions to communicate about animals that children were generally less likely to know. Animals in blue are those that were estimated to be earlier learned from prior norms, animals in red are those that were estimated to be later learned. Error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}
p1_annotations <- tibble(avg_known = c(.25, .85),
                         length = c(12.5, 11),
                         avg_known_lower = as.numeric(c(NA, NA)),
                         avg_known_upper = as.numeric(c(NA, NA)),
                         length_lower = as.numeric(c(NA, NA)),
                         length_upper = as.numeric(c(NA, NA)),
                         plotting_target = c("late AoA", "early AoA"),
                         type = c("late", "early"))


ggplot(continuous_plot_data, aes(x = avg_known, y = length, 
                                 xmin = avg_known_lower,
                          xmax = avg_known_upper, ymin = length_lower,
                          ymax = length_upper,
                          label = plotting_target,
                          color = type)) + 
  geom_smooth(method = "lm", formula = "y ~ x", se = F, color = "black") +
  geom_pointrange()+ 
  geom_errorbarh() + 
  geom_label_repel(size = 2.5) +
  theme(legend.position = "none") +
  labs(x = "Proportion of children reported knowing target word", 
       y = "Referring expression length") +
  geom_text(data = p1_annotations) +
  scale_color_ptol() 
```

```{r individual-plot-data}
individual_data <- target_data %>%
  filter(appearance == "first") %>%
  group_by(understands, subj, trial_target) %>%
  summarise(length = sum(length)) %>%
  summarise(length = mean(length)) %>%
  mutate(type = "First appearance")

group_data <- individual_data %>%
  tidyboot_mean(length) %>%
  rename(length = empirical_stat,
         length_upper = ci_upper,
         length_lower = ci_lower) %>%
  mutate(subj = "mean") %>%
  mutate(type = "First appearance")
```

```{r lag-data}
setup_lag_data <- test_prediction_data %>%
  arrange(subj, understands, target,appearance, correct) %>%
  group_by(subj, understands, target) %>%
  mutate(correct = factor(correct, levels = c(TRUE, FALSE), 
                          labels = c("correct", "incorrect"))) %>%
  mutate(lag_correct = lag(correct)) 


indiv_lag_data <- setup_lag_data %>%
    filter(appearance == "second", !is.na(lag_correct)) %>%
  group_by(understands, lag_correct, subj, target) %>%
  rename(type = lag_correct) %>%
  group_by(understands, type, subj) %>%
  summarise(length = mean(length))

group_lag_data <- indiv_lag_data %>%
  tidyboot_mean(length) %>%
  rename(length = empirical_stat,
         length_upper = ci_upper,
         length_lower = ci_lower) %>%
  mutate(subj = "mean")
```

## Tuning

```{r trial-known-data}
trial_known_data <- target_data %>%
  mutate(phase = factor(phase, levels = c("pre", "post")),
         understands = factor(understands, labels = c("unknown animal", 
                                                      "known animal"))) %>%
  group_by(phase, avg_known, understands, subj, appearance, trial, trial_target) %>%
  summarise(length = sum(length)) %>%
  left_join(select(subj_vocab, subj, vocab), by = "subj") %>%
  mutate(log_length = log(length))

mean_known_data <- trial_known_data %>%
  group_by(phase, understands, subj, appearance, trial_target) %>%
  summarise(length = mean(length)) %>%
  summarise(length = mean(length)) %>%
  summarise(length = mean(length)) %>%
  tidyboot_mean(length)
```

```{r primary-lmer}
primary_lmer <- trial_known_data %>%
  lmer(log_length ~ appearance * understands + avg_known + scale(vocab) +
         (understands | subj) + 
         (appearance | trial_target),
       data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
  mutate(d = t_to_d(statistic, df)$d,
         d_low = t_to_d(statistic, df)$CI_low,
         d_high = t_to_d(statistic, df)$CI_high) %>%
  select(-effect, -group, -std.error) %>%
  mutate(p.value = printp(p.value))

walk2(c("second_appearance", "avg_known", "vocab", "known_animal", 
        "second_appearance_known"),
      c("appearancesecond", "avg_known", "scale(vocab)", 
        "understandsknown animal", "appearancesecond:understandsknown animal"), 
      ~ make_text_vars(primary_lmer, .x, .y))
```


If parents calibrate their referring expressions to their children's linguistic knowledge, they should provide more information to children for whom a simple bare noun (e.g. "leopard") would be insufficient to identify the target. Parents did this in a number of ways: with one or more adjectives (e.g. "the spotted, yellow leopard"), with similes (e.g. "the one that's like a cat"), and with allusions to familiar animal exemplars of the category (e.g. "Limelight Larry"). In many of these cases, parents would be required to produce more words (see below for further qualitative analyses). Thus, we first analyzed the (log) length of parents' referring expressions as a proxy for informativeness.

When do parents produce longer referring expressions? One possibility is that parents tune at the coarsest level, using more words when speaking to children with smaller vocabularies. This was not the case--the total number of words parents thought their children knew did not reliably affect the length of their referring expressions ($\beta =$ `r vocab_estimate`, $p =$ `r vocab_p.value`, $d$ = `r vocab_d` [`r vocab_d_low`,  `r vocab_d_high`]). A second possibility is that parents have a sense for how difficult each animal is in general, and tune coarsely to this. Our analyses confirmed this coarse-tuning: parents said reliably fewer words for animals that more children were reported to know ($\beta =$ `r avg_known_estimate`, $p =$ `r avg_known_p.value`, $d$ = `r avg_known_d` [`r avg_known_d_low`,  `r avg_known_d_high`]); Figure \ref{fig:known-fig}A). Finally, parents could fine-tune their referring expressions to their children's individual knowledge, over and above the average difficulty of each animal. Our analyses supported this conclusion: parents used reliably fewer words to refer to animals that they thought their individual child knew ($\beta =$ `r known_animal_estimate`, $p =$ `r known_animal_p.value`, $d$ = `r known_animal_d` [`r known_animal_d_low`,  `r known_animal_d_high`]); Figure \ref{fig:known-fig}B). Thus parents fine-tuned the amount of information in their referring expressions, calibrating the amount of information they provided to their children's knowledge, even after accounting for the average difficulty of the target animal. 


```{r known-fig, fig.width = 5, fig.height = 2.5, fig.env = "figure", fig.cap = "(A) Parents produced longer referring expressions for animals they thought their child did not know (first appearance). (B) When children chose the correct animal, parents continued to produce longer expressions for animals they thought their children did not know (left). However, if parents thought their child knew an animal, and they chose incorrectly, parents produced longer expressions on its second appearance (right). Gray points and lines represent individual participants, black points and lines show group averaged proportions; error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}

individual_plot_data <- bind_rows(individual_data, indiv_lag_data) %>%
  mutate(type = factor(type, 
                       levels = c("First appearance", "correct", "incorrect"),
                       labels = c("First appearance", "Following correct", 
                                  "Following incorrect")))

group_plot_data <- bind_rows(group_data, group_lag_data) %>%
  mutate(type = factor(type, 
                       levels = c("First appearance", "correct", "incorrect"),
                       labels = c("First appearance", "Following correct", 
                                  "Following incorrect")))

p1 <- ggplot(individual_plot_data %>% filter(type == "First appearance"),
             aes(x = understands, y = length, 
                                       group = subj)) + 
  geom_point(alpha = .5, position=position_dodge(0.06), color = "light gray") +
  geom_line(alpha = .5, color = "light gray", position=position_dodge(0.06)) +
  scale_x_discrete(labels= c("Known\nanimal", "Unknown\nanimal"), 
                   expand = c(.1, .1)) + 
  labs(x ="", y = "Referring expression length") +
  facet_wrap(~ type) +
  geom_pointrange(aes(ymin = length_lower, ymax = length_upper), 
                  data = group_plot_data %>% 
                    filter(type == "First appearance")) + 
  #geom_line(data = group_plot_data) +
  scale_y_continuous(limits = c(2, 15)) + 
  theme(strip.background = element_rect(color = "black"),
        plot.margin = unit(c(6, 12, 6, 6), "points"))

p2 <- ggplot(individual_plot_data %>% filter(type != "First appearance"),
             aes(x = understands, y = length, 
                                       group = subj)) + 
  geom_point(alpha = .5, position=position_dodge(0.06), color = "light gray") +
  geom_line(alpha = .5, color = "light gray", position=position_dodge(0.06)) +
  scale_x_discrete(labels= c("Known\nanimal", "Unknown\nanimal"), 
                   expand = c(.1, .1)) + 
  labs(x ="", y = "Referring expression length") +
  facet_wrap(~ type) +
  geom_pointrange(aes(ymin = length_lower, ymax = length_upper), 
                  data = group_plot_data %>% 
                    filter(type != "First appearance")) + 
  #geom_line(data = group_plot_data) +
  scale_y_continuous(limits = c(2, 15)) +
  theme(strip.background = element_rect(color = "black"),
        plot.margin = unit(c(6, 6, 6, 12), "points"))

ggarrange(p1, p2, nrow = 1, widths = c(1, 2), labels = c("a", "b"), padding = 2)
```


In addition, because each animal appeared as a target twice, we asked whether parents tuned their referring expressions over successive appearances. We found that parents used fewer words on the second appearance of each animal ($\beta =$ `r second_appearance_estimate`, $p =$ `r second_appearance_p.value`, $d$ = `r second_appearance_d` [`r second_appearance_d_low`,  `r second_appearance_d_high`]), but that the difference in utterance length between animals they thought their children knew versus didn't know was smaller on their second appearance ($\beta =$ `r second_appearance_known_estimate`, $p$ `r second_appearance_known_p.value`, $d$ = `r second_appearance_known_d` [`r second_appearance_known_d_low`,  `r second_appearance_known_d_high`]). Why might that be? One possibility is that parents obtain information from the first appearance of each animal: they may have thought their child knew "leopard," but discovered from their incorrect choice that they did not. If so, they might provide more information the second time around.

```{r lag-lmer}
lag_lmer <- setup_lag_data %>%
  ungroup() %>%
  mutate(understands = fct_relevel(understands, 
                                    "Doesn't Know", "Knows")) %>%
  mutate(lag_correct = fct_explicit_na(lag_correct, na_level = "first") %>% 
           fct_relevel(., "first", "correct", "incorrect")) %>%
  lmer(log_length ~ lag_correct * understands + 
         ( understands | subj) + (1 | target), data = .) %>%
  tidy() %>%
  filter(effect == "fixed") %>%
    mutate(d = t_to_d(statistic, df)$d,
         d_low = t_to_d(statistic, df)$CI_low,
         d_high = t_to_d(statistic, df)$CI_high) %>%
  select(-effect, -group, -df, -std.error) %>%
  mutate(p.value = printp(p.value))

walk2(c("lag_correct", "lag_incorrect", "lag_known", "lag_correct_known",
        "lag_incorrect_known"),
      c("lag_correctcorrect", "lag_correctincorrect", "understandsKnows", 
        "lag_correctcorrect:understandsKnows", "lag_correctincorrect:understandsKnows"), 
      ~ make_text_vars(lag_lmer, .x, .y))
```

To test this prediction, we fit a model predicting the (log) length of parents' referring expressions from appearance type (first, following correct, following incorrect), whether the parent thought their child knew the animal prior to the experiment, and their interaction between appearance type and prior belief. Relative to their utterances on an animal's first appearance, parents produced shorter referring expressions on an animal's second appearance following both correct responses ($\beta =$ `r lag_correct_estimate`, $p =$ `r lag_correct_p.value`, $d$ = `r lag_correct_d` [`r lag_correct_d_low`,  `r lag_correct_d_high`]) and  incorrect responses ($\beta =$ `r lag_incorrect_estimate`, $p =$ `r lag_incorrect_p.value`, $d$ = `r lag_incorrect_d` [`r lag_incorrect_d_low`,  `r lag_incorrect_d_high`]). As before, parents produced shorter utterances for animals they thought their child knew ($\beta =$ `r lag_known_estimate`, $p =$ `r lag_known_p.value`, $d$ = `r lag_known_d` [`r lag_known_d_low`,  `r lag_known_d_high`]). When children were correct on an animal's first appearance, parents' referring expressions on its second appearance did not differ in length based on whether they thought their child knew the animal prior to the experiment ($\beta =$ `r lag_correct_known_estimate`, $p =$ `r lag_correct_known_p.value`, $d$ = `r lag_correct_known_d` [`r lag_correct_known_d_low`,  `r lag_correct_known_d_high`]). However, when children were incorrect on an animal's first appearance, and parents thought they knew the animal prior to the experiment, they produced reliably longer referring expressions on its second appearance ($\beta =$ `r lag_incorrect_known_estimate`, $p =$ `r lag_incorrect_known_p.value`, $d$ = `r lag_incorrect_known_d` [`r lag_incorrect_known_d_low`,  `r lag_incorrect_known_d_high`]; Figure \ref{fig:known-fig}B). 

As we predicted, when parents thought their children knew an animal, but then observed evidence to the contrary, they provided more information in their referring expressions to help children make the correct selection the second time. However, we did not find the opposite pattern: when children were successful for animals that parents thought they did not know, parents did not update their beliefs. Why should parents update their beliefs in one direction but not the other? One likely explanation comes from parents' linguistic tuning itself. Parents' goal in this task is to produce a referring expression that allows their children to select the target animal whether or not they know its canonical label. Consequently, when children select correctly on these trials, parents cannot know how their child arrived at the correct target. For example, if an expression contained the phrase "spotted leopard," it is unclear whether the child knew the word "leopard," or was simply correct because the descriptor provided sufficient information to identify the target animal.

Together, these two sets of analyses suggest that parents tune their referring expressions not just coarsely to how much language their children generally know, nor their knowledge about how hard animal words are on average, but finely to their beliefs about their individual children's knowledge of specific lexical items. Further, when parents discover that they have incorrect beliefs about their children's knowledge during interaction, they update these beliefs in real-time and adjust subsequent references to the same item.

## Content of referring expressions

```{r qual-vars}
make_qual_vars <- function(df, term_name, measure, knowledge = NULL) {
  filtered_df <- df %>%
    filter(qual_measure == measure) %>%
    mutate(used = used * 100)
  
  if(!is.null(knowledge)) {
    filtered_df <- filtered_df %>%
      filter(understands == knowledge)
  }
  
  assign(glue("{term_name}_used"), filtered_df %>% pull(used), 
         envir = globalenv())
}
```

```{r qual-coding}
tidy_qual_data <- target_data %>%
  select(-num_anaphoric, -num_animal_sounds) %>%
  group_by(understands, appearance, trial_target, target, subj) %>%
  summarise_at(vars(length, num_descriptors:num_superordinate), sum) %>%
  pivot_longer(cols = num_descriptors:num_superordinate, 
               names_to = "qual_measure", 
               values_to = "used") %>%
  mutate(qual_measure = factor(qual_measure, 
                                levels = c("num_canonical", "num_comparison",
                                           "num_descriptors", "num_superordinate",
                                           "num_subordinate"))) %>%
  rowwise() %>%
  mutate(used = min(used, 1)) %>%
  ungroup()

qual_means <- tidy_qual_data %>%
  group_by(qual_measure, subj, target, appearance) %>%
  summarise(used = mean(used)) %>%
  summarise(used = mean(used)) %>%
  summarise(used = mean(used)) %>%
  summarise(used = mean(used))

qual_knowledge_means <- tidy_qual_data %>%
  group_by(qual_measure, understands, subj, target, appearance) %>%
  summarise(used = mean(used)) %>%
  summarise(used = mean(used)) %>%
  summarise(used = mean(used)) %>%
  tidyboot_mean(used) %>%
  rename(used = empirical_stat)

walk2(c("canonical", "subordinate"), c("num_canonical", "num_subordinate"),
      ~make_qual_vars(qual_means, .x, .y))

qual_lmer <- tidy_qual_data %>%
  group_by(qual_measure) %>%
  nest() %>%
  mutate(model = map(data, 
                     ~glmer(used ~ understands + (1 | subj) + (1 | target), 
                    family = "binomial", data = . ) %>% tidy())) %>%
  select(-data) %>%
  unnest(cols = c(model)) %>%
  filter(effect == "fixed") %>%
  mutate(p.value = printp(p.value),
         d = odds_to_d(estimate, log = TRUE),
         d_high = odds_to_d(estimate + 1.96 * std.error, log = TRUE),
         d_low = odds_to_d(estimate - 1.96 * std.error, log = TRUE)) %>%
  select(-effect, -group, -std.error) %>%
  filter(term != "(Intercept)") %>%
  select(-term) %>%
  rename(term = qual_measure)

walk2(c("canonical_known", "canonical_unknown"), c("Knows", "Doesn't Know"),
      ~make_qual_vars(qual_knowledge_means, .x, "num_canonical", .y))

walk2(c("canonical", "comparison", "descriptors", "superordinate", 
        "subordinate"),
      c("num_canonical", "num_comparison", "num_descriptors", 
        "num_superordinate", "num_subordinate"),
      ~ make_text_vars(qual_lmer, .x, .y))
```

```{r qual-plot, fig.height = 2.5, fig.width = 6, fig.env = "figure", eval = T, fig.cap = "Proportion of trials on which parents used comparison, descriptors, superordinate category labels, and subordinate category labels in their referring expressions. Colored bars show means across parents, error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}
plot_qual_means <- qual_knowledge_means %>%
  ungroup()%>%
  mutate(qual_measure = factor(qual_measure, 
                               levels = c("num_comparison", "num_descriptors",
                                          "num_superordinate", 
                                          "num_subordinate", "num_canonical"),
                               labels = c("Comparison", "Descriptors", 
                                          "Superordinate", "Subordinate", 
                                          "Canonical")))

ggplot(plot_qual_means %>% filter(qual_measure != "Canonical"),
       aes(x = understands, y = used, 
             ymin = ci_lower, ymax = ci_upper, 
             fill = understands)) + 
  facet_grid(~ qual_measure) + 
  labs(x = "", y = "Parents' production probability") +
  geom_col(position = position_dodge(.5), width = .5) + 
  geom_linerange(position = position_dodge(.5)) + 
  scale_x_discrete(labels= c("Known\nanimal", "Unknown\nanimal")) +
  scale_fill_ptol() +
  theme(strip.background = element_rect(color = "black"))
```

Parents produced reliably longer referring expressions when trying to communicate about animals that they thought their children did not know. In the analyses presented so far, we used length as a theory-agnostic, quantitative measure of information. *How* did parents successfully refer to animals that their children did not know? As a post-hoc descriptive analysis, we coded five qualitative features of referring expressions: (1) Use of the animal's canonical label (e.g., "leopard"), (2) Use of a descriptor (e.g., "spotted"), (3) Use of a comparison (e.g., "like a cat"), (4) Use of a superordinate level category label (e.g., "bird" for peacock), and (5) Use of a subordinate level category label (e.g., "Limelight Larry" for peacock). Because the rates of usage of each of these kinds of reference varied widely (e.g. canonical labels were used on `r canonical_used`\% of trials, but subordinates were used on `r subordinate_used`\% of trials), we fit a logistic mixed effects model separately for each reference kind, estimating whether it would be used on each trial from whether the parent thought their child knew the animal. 

Canonical labels were used on almost all trials, and did not differ in frequency between animals parents thought their children did not know ($M =$ `r canonical_unknown_used`\%) and animals they thought their children knew ($M =$ `r canonical_known_used`\%, $\beta =$ `r canonical_estimate`, $p =$ `r canonical_p.value`, $d$ = `r canonical_d` [`r canonical_d_low`, `r canonical_d_high`]). Parents thus produced canonical labels even when they thought their children did not know these labels. One plausible explanation for this is that the target animal on each trial was identified in writing for the parent, activating the canonical label and thus lowering the cost of retrieving and producing it [@wingfield1968]. Another possibility is that this reflects parents' general tendency to produce basic-level category labels when talking to children [@callanan1985; @blewitt1983]. Finally, it could have been produced for implicitly or explicitly pedagogical reasons even though it was not referentially necessary. We expand on this possibility in the Discussion below.

Comparisons were used reliably more for animals parents believed their children did not know than for animals they thought their children knew ($\beta =$ `r comparison_estimate`, $p =$ `r comparison_p.value`, $d$ = `r comparison_d` [`r comparison_d_low`, `r comparison_d_high`]), as were descriptors ($\beta =$ `r descriptors_estimate`, $p$ `r descriptors_p.value`, $d$ = `r descriptors_d` [`r descriptors_d_low`, `r descriptors_d_high`]) and superordinate category labels ($\beta =$ `r superordinate_estimate`, $p =$ `r superordinate_p.value`, $d$ = `r superordinate_d` [`r superordinate_d_low`, `r superordinate_d_high`]). Subordinates were used less for animals parents thought their children did not know than for animals they thought their children knew ($\beta =$ `r subordinate_estimate`, $p =$ `r subordinate_p.value`, $d$ = `r subordinate_d` [`r subordinate_d_low`, `r subordinate_d_high`]). Thus, parents used a variety of strategies to refer to animals that they believed their children do not know, but the use of descriptors was the most prominent (Figure \ref{fig:qual-plot}). These descriptors are particularly apt to facilitate children's learning, connecting parents' fine-tuning for reference with their children's language acquisition.

# Discussion

Parents have a wealth of knowledge about their kids, including their linguistic development [@fenson2007]. In this study, we asked whether parents leverage this knowledge to communicate successfully with their children. When playing a referential communication game, parents drew on their knowledge of their children in three ways: (1) parents produced longer, more informative referring expressions for animals that children generally learn later, (2) over and above this coarse-tuning, parents fine-tuned information to their individual children’s knowledge of specific animals, and (3) when children did not know an animal that parents thought they did, parents subsequently produced longer referring expressions for that animal. Further, this tuning was associated with more successful communication: children were more likely to correctly select animals whose names parents thought they did not know if  parents produced more informative referring expressions. 

These data are consistent with prior evidence of coarse-tuning in child-directed speech, but importantly provide the first experimental evidence for fine-tuning at the lexical level. When communicating with their children, parents not only take into account the average difficulty of each animal word, but they also rely on (and update) their estimates of their individual child’s knowledge of those animals. Coarse-tuning and fine-tuning may be distinct adaptations that happen independently at different timescales, but our data suggest an intriguing alternative possibility: parents’ coarse-grained estimates of their children’s language development may be built hierarchically from estimates of their developing knowledge of individual lexical, syntactic, and other linguistic items. Hierarchical representations are a powerful vehicle for maximizing both speed and generalizability of learning, and they may play the same role here, allowing parents to efficiently track and use their knowledge of their children’s language development [@gelman2006; @tenenbaum2011].

While parents’ speech to children is unlikely to reflect an explicit goal to teach, it is nonetheless goal-oriented: parents want to communicate successfully [@bruner1983]. Our reference game was designed to manipulate and measure a particular communicative goal that can be instantiated in the laboratory, but similar communicative pressures structure the daily conversations between children and their parents [@tamis-lemonda2017]. When talking about animals that they thought their children did not know, parents used referring expressions rich with descriptors and comparisons, as in previous observational studies [@blewitt1983; @masur1997; @mervis1982b]. These strategies scaffold communication--parents use what they think their children know (e.g. color words) in order to communicate about animals they think their children do not know. Because communication and learning are intertwined, these same strategies may work in the service of language acquisition [@yurovsky2018]. While parents produced rich descriptions to help their children select unfamiliar animals, they almost always produced the canonical label as well. These referring expressions are thus an ideal opportunity to learn the relationship between the referent, its label, and its important identifying features. We did not independently measure children's knowledge of each animal, so we cannot determine whether they learned any new animals while playing the game. The relationship between referential strategies and ultimate learning is a promising direction for future work.

Parents fine-tune language to their children’s knowledge in order to communicate successfully. In the service of proximal communicative goals, they may also provide children with input that ultimately accelerates learning. Focusing on the interactive and communicative nature of language captures a more complete picture of language development: while children bring powerful learning mechanisms to language acquisition, these mechanisms are supported by an ecological niche designed for their success.

# Acknowledgements

This research was funded by a James S. McDonnell Foundation Scholar Award to the last author.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
