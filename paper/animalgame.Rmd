---
title: "Parents Calibrate Speech to Children's Vocabulary Knowledge"
bibliography: animalgame.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Ashley Leung, Alexandra Tunkel, and Daniel Yurovsky \\
         \texttt{\{ashleyleung, aetunkel, yurovsky\}@uchicago.edu} \\
        Department of Psychology \\ University of Chicago}

abstract: >
  Young children acquire language at rapid rates, and the proposed mechanisms for learning often focus on children’s unique ability to harvest information from their environments. On the other hand, some lines of research focus on the role that parents' speech and responsiveness play in children's language learning. However, language development is not simply absorbing input- language is social in nature. We cannot ignore the communicative intent and interactive nature of language when considering how input influences children’s language development. The present study examined whether parents calibrate speech to their children’s knowledge in an interactive game. Our results show that parents modify their language according to beliefs about their children’s vocabulary knowledge, using longer sentences when describing unfamiliar objects and shorter sentences for familiar objects.

keywords:
  "parent-child interaction; language development; communication"
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)

options(digits=2)
```

```{r, libraries}
library(png)
library(grid)
library(xtable)
library(tidyverse)
library(knitr)
library(papaja)
library(ggthemes)
library(lme4)
library(lmerTest)
library(directlabels)
library(ggrepel)
library(feather)
library(here)
library(tidyboot)
library(broom)
library(broom.mixed)

theme_set(theme_classic(base_size = 10))
```

# Introduction

Children learn language at astonishing rates, acquiring thousands of words by the time they are toddlers. How do children learn so many words before they know how to dress themselves? One account for children's rapid language acquisition is statistical learning. Young children can attend to the distributional structure of language, learning to discriminate words and identify word order from speech streams [@saffran1996; @saffran2003]. Statistical learning can be a powerful tool for early language learning, and showcases the ability that children have to harvest information from their surroundings. However, the particular structure of children's language environments may also play a role in supporting language development.

The way we speak to children often differs from the way we speak to adults. Child-directed speech (CDS) exists across cultures, and is characterized by higher pitches and more exaggerated enunciations when compared to adult-directed speech (ADS) [@cooper1990; @grieser1988]. Not only do children prefer CDS over ADS, CDS is also a better predictor for language learning than overheard ADS [@shneidman2013]. CDS does not only differ from ADS in prosodic features- the structural qualities of CDS make speech segmentation and word learning easier [@thiessen2005; @yurovsky2012]. While children live in the same physical environments as adults, their *language environments* contain specific types of input that facilitate early language learning.

Children’s language environments are not only suited for their abilities; they also change across development. Parents play a role in changing their children's language environment, and there is evidence suggesting that these changes aid language development. Parents use simpler and more redundant language when talking to toddlers, and more complex syntactic structures when speaking with school-aged children [@snow1972]. Importantly, sensitive modification of parent response shapes language learning in children [@hoff-ginsberg1982; @tamis-lemonda2014]. 

Why do parents modify the way they speak according to their children? One possible explanation is that parents are actively teaching their children. Indeed, some have posited that CDS is an ostensive cue for social learning, and that infants are born prepared to attend to these cues [@csibra2009]. While it may be true that parents hope to impart knowledge to their children, we argue that effective communication is the proximal goal. The field of linguistics has long established that adults communicate in ways that are efficient. For example, Grice's [-@grice1975] maxim of quantity states that speech should be as informative as necessary, and no more. Adults are able to adhere to these maxims, adapting speech according to conversational partners' knowledge as needed for successful communication [@clark1986]. We argue that the parent's goal to communicate with their child drives the change in language use. Specifically, parents adapt their speech according to their children's language abilities. 


Parents modify their language as a *means* to achieve successful communication. Research show that parents use simpler language and are more linguistically aligned with their younger children, and these patterns of speech change as their children develop [@snow1972; @yurovsky2016]. Parents are also sensitive to children’s vocabulary knowledge, and the way they refer to objects change markedly depending on whether they are novel, comprehended, or familiar to their children [@masur1997]. These changes in parent speech may indicate adaptations that are aimed at fulfilling the goal of effective communication, and that the language necessary to fulfill that goal changes as children develop.

Based on work by @masur1997, we developed a study to investigate how parents adapt their speech according to their children’s vocabulary knowledge. Masur’s study involved parents and children engaging in unstructured free play, and parents reported their children’s vocabulary knowledge after the session. Our study uses a structured interactive game that allows us to control for the amount and type of stimuli presented to the parent-child dyads. While experimental manipulation is not completely possible while also eliciting natural speech, our paradigm allows for more experimental control, and introducing a communicative goal within a game setting also allows parent utterances to be more comparable across dyads.

We designed an interactive iPad game in which parents verbally guide their children to select a particular animal on an iPad. The game featured 18 animals, and each animal appears as the target twice during the game. We predicted that parents would modify their speech based on their beliefs about their children’s vocabulary knowledge. Specifically, we predicted: (1) Parents should use shorter sentences when describing animals that they believe their children know, and (2) Upon the second appearance of an animal, parents would adapt the length of their sentence according to whether the child responded accurately on the first appearance of the animal.

# Method

```{r load_data}
target_data <- read_feather(here("data/target_data.feather")) %>%
    filter(phase != "during", person == "parent", !is.na(understands))

word_difficulty <- read_feather(here("data/word_difficulty.feather"))
demos <- read_feather(here("data/demos.feather"))
```

## Participants

Two to two and a half year old children and their parents were recruited from a databse of families in the local community or approached on the floor of a local science museum in order to achieve a planned sample of 40 parent-child dyads. A total of 47 parent-child pairs were recruited, but data from six pairs was dropped from analysis due to experimental error or failure to complete the study. The final sample consistend of `r nrow(demos)` children aged `r min(demos$age)` to `r max(demos$age)` years ($M =$ `r mean(demos$age)`), `r demos %>% summarise(female = sum(demos == "female")) %>% pull` of whom were girls. 

```{r word_difficulty}
group_difficulty <- word_difficulty %>%
  group_by(type, word) %>%
  summarise(understands = mean(understands)) %>%
  tidyboot_mean(understands)

difficulty_lmer <- word_difficulty %>%
  filter(type %in% c("early", "late")) %>%
  mutate(type = factor(type, levels = c("early", "late"))) %>%
  glmer(understands ~ type + (1|subj), family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed")
```


## Stimuli
Eighteen animal images were selected from @rossion2004 image set, which is a colored version of the @snodgrass1980 object set. Animals were selected based on age of acquisition (AoA), using data from WordBank [@frank2017]. The AoA of the selected animals ranged from 12 to 31 months. Half of the animals had lower AoA (12-20 months), and the other half had higher AoA (25-31 months). 

A modified version of the MacArthur-Bates Communicative Development Inventory [CDI; @fenson2007], a parent-reported measure of children’s vocabulary, was administered before the testing session via an online survey. The selected animal words were embedded among the 85 words in the survey. Two of the animal words--one in the early AOA and one in the late AOA category--were excluded by accident, so trials for those words were not included in analysis.


## Design and Procedure
Each parent-child pair played an interactive game using two iPads. Children were given two warm-up trials to get used to tapping on an iPad. The practice and experiment trials began after the warm-up. On each trial, three images of animals were displayed side by side on the child’s screen, and a single word appeared on the parent’s screen (Figure \ref{fig:ipads}). Parents were instructed to communicate as they normally would with their child, and encourage them to choose the object corresponding to the word on their screen. The child was instructed to listen to their parent for cues. Once an animal was tapped, the trial ended, and a new trial began. There were a total of 36 experiment trials, such that each animal appeared as the target twice. Trials were randomized for each participant, with the constraint that the same animal could not be the target twice in a row. Practice trials followed the same format as experimental trials, with the exception that images of fruit and vegetables were shown. All sessions were videotaped for transcription and coding.

```{r ipads, fig.env = "figure", fig.pos = "tb", fig.align='center', fig.width=2, fig.height=2, out.width = "150px", set.cap.width=T, num.cols.cap=1, fig.cap = "Example iPad screens for the child (top) and parent (bottom) during the experiment."}
include_graphics("figs/ipads.pdf")
```

## Results

The data of interest in this study were parent utterances used during the interactive game and parents’ responses on the adapted CDI. Transcripts of the videos were analyzed for utterance length. We measured the length of parents' referring utterances as a proxy for amount of information given in each utterance. Parent utterances irrelevant to the iPad game (e.g. asking the child to sit down) were not analyzed. Children’s utterances were coded when audible, but were not analyzed.

### Word difficulty 

We first confirm that the animals predicted be later learned were less likely to be marked known by the parents of children in our studies. Analyses confirmed that animals in the early AoA category were judged to be understood by `r group_difficulty %>% filter(type == "early") %>% mutate(empirical_stat = empirical_stat * 100) %>% pull(empirical_stat) %>% round(0)`% of parents, and items in the late AoA category were judged understood by `r group_difficulty %>% filter(type == "late") %>% mutate(empirical_stat = empirical_stat * 100) %>% pull(empirical_stat) %>% round(0)`%.

```{r difficulty_fig, set.cap.width=T, num.cols.cap=1, fig.cap = "Parent responses on the CDI for the selected animal words.", fig.height = 2}
mean_word_difficulty <- word_difficulty %>%
  group_by(word) %>%
  tidyboot_mean(understands) %>%
  arrange(desc(empirical_stat))

plotting_words <- mean_word_difficulty %>%
  mutate(word = factor(word, levels = unique(word)))

ggplot(plotting_words, aes(x = word, y = empirical_stat, ymin = ci_lower, 
                            ymax = ci_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), 
        text = element_text(size = 10)) +
  geom_pointrange(size = .3) +
  labs(y = "Parents report known", x = "") 
```

The difference between these groups was confirmed statistically with a logistic mixed effects regression with a fixed effect of AoA type and random effects of participants. The late AoA items were judged known by a significantly smaller proportion of parents ($\beta =$ `r difficulty_lmer %>% filter(term == "typelate") %>% pull(estimate)`, $t =$ `r difficulty_lmer %>% filter(term == "typelate") %>% pull(statistic)`, $p$ `r difficulty_lmer %>% filter(term == "typelate") %>% pull(p.value) %>% printp()`). Parents' judgments for each target word are shown in Figure \ref{fig:difficulty_fig}.

### Utterance length

```{r known_plot, cache = T, fig.cap = "Length of parents' utterances before their child before and after they selected a target. Points indicate means, error bars indicate 95\\% confidence intervals computed by non-parametric bootstrapping", fig.height = 2}
trial_known_data <- target_data %>%
  mutate(phase = factor(phase, levels = c("pre", "post"), labels = c("before selection",
                                                                     "after selection")),
         understands = factor(understands, labels = c("unknown animal", "known animal"))) %>%
  group_by(phase, understands, subj, appearance, trial_target) %>%
  summarise(length = sum(length)) 

mean_known_data <- trial_known_data %>%
  summarise(length = mean(length)) %>%
  summarise(length = mean(length)) %>%
  tidyboot_mean(length)

ggplot(mean_known_data,
       aes(x = phase, y = empirical_stat, 
           ymin = ci_lower, ymax = ci_upper, color = understands,
           label = understands)) + 
  geom_pointrange(position = position_dodge(.25)) +
  scale_color_ptol() +
  labs(x = "", y = "Utterance length (log)") +
  geom_dl(method = list(dl.trans(x=x +2.2), "first.points", cex=.7)) +
  theme(legend.position = "none")
```

```{r}
known_lmer <- trial_known_data %>%
  ungroup() %>%
  mutate(phase = factor(phase, levels = c("after selection", "before selection")),
         understands = factor(understands, levels = c("known animal", "unknown animal"))) %>%
  lmer(length ~ phase * understands + (1|subj) + (1|trial_target), data = .) %>%
  tidy() %>%
  filter(effect == "fixed")
```

If parents are calibrating the referential expressions to their children's linguistic knowledge, we predicted that they should produce more informative-- and thus longer--utterances to refer to animals that they thought their children did not know. Figure ~\ref{fig:known_plot} shows parents’ utterance lengths for animals that they believe their children know versus those they believe their children do not know--both before their child selected an animal and after. In line with our prediction, parents used significantly shorter utterances when talking about animals that they believe their children know. However, once the child had selected an animal, the encouragement and other conversation did not differ between known and unknown animals. We confimed this result statistically, predicting utterance length from a mixed effects model with fixed effects of phase and animal knowledge and their interaction, and random effects of participant and item. All phase and the interaction of phase and knowledge were  significant: Utterances were longer before selection ($\beta =$ `r known_lmer %>% filter(term == "phasebefore selection") %>% pull(estimate)`, $t =$ `r known_lmer %>% filter(term == "phasebefore selection") %>% pull(statistic)`,
$p$ `r known_lmer %>% filter(term == "phasebefore selection") %>% pull(p.value) %>% printp()`), and before selection when the animal was unknown ($\beta =$ `r known_lmer %>% filter(term == "phasebefore selection:understandsunknown animal") %>% pull(estimate)`, $t =$ `r known_lmer %>% filter(term == "phasebefore selection:understandsunknown animal") %>% pull(statistic)`, $p =$ `r known_lmer %>% filter(term == "phasebefore selection:understandsunknown animal") %>% pull(p.value) %>% printp()`), but not after the selection ($\beta =$ `r known_lmer %>% filter(term == "understandsunknown animal") %>% pull(estimate)`, $t =$ `r known_lmer %>% filter(term == "understandsunknown animal") %>% pull(statistic)`, $p =$ `r known_lmer %>% filter(term == "understandsunknown animal") %>% pull(p.value) %>% printp()`).

```{r length_fig, set.cap.width=T, num.cols.cap=1, fig.cap = "Parent utterance length on the first (blue) and second (red) appearance of targets. The x-axis is proportion of parents who responded that their children knew the word.", fig.height = 2, eval = F}
plotting_data <- target_data %>%
  group_by(appearance, target, subj) %>%
  summarise(length = sum(length)) %>%
  tidyboot_mean(length) %>%
  rename(length = empirical_stat,
         length_upper = ci_upper,
         length_lower = ci_lower) %>%
  select(-mean, -n) %>%
  left_join(word_difficulty, by = c("target" = "word")) %>%
  rename(understands_lower = ci_lower, understands_upper = ci_upper) %>%
  select(-mean) %>%
  mutate(plotting_appearance = factor(appearance, levels = c("first", "second"),
                                     labels = c("1st appearance",
                                                "2nd appearance")))

ggplot(plotting_data, aes(x = understands, y = length, xmin = understands_lower,
                          xmax = understands_upper, ymin = length_lower,
                          ymax = length_upper, color = plotting_appearance)) + 
  geom_smooth(method = "lm", se = F) +
  geom_pointrange(alpha = .2)+ 
  geom_errorbarh(alpha = .2) + 
  scale_color_ptol() +
  scale_y_continuous(limits = c(0, 20)) +
  theme(legend.position = "none") +
  labs(x = "prop. parents report known", 
       y = "log utterance length") 
```

In order to examine whether parents modify their speech behaviors within the timeframe of the experiment, we compared utterance lengths between first and second appearances of each target. Overall, utterances were shorter for the second appearance of a given target (Fig. \ref{fig:length_fig}).

We also asked whether parents changed their behaviors according to their children’s responses during the game. When children responded incorrectly on the first appearance of a target that parents believe they know (e.g. cat), we found that parents used longer utterances when the same target appeared a second time. However, we did not find this effect for targets that parents believe their children do not know (e.g. lobster). That is, regardless of whether children responded correctly or incorrectly on the first appearance of these targets, there was no difference in utterance length on the second appearance (Fig. 5). These results are partially in line with our prediction that parents would modify their speech during the game based on their children's accuracy.

A linear model predicting children’s response accuracy found that utterance length, parent belief about children’s knowledge, and the interaction between these two factors were reliable predictors (Table 1). Specifically, children were more likely to be correct on trials in which parents used longer utterances. On trials where parents believe their children know the word, children were more likely to be correct. Finally, there was an interaction between utterance length and parent belief. Longer utterances were more likely to be associated with a correct response on trials in which parents believe their children do not know the word. Overall, children performed significantly above chance for both high and low AoA words (**Accuracy Plot Here**).

# Discussion

Our results indicate that parents speak differently depending on their beliefs about their children's vocabulary knowledge. Specifically, parents use shorter sentences when talking about animals that they believe their children know. Since parents completed the CDI prior to participating in our study, our data allow us to correlate parent beliefs with their behaviors. A conceptually similar observational study by @masur1997 asked parents to indicate children's vocabulary knowledge after the observational sessions. While their results also show that parents' speech differs for familiar and unfamiliar animals, it is unclear whether parents used children's responses during each session to assess children's knowledge. Our results not only corroborate their findings, but allow us to argue that parents' prior beliefs about their children's knowledge predict the way they will talk.

Contrary to our predictions, parents did not consistently modify their speech depending on their children’s response. Specifically, utterance length remained the same for words that parents think their children do not know, regardless of whether their children responded correctly. This may indicate that parent sensitivity does not emerge within the short time frame of a game. However, qualitative analysis of our transcripts may reveal more about parent behaviors in the case of unfamiliar words. Utterance length remains relatively long across the first and second appearance of these animals (see Fig. 5). It is possible that parents believe their children’s accuracy was due to their sufficient description of the animal (e.g. “the one that looks like a cat” for leopard).

Overall, children performed significantly above chance across all trials. This finding may seem surprising at first glance, given our CDI data (see Fig. 2). Given that previous studies have found the CDI to be a valid measure of children’s vocabulary [@dale1989], our results suggest that parents' behavior influences childrens' behavior. The high accuracy may reflect that parents succeeded in their goal to communicate, such that children were able to respond correctly even on unfamiliar trials. Since three animals appeared on each trial, children could have used strategies such as mutual exclusivity to find the correct target. However, since low and high AoA animals appeared separately, there may not have been many opportunities to use mutual exclusitivity. Taken together with our finding that parents used longer sentences for words they think their children do not know, our results suggest that parents modified their speech as a means to communicate.

One account for explaining our results is that communicative efficiency is driving parent behavior. Our results suggest that parents may be adhering Gricean conversational maxims when communicating with their children, using longer sentences for unfamiliar words, presumably because more information is necessary in cases where children do not know the target word. Our study analyzed utterance length as a proxy for information, but further qualitative analysis will be needed to determine whether longer utterances do indeed contain more information rather than simply consist of repeated words.

Our results can also be interpreted from a different perspective, in terms of speaker-design and listener- design theories. Proponents of speaker-design accounts argue that the speaker's own cognitive capacities, such as memory retrieval, influences the speech they produce [@macdonald2013]. On the other hand, those supporting listener-design accounts argue that the listener's needs, such as their knowledge level, drives speaker's language [@jaeger2013]. The opposing forces driving communication can be understood as the pressure to produce speech quickly within a communicative exchange (speaker-design), versus the pressure to produce speech that is understood by the partner (listener-design). Within the context of our game, our results support more strongly a listener-design account of communication. While parents could be using longer sentences for some words because those words are harder to retrieve, our design essentially rules out this possibility. On each trial, parents are given the target word on their iPad screen. In this case, speaker-design accounts should predict that parents will also simply say the word given to them, as that is the least cognitively taxing option. The fact that parents are using long and short sentences depending on their beliefs about children's vocabulary knowledge suggests that they are calibrating to their children.

It is important to note that our current results do not rule out the possibility that parents are engaging in pedagody. Parents may be using longer utterances because they wish to teach their children certain words. This account potentially explains why parents use longer sentences for words they believe their children do not know, and our current analysis does not allow us to distinguish between the the pedagogical and communicative hypotheses. However, analyzing the content of parents' speech will be helpful in understanding the motivations behind long and short utterances. While full qualitative analysis is underway, there is preliminary evidence in support of the communicative account. Parents often do not use the canonical name of an animal during a trial (e.g. "Find the red one" for lobster), suggesting that they are not (at least not always) engaging in pedagogy.

Our work contributes to the current literature on parent-child interaction, and forms the basis for further experimental work examining the influences that parent speech has on children’s language development. In line with @masur1997, our findings provide evidence that parents calibrate speech sensitively to their children's vocabulary knowledge. These results are important in light of previous work suggesting that parent responsiveness and sensitivity shapes the way young children learn language [@hoff-ginsberg1982; @tamis-lemonda2014]. Furthermore, we propose that parents are modifying their speech as a means to communicate, and that communicative intent shapes the language environments children experience. Further qualitative analysis of our dataset will shed light onto the characteristics of parent-child communication that are helpful for language acquisition.

Finally, this study highlights the importance of studying the parent-child pair as a unit, rather than viewing children as isolated learners. As argued many years ago by @hoff-ginsberg1982, both parents and children contribute to the process of language development. Focusing on the interactive and communicative nature of language captures a more realistic picture of children's language environments: the input that children receive is not random – it is sensitive to their developmental level.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
