---
title: "Parents Calibrate Speech to Children's Vocabulary Knowledge"
bibliography: animalgame.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Ashley Leung, Alexandra Tunkel, and Daniel Yurovsky \\
         \texttt{\{ashleyleung, aetunkel, yurovsky\}@uchicago.edu} \\
        Department of Psychology \\ University of Chicago}

abstract: >
  Young children acquire language at rapid rates, and the proposed mechanisms for learning often focus on children’s unique ability to harvest information from their environments. On the other hand, some lines of research focus on the role that parents' speech and responsiveness play in children's language learning. However, language development is not simply absorbing input- language is social in nature. We cannot ignore the communicative intent and interactive nature of language when considering how input influences children’s language development. The present study examined whether parents calibrate speech to their children’s knowledge in an interactive game. Our results show that parents modify their language according to beliefs about their children’s vocabulary knowledge, using longer sentences when describing unfamiliar objects and shorter sentences for familiar objects.

keywords:
  "parent-child interaction; language development; communication"
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)

options(digits=2)
```

```{r, libraries}
library(png)
library(grid)
library(xtable)
library(tidyverse)
library(knitr)
library(papaja)
library(ggthemes)
library(lme4)
library(lmerTest)
library(directlabels)
library(ggrepel)
library(feather)
library(here)
library(tidyboot)
library(broom)
library(broom.mixed)

theme_set(theme_classic(base_size = 1))
```

# Introduction

Children learn language at astonishing rates, acquiring thousands of words by the time they are toddlers. How do children learn so many words before they know how to dress themselves? One account for children's rapid language acquisition is statistical learning. Young children can attend to the distributional structure of language, learning to discriminate words and identify word order from speech streams [@saffran1996; @saffran2003]. Statistical learning can be a powerful tool for early language learning, and showcases the ability that children have to harvest information from their surroundings. However, the particular structure of children's language environments may also play a role in supporting language development.

The way we speak to children often differs from the way we speak to adults. Child-directed speech (CDS) exists across cultures, and is characterized by higher pitches and more exaggerated enunciations when compared to adult-directed speech (ADS) [@cooper1990; @grieser1988]. Not only do children prefer CDS over ADS, CDS is also a better predictor for language learning than overheard ADS [@shneidman2013]. CDS does not only differ from ADS in prosodic features- the structural qualities of CDS make speech segmentation and word learning easier [@thiessen2005; @yurovsky2012]. While children live in the same physical environments as adults, their *language environments* contain specific types of input that facilitate early language learning.

Children’s language environments are not only suited for their abilities; they also change across development. Parents play a role in changing their children's language environment, and there is evidence suggesting that these changes aid language development. Parents use simpler and more redundant language when talking to toddlers, and more complex syntactic structures when speaking with school-aged children [@snow1972]. Importantly, sensitive modification of parent response shapes language learning in children [@hoff-ginsberg1982; @tamis-lemonda2014]. 

Why do parents modify the way they speak according to their children? One possible explanation is that parents are actively teaching their children. Indeed, some have posited that CDS is an ostensive cue for social learning, and that infants are born prepared to attend to these cues [@csibra2009]. While it may be true that parents hope to impart knowledge to their children, we argue that effective communication is the proximal goal. The field of linguistics has long established that adults communicate in ways that are efficient. For example, Grice's [-@grice1975] maxim of quantity states that speech should be as informative as necessary, and no more. Adults are able to adhere to these maxims, adapting speech according to conversational partners' knowledge as needed for successful communication [@clark1986]. We argue that the parent's goal to communicate with their child drives the change in language use. Specifically, parents adapt their speech according to their children's language abilities. 


Parents modify their language as a *means* to achieve successful communication. Research show that parents use simpler language and are more linguistically aligned with their younger children, and these patterns of speech change as their children develop [@snow1972; @yurovsky2016]. Parents are also sensitive to children’s vocabulary knowledge, and the way they refer to objects change markedly depending on whether they are novel, comprehended, or familiar to their children [@masur1997]. These changes in parent speech may indicate adaptations that are aimed at fulfilling the goal of effective communication, and that the language necessary to fulfill that goal changes as children develop.

Based on work by @masur1997, we developed a study to investigate how parents adapt their speech according to their children’s vocabulary knowledge. Masur’s study involved parents and children engaging in unstructured free play, and parents reported their children’s vocabulary knowledge after the session. Our study uses a structured interactive game that allows us to control for the amount and type of stimuli presented to the parent-child dyads. While experimental manipulation is not completely possible while also eliciting natural speech, our paradigm allows for more experimental control, and introducing a communicative goal within a game setting also allows parent utterances to be more comparable across dyads.

We designed an interactive iPad game in which parents verbally guide their children to select a particular animal on an iPad. The game featured 18 animals, and each animal appears as the target twice during the game. We predicted that parents would modify their speech based on their beliefs about their children’s vocabulary knowledge. Specifically, we predicted: (1) Parents should use shorter sentences when describing animals that they believe their children know, and (2) Upon the second appearance of an animal, parents would adapt the length of their sentence according to whether the child responded accurately on the first appearance of the animal.

# Method

```{r load_data}
target_data <- read_feather(here("data/animalgame_data.feather"))
word_difficulty <- read_feather(here("data/word_difficulty.feather"))
demo_data <- read_csv(here("analysis/raw_data/animalgame_demo.csv"))
```

## Participants
Children from age `r (min(demo_data$age))/365 ` to `r (max(demo_data$age))/365` (M = `r (mean(demo_data$age))/365`) and their parents were recruited from a databse of families in the local community or approached on the floor of a local science museum in order to achieve a planned sample of 40 parent-child dyads. A total of 47 parent-child pairs were recruited, but data from six pairs was dropped from analysis due to experimental error or failure to complete the study. The 40 children **D DEMOGRAPHICS HERE**

```{r word_difficulty}
group_difficulty <- word_difficulty %>%
  group_by(type, word) %>%
  summarise(understands = mean(understands)) %>%
  tidyboot_mean(understands)

difficulty_lmer <- word_difficulty %>%
  filter(type %in% c("early", "late")) %>%
  mutate(type = factor(type, levels = c("early", "late"))) %>%
  glmer(understands ~ type + (1|subj), family = "binomial", data = .) %>%
  tidy() %>%
  filter(effect == "fixed")
```


## Stimuli
Eighteen animal images were selected from @rossion2004 image set, which is a colored version of the @snodgrass1980 object set. Animals were selected based on age of acquisition (AoA), using data from WordBank [@frank2017]. The AoA of the selected animals ranged from 12 to 31 months. Half of the animals had lower AoA (12-20 months), and the other half had higher AoA (25-31 months). 

A modified version of the MacArthur-Bates Communicative Development Inventory [CDI; @fenson2007], a parent-reported measure of children’s vocabulary, was administered before the testing session via an online survey. The selected animal words were embedded among the **AMOUNT** words in the survey. Two of the animal words--one in the early AOA and one in the late AOA category were excluded by accident, and so trials for those words were not included in analysis.

## Design and Procedure
Each parent-child pair played an interactive game on iPads. Children were given two warm-up trials to get used to tapping on an iPad. The practice and experiment trials began after the warm-up. On each trial, three images were displayed side by side on the child’s screen, and a single word appeared on the parent’s screen (Figure \ref{fig:ipads}). Parents were instructed to communicate as they normally would with their child, and encourage them to choose the object corresponding to the word on their screen. The child was instructed to listen to their parent for cues. Once an animal was tapped, the trial ended, and a new trial began. There were a total of 36 experiment trials, such that each animal appeared as the target twice. Trials were randomized for each participant, with the constraint that the same animal could not be the target twice in a row. Practice trials followed the same format as experimental trials, with the exception that images of fruit and vegetables were shown. All sessions were videotaped for transcription and coding.

```{r ipads, fig.env = "figure", fig.pos = "tb", fig.align='center', fig.width=2, fig.height=2, out.width = "200px", set.cap.width=T, num.cols.cap=1, fig.cap = "Example iPad screens for the child (top) and parent (bottom) during the experiment."}
include_graphics("figs/ipads.pdf")
```

## Results

The data of interest in this study were parent utterances used during the interactive game, and parents’ responses on the CDI. Transcripts of the videos were analyzed for utterance length. We measured the length of parents' referring utterances as a proxy for amount of information given in each utterance. Parent utterances irrelevant to the iPad game (e.g. asking the child to sit down) were not analyzed. Children’s utterances were coded when audible, but were not analyzed.

### Word difficulty 

We first confirm that the animals predicted be later learned were less likely to be marked known by the parents of children in our studies. Analyses confirmed that animals in the early AoA category were judged to be understood by `r group_difficulty %>% filter(type == "early") %>% mutate(empirical_stat = empirical_stat * 100) %>% pull(empirical_stat) %>% round(0)`% of parents, and items in the late AoA category were judged understood by `r group_difficulty %>% filter(type == "late") %>% mutate(empirical_stat = empirical_stat * 100) %>% pull(empirical_stat) %>% round(0)`%.

```{r difficulty_fig, set.cap.width=T, num.cols.cap=1, fig.cap = "Word difficulties.", fig.height = 2}
mean_word_difficulty <- word_difficulty %>%
  group_by(word) %>%
  tidyboot_mean(understands) %>%
  arrange(desc(empirical_stat))

plotting_words <- mean_word_difficulty %>%
  mutate(word = factor(word, levels = unique(word)))

ggplot(plotting_words, aes(x = word, y = empirical_stat, ymin = ci_lower, 
                            ymax = ci_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5), 
        text = element_text(size = 10)) +
  geom_pointrange(size = .3) +
  labs(y = "Parents report known", x = "") 
```

The difference between these groups was confirmed statistically with a logistic mixed effects regression with a fixed effect of AoA type and random effects of participants. The late AoA items were judged known by a significantly smaller proportion of parents ($\beta =$ `r difficulty_lmer %>% filter(term == "typelate") %>% pull(estimate)`, $t =$ `r difficulty_lmer %>% filter(term == "typelate") %>% pull(statistic)`, $p$ `r difficulty_lmer %>% filter(term == "typelate") %>% pull(p.value) %>% printp()`). Parents' judgments for each target word are shown in Figure \ref{fig:difficulty_fig}.

### Utterance length

Utterance length was analyzed in relation to parents’ beliefs about their children’s knowledge. Figure 3 shows parents’ utterance lengths for animals that they believe their children know versus those they believe their children do not know. In line with our prediction, parents used significantly shorter utterances when talking about animals that they believe their children know.

In order to examine whether parents modify their speech behaviors within the timeframe of the experiment, we compared utterance lengths between first and second appearances of each target. Overall, utterances were shorter for the second appearance of a given target (Fig. 4).

We also asked whether parents changed their behaviors according to their children’s responses during the game. When accounting for children’s accuracy, we found that parents used longer utterances when their child responded incorrectly on the first appearance of a target that they believe their children know. However, no difference in utterance length was found for words that parents believed their children did not know (Fig. 5). These results are partially in line with our prediction that parents would modify their speech during the game.

We did not make specific predictions about accuracy, but our results show that children performed significantly above chance for both high and low AoA words (plot coming soon!).
A linear model predicting children’s response accuracy found that utterance length, parent belief about children’s knowledge, and the interaction between these two factors were reliable predictors (Table 1). Specifically, children were more likely to be correct on trials where parents used longer utterances. On trials where parents believe their children know the word, children were more likely to be correct. Finally, there was an interaction between utterance length and parent belief. Longer utterances were more likely to be associated with a correct response on trials where parents believe their children do not know the word.

# Discussion

Our results indicate that parents speak differently depending on their beliefs about their children's vocabulary knowledge. Specifically, parents use shorter sentences when talking about animals that they believe their children know. Since parents completed the CDI prior to participating in our study, our data allow us to correlate parent beliefs and their behaviors. A conceptually similar observational study by @masur1997 asked parents to indicate children's vocabulary knowledge after the observational sessions. While their results also show that parents' speech differ for familiar and unfamiliar animals, it is unclear whether parents used children's responses during each session to assess children's knowledge. Our results not only corroborate their findings, but allows us to argue that parents' prior beliefs about their children's knowledge predicts the way they will talk.

Contrary to our predictions, parents did not consistently modify their speech depending on their children’s response. Specifically, utterance length remained the same for words that parents think their children do not know, regardless of whether their children responded correctly. This may indicate that parent sensitivity does not emerge within the short time frame of a game. However, qualitative analysis of our transcripts may reveal more about parent behaviors in the case of unfamiliar words. Utterance length remains relatively long across the first and second appearance of these animals (see Fig. 5). It is possible that parents believe their children’s accuracy was due to their sufficient description of the animal (e.g. “the one that looks like a cat” for leopard).

Overall, children performed significantly above change across all trials. This finding may seem surprising at first glance, given our CDI data (see Fig. 2). While our results could indicate that parents are inaccurate in their estimation of children’s vocabulary knowledge, previous studies have found the CDI to be a valid measure of children’s vocabulary [@dale1989]. What the high accuracy may instead reflect is that parents succeeded in their goal to communicate. Taken together with our finding that parents used longer sentences for words they think their children do not know, our results suggest that parents modified their speech as a means to communicate.

One account for explaining our results is that communicative efficiency is driving parent behavior. Our results suggest that parents may be adhering Gricean conversational maxims when communicating with their children, using longer sentences for unfamiliar words, presumably because more information is necessary in cases where children do not know the target word. Our study analyzed utterance length as a proxy for information, but further qualitative analysis will be needed to determine whether longer utterances do indeed contain more information (rather than simply consisting of repeated words).

Our results can also be interpreted from a different perspective, in terms of speaker-design and listener- design theories. Proponents of speaker-design accounts argue that the speaker's own cognitive capacities, such as memory retrieval, influences the speech they produce [@macdonald2013]. On the other hand, those supporting listener-design accounts argue that the listener's needs, such as their knowledge level, drives speaker's language [@jaeger2013]. The opposing forces driving communication can be understood as the pressure to produce speech quickly within a communicative exchange (speaker-design), versus the pressure to produce speech that is understood by the partner (listener-design). Upon first glance, it may seem as though both forces are at play, and this may be true of general conversations. However, within the context of our game, our results support more strongly a listener-design account of communication. While parents could be using longer sentences for some words because those words are harder to retrieve, our design essentially rules out this possibility. On each trial, parents are given the target word on their iPad screen. In this case, speaker-design accounts should predict that parents will also simply say the word given to them, as that is the least cognitively taxing option. The fact that parents are using long and short sentences depending on their beliefs about children's vocabulary knowledge suggests that they are calibrating to their children.

We therefore argue for an account that combines both Grice's [-@grice1975] maxim of quantity and listener-design. Our results show that parents are motivated to communicate efficiently, while adapting their speech to accommodate for their children's knowledge. The maxim of quantity is not in opposition with listener- design accounts, and perhaps even implicitly assumes it. However, combining these two perspectives provides a more holistic understanding of our results: parents are motivated to provide sufficient information, and they do so by taking into account their children’s developmental abilities over their own production pressures.

Our work contributes to the current literature on parent-child interaction, and forms the basis for further experimental work examining the influences that parent speech has on children’s language development. Previous work suggests that parent responsiveness and sensitivity shapes the way young children learn language [@hoff-ginsberg1982; @tamis-lemonda2014], and further analysis of our dataset may reveal the specific characteristics of parents’ speech that is helpful for language learning.

Finally, this study highlights the importance of studying the parent-child pair as a unit, rather than viewing children as isolated learners. As argued many years ago @hoff-ginsberg1982, both parents and children contribute to the process of language development. Focusing on the interactive and communicative nature of language captures a more realistic picture of children's language environments. The input that children receive is not random – it is sensitive to their developmental level.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
